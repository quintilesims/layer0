{
    "docs": [
        {
            "location": "/", 
            "text": "Build, Manage, and Deploy Your Application\n#\n\n\nMeet Layer0\n#\n\n\nLayer0 is a framework that helps you deploy web applications to the cloud with minimal fuss. Using a simple command line interface (CLI), you can manage the entire life cycle of your application without having to focus on infrastructure.\n\n\nReady to learn more about Layer0? See our \nintroduction page\n to learn about some important concepts. When you're ready to get started, take a look at the \ninstallation page\n for information about setting up Layer0.\n\n\nDownload\n#\n\n\n\n\n\n\n\n\nDownload \nv0.10.3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmacOS\n\n\nLinux\n\n\nWindows\n\n\n\n\n\n\n\n\nContact Us\n#\n\n\nIf you have questions about Layer0, email the development team at \ncarbon@us.imshealth.com\n.", 
            "title": "Home"
        }, 
        {
            "location": "/#build-manage-and-deploy-your-application", 
            "text": "", 
            "title": "Build, Manage, and Deploy Your Application"
        }, 
        {
            "location": "/#meet-layer0", 
            "text": "Layer0 is a framework that helps you deploy web applications to the cloud with minimal fuss. Using a simple command line interface (CLI), you can manage the entire life cycle of your application without having to focus on infrastructure.  Ready to learn more about Layer0? See our  introduction page  to learn about some important concepts. When you're ready to get started, take a look at the  installation page  for information about setting up Layer0.", 
            "title": "Meet Layer0"
        }, 
        {
            "location": "/#download", 
            "text": "Download  v0.10.3             macOS  Linux  Windows", 
            "title": "Download"
        }, 
        {
            "location": "/#contact-us", 
            "text": "If you have questions about Layer0, email the development team at  carbon@us.imshealth.com .", 
            "title": "Contact Us"
        }, 
        {
            "location": "/releases/", 
            "text": "Version\n\n\nmacOS\n\n\nLinux\n\n\nWindows\n\n\n\n\n\n\n\n\n\n\nv0.10.3\n\n\nmacOS\n\n\nLinux\n\n\nWindows\n\n\n\n\n\n\nv0.10.2\n\n\nmacOS\n\n\nLinux\n\n\nWindows\n\n\n\n\n\n\nv0.10.1\n\n\nmacOS\n\n\nLinux\n\n\nWindows\n\n\n\n\n\n\nv0.10.0\n\n\nmacOS\n\n\nLinux\n\n\nWindows\n\n\n\n\n\n\nv0.9.0\n\n\nmacOS\n\n\nLinux\n\n\nWindows\n\n\n\n\n\n\nv0.8.4\n\n\nmacOS\n\n\nLinux\n\n\nWindows", 
            "title": "Releases"
        }, 
        {
            "location": "/intro/", 
            "text": "Layer0 Introduction\n#\n\n\nIn recent years, the process of deploying applications has seen incredible innovation. However, this innovation has taken a somewhat simple task and made it into something quite \ncomplicated\n. Cloud providers, load balancing, virtual servers, IP subnets, and a continuing list of technological considerations are not only required to be understood, but their creation and management must be automated for a modern application to be successful at scale.\n\n\nThe burden of understanding a complicated and ever-growing infrastructure is a large aspect of what Layer0 is trying to fix. We've already done the leg work for huge swathes of your backend infrastructure, and we've made it easy to tear down and start over again, too. Meanwhile, you can develop locally using \nDocker\n and be assured that your application will properly translate to the cloud when you're ready to deploy.\n\n\nLayer0 requires a solid understanding of Docker to get the most out of it. We highly recommend starting with \nDocker's Understanding the Architecture\n to learn more about using Docker locally and in the cloud. We also recommend the \nTwelve-Factor App\n primer, which is a critical resource for understanding how to build a microservice.\n\n\n\n\nLayer0 Concepts\n#\n\n\nThe following concepts are core Layer0 abstractions for the technologies and features we use \nbehind the scenes\n. These terms will be used throughout our guides, so having a general understanding of them is helpful.\n\n\nCertificates\n#\n\n\nSSL certificates obtained from a valid \nCertificate Authority (CA)\n. You can use these certificates to secure your HTTPS services by applying them to your Layer0 load balancers.\n\n\nDeploys\n#\n\n\nECS Task Definitions\n. These configuration files detail how to deploy your application. We have several \nsample applications\n available that show what these files look like --- they're called \nDockerrun.aws.json\n within each sample app.\n\n\nTasks\n#\n\n\nManual one-off commands that don't necessarily make sense to keep running, or to restart when they finish. These run using Amazon's \nRunTask\n action (more info \nhere\n), and are \"ideally suited for processes such as batch jobs that perform work and then stop.\"\n\n\nLoad Balancers\n#\n\n\nPowerful tools that give you the basic building blocks for high-availability, scaling, and HTTPS. We currently use Amazon's \nElastic Load Balancing\n, and it pays to understand the basics of this service when working with Layer0.\n\n\nServices\n#\n\n\nYour running Layer0 applications. We also use the term \nservice\n for tools such as Consul, for which we provide a pre-built \nsample implementation\n using Layer0.\n\n\nEnvironments\n#\n\n\nLogical groupings of services. Typically, you would make a single environment for each tier of your application, such as \ndev\n, \nstaging\n, and \nprod\n.", 
            "title": "Introduction"
        }, 
        {
            "location": "/intro/#layer0-introduction", 
            "text": "In recent years, the process of deploying applications has seen incredible innovation. However, this innovation has taken a somewhat simple task and made it into something quite  complicated . Cloud providers, load balancing, virtual servers, IP subnets, and a continuing list of technological considerations are not only required to be understood, but their creation and management must be automated for a modern application to be successful at scale.  The burden of understanding a complicated and ever-growing infrastructure is a large aspect of what Layer0 is trying to fix. We've already done the leg work for huge swathes of your backend infrastructure, and we've made it easy to tear down and start over again, too. Meanwhile, you can develop locally using  Docker  and be assured that your application will properly translate to the cloud when you're ready to deploy.  Layer0 requires a solid understanding of Docker to get the most out of it. We highly recommend starting with  Docker's Understanding the Architecture  to learn more about using Docker locally and in the cloud. We also recommend the  Twelve-Factor App  primer, which is a critical resource for understanding how to build a microservice.", 
            "title": "Layer0 Introduction"
        }, 
        {
            "location": "/intro/#layer0-concepts", 
            "text": "The following concepts are core Layer0 abstractions for the technologies and features we use  behind the scenes . These terms will be used throughout our guides, so having a general understanding of them is helpful.", 
            "title": "Layer0 Concepts"
        }, 
        {
            "location": "/intro/#certificates", 
            "text": "SSL certificates obtained from a valid  Certificate Authority (CA) . You can use these certificates to secure your HTTPS services by applying them to your Layer0 load balancers.", 
            "title": "Certificates"
        }, 
        {
            "location": "/intro/#deploys", 
            "text": "ECS Task Definitions . These configuration files detail how to deploy your application. We have several  sample applications  available that show what these files look like --- they're called  Dockerrun.aws.json  within each sample app.", 
            "title": "Deploys"
        }, 
        {
            "location": "/intro/#tasks", 
            "text": "Manual one-off commands that don't necessarily make sense to keep running, or to restart when they finish. These run using Amazon's  RunTask  action (more info  here ), and are \"ideally suited for processes such as batch jobs that perform work and then stop.\"", 
            "title": "Tasks"
        }, 
        {
            "location": "/intro/#load-balancers", 
            "text": "Powerful tools that give you the basic building blocks for high-availability, scaling, and HTTPS. We currently use Amazon's  Elastic Load Balancing , and it pays to understand the basics of this service when working with Layer0.", 
            "title": "Load Balancers"
        }, 
        {
            "location": "/intro/#services", 
            "text": "Your running Layer0 applications. We also use the term  service  for tools such as Consul, for which we provide a pre-built  sample implementation  using Layer0.", 
            "title": "Services"
        }, 
        {
            "location": "/intro/#environments", 
            "text": "Logical groupings of services. Typically, you would make a single environment for each tier of your application, such as  dev ,  staging , and  prod .", 
            "title": "Environments"
        }, 
        {
            "location": "/setup/install/", 
            "text": "Create a new Layer0 Instance\n#\n\n\nPrerequisites\n#\n\n\nBefore you can install and configure Layer0, you must obtain the following:\n\n\n\n\nAn AWS account.\n\nPlease have your AWS access key and secret ready.\n\n\nIf the aws-cli is installed you can check them here \ncat ~/.aws/credentials\n\n\n\n\nIf it's not installed assign them with \nl0-setup init --aws-access-key \nvalue\n --aws-secret-key \nvalue\n\n\n\n\n\n\nAn EC2 Key Pair.\n\nThis key pair allows you to access the EC2 instances running your Services using SSH.\nIf you have already created a key pair, you can use it for this process.\nOtherwise, follow the \ninstructions at aws.amazon.com\n to create a new key pair.\nMake a note of the name that you selected when creating the key pair.\n\n\n\n\n\n\nTerraform v0.11++\n\nWe use Terraform to create the resources that Layer0 needs.\nIf you're unfamiliar with Terraform, you may want to check out our \nintroduction\n.\nIf you're ready to install Terraform, there are instructions in the \nTerraform documentation\n.\n\n\n\n\n\n\nPart 1: Download and extract Layer0\n#\n\n\n\n\nIn the \nDownloads section of the home page\n, select the appropriate installation file for your operating system. Extract the zip file to a directory on your computer.\n\n\n(Optional) Place the \nl0\n and \nl0-setup\n binaries into your system path. \nFor more information about adding directories to your system path, see the following resources:\n\n\n(Windows): \nHow to Edit Your System PATH for Easy Command Line Access in Windows\n\n\n(Linux/macOS): \nAdding a Directory to the Path\n\n\n\n\n\n\n\n\nPart 2: Create an Access Key\n#\n\n\nThis step will create an Identity \n Access Management (IAM) access key for your AWS account. \nYou will use the credentials created in this section when creating, updating, or removing Layer0 instances.\n\n\nTo create an Access Key:\n\n\n\n\n\n\nIn a web browser, login to the \nAWS Console\n.\n\n\n\n\n\n\nClick the \nServices\n dropdown menu in the upper left portion of the console page, then type \nIAM\n in the text box that appears at the top of the page after you click \nServices\n. As you type IAM, a search result will appear below the text box. Click on the IAM service result that appears below the text box.\n\n\n\n\n\n\nIn the left panel, click \nGroups\n, and then confirm that you have a group called \nAdministrators\n. \nNote\nIf the \nAdministrators\n group does not already exist, complete the following steps: \nClick \nCreate New Group\n. Name the new group \nAdministrators\n, and then click \nNext Step\n.\nCheck the \nAdministratorAccess\n policy to attach the Administrator policy to your new group.\nClick \nNext Step\n, and then click \nCreate Group\n.\n\n\n\n\n\n\nIn the left panel, click \nUsers\n.\n\n\n\n\n\n\nClick the \nNew User\n button and enter a unique user name you will use for Layer0. This user name can be used for multiple Layer0 installations. Check the box next to \nProgrammatic access\n, and then click the \nNext: Permissions\n button.\n\n\n\n\n\n\nMake sure the \nAdd user to group\n button is highlighted. Find and check the box next to the group \nAdministrators\n. Click \nNext: Review\n button to continue. This will make your newly created user an administrator for your AWS account, so be sure to keep your security credentials safe!\n\n\n\n\n\n\nReview your choices and then click the \nCreate user\n button.\n\n\n\n\n\n\nOnce your user account has been created, click the \nDownload .csv\n button to save your access and secret key to a CSV file.\n\n\n\n\n\n\nPart 3: Create a new Layer0 Instance\n#\n\n\nNow that you have downloaded Layer0 and configured your AWS account, you can create your Layer0 instance.\nFrom a command prompt, run the following (replacing \ninstance_name\n with a name for your Layer0 instance):\n\n\n$ l0-setup init \ninstance_name\n\n\n\n\n\n\nThis command will prompt you for many different inputs. \nEnter the required values for \nAWS Access Key\n, \nAWS Secret Key\n, and \nAWS SSH Key\n as they come up.\nAll remaining inputs are optional and can be set to their default by pressing enter.\n\n\n...\nAWS Access Key: The access_key input variable is used to provision the AWS resources\nrequired for Layer0. This corresponds to the Access Key ID portion of an AWS Access Key.\nIt is recommended this key has the \nAdministratorAccess\n policy. Note that Layer0 will\nonly use this key for \nl0-setup\n commands associated with this Layer0 instance; the\nLayer0 API will use its own key with limited permissions to provision AWS resources.\n\n[current: \nnone\n]\nPlease enter a value and press \nenter\n.\n        Input: ABC123xzy\n\nAWS Secret Key: The secret_key input variable is used to provision the AWS resources\nrequired for Layer0. This corresponds to the Secret Access Key portion of an AWS Access Key.\nIt is recommended this key has the \nAdministratorAccess\n policy. Note that Layer0 will\nonly use this key for \nl0-setup\n commands associated with this Layer0 instance; the\nLayer0 API will use its own key with limited permissions to provision AWS resources.\n\n[current: \nnone\n]\nPlease enter a value and press \nenter\n.\n        Input: ZXY987cba\n\nAWS SSH Key Pair: The ssh_key_pair input variable specifies the name of the\nssh key pair to include in EC2 instances provisioned by Layer0. This key pair must\nalready exist in the AWS account. The names of existing key pairs can be found\nin the EC2 dashboard. Note that changing this value will not effect instances\nthat have already been provisioned.\n\n[current: \nnone\n]\nPlease enter a value and press \nenter\n.\n        Input: mySSHKey\n...\n\n\n\n\n\nOnce the \ninit\n command has successfully completed, you're ready to actually create the resources needed to use Layer0.\nRun the following command (again, replace \ninstance_name\n with the name you've chosen for your Layer0 instance):\n\n\nl0-setup apply \ninstance_name\n\n\n\n\n\n\nThe first time you run the \napply\n command, it may take around 5 minutes to complete. \nThis command is idempotent; it is safe to run multiple times if it fails the first.\n\n\nAt the end of the apply command, your Layer0 instance's configuration and state will be automatically backed up to an S3 bucket. You can manually back up your configuration at any time using the \npush\n command. It's a good idea to run this command regularly (\nl0-setup push \ninstance_name\n) to ensure that your configuration is backed up.\nThese files can be downloaded at any time using the \npull\n command (\nl0-setup pull \ninstance_name\n).\n\n\n\n\nUsing a Private Docker Registry\n\n\nThe procedures in this section are optional, but are highly recommended for production use.\n\n\n\n\nIf you require authentication to a private Docker registry, you will need a Docker configuration file present on your machine with access to private repositories (typically located at \n~/.docker/config.json\n). \n\n\nIf you don't have a config file yet, you can generate one by running \ndocker login [registry-address]\n. \nA configuration file will be generated at \n~/.docker/config.json\n.\n\n\nTo add this authentication to your Layer0 instance, run:\n\n\n$ l0-setup init --docker-path\n=\npath/to/config.json\n \ninstance_name\n\n\n\n\n\n\nThis will add a rendered file into your Layer0 instance's directory at \n~/.layer0/\ninstance_name\n/dockercfg.json\n.\n\n\nYou can modify a Layer0 instance's \ndockercfg.json\n file and re-run the \napply\n command (\nl0-setup apply \ninstance_name\n) to make changes to your authentication. \n\nNote:\n Any EC2 instances created prior to changing your \ndockercfg.json\n file will need to be manually terminated since they only grab the authentication file during instance creation. \nTerminated EC2 instances will be automatically re-created by autoscaling.\n\n\n\n\nUsing an Existing VPC\n\n\nThe procedures in this section must be followed precisely to properly install Layer0 into an existing VPC\n\n\n\n\nBy default, l0-setup creates a new VPC to place resources. \nHowever, l0-setup can place resources in an existing VPC if the VPC meets all of the following conditions:\n\n\n\n\nHas access to the public internet (through a NAT instance or gateway)\n\n\nHas at least 1 public and 1 private subnet\n\n\nThe public and private subnets have the tag \nTier: Public\n or \nTier: Private\n, respectively.\nFor information on how to tag AWS resources, please visit the \nAWS documentation\n. \n\n\n\n\nOnce you are sure the existing VPC satisfies these requirements, run the \ninit\n command, \nplacing the VPC ID when prompted:\n\n\n$ l0-setup init \ninstance_name\n\n...\nVPC ID \n(\noptional\n)\n: The vpc_id input variable specifies an existing AWS VPC to provision\nthe AWS resources required \nfor\n Layer0. If no input is specified, a new VPC will be\ncreated \nfor\n you. Existing VPCs must satisfy the following constraints:\n\n    - Have access to the public internet \n(\nthrough a NAT instance or gateway\n)\n\n    - Have at least \n1\n public and \n1\n private subnet\n    - Each subnet must be tagged with \n[\nTier\n: \nPrivate\n]\n or \n[\nTier\n: \nPublic\n]\n\n\nNote that changing this value will destroy and recreate any existing resources.\n\n\n[\ncurrent: \n]\n\nPlease enter a new value, or press \nenter\n to keep the current value.\n        Input: vpc123\n\n\n\n\n\nOnce the command has completed, it is safe to run \napply\n to provision the resources. \n\n\nPart 4: Connect to a Layer0 Instance\n#\n\n\nOnce the \napply\n command has run successfully, you can configure the environment variables needed to connect to the Layer0 API using the \nendpoint\n command.\n\n\n$ l0-setup endpoint --insecure \ninstance_name\n\n\nexport\n \nLAYER0_API_ENDPOINT\n=\nhttps://l0-instance_name-api-123456.us-west-2.elb.amazonaws.com\n\n\nexport\n \nLAYER0_AUTH_TOKEN\n=\nabcDEFG123\n\n\nexport\n \nLAYER0_SKIP_SSL_VERIFY\n=\n1\n\n\nexport\n \nLAYER0_SKIP_VERSION_VERIFY\n=\n1\n\n\n\n\n\n\nThe \n--insecure\n flag shows configurations that bypass SSL and version verifications. \nThis is required as the Layer0 API created uses a self-signed SSL certificate by default.\nThese settings are \nnot\n recommended for production use!\n\n\nThe \nendpoint\n command supports a \n--syntax\n option, which can be used to turn configuration into a single line:\n\n\n\n\nBash (default) - \n$ eval \"$(l0-setup endpoint --insecure \ninstance_name\n)\"\n\n\nPowershell - \n$ l0-setup endpoint --insecure --syntax=powershell \ninstance_name\n | Out-String | Invoke-Expression", 
            "title": "Install"
        }, 
        {
            "location": "/setup/install/#create-a-new-layer0-instance", 
            "text": "", 
            "title": "Create a new Layer0 Instance"
        }, 
        {
            "location": "/setup/install/#prerequisites", 
            "text": "Before you can install and configure Layer0, you must obtain the following:   An AWS account. \nPlease have your AWS access key and secret ready.  If the aws-cli is installed you can check them here  cat ~/.aws/credentials   If it's not installed assign them with  l0-setup init --aws-access-key  value  --aws-secret-key  value    An EC2 Key Pair. \nThis key pair allows you to access the EC2 instances running your Services using SSH.\nIf you have already created a key pair, you can use it for this process.\nOtherwise, follow the  instructions at aws.amazon.com  to create a new key pair.\nMake a note of the name that you selected when creating the key pair.    Terraform v0.11++ \nWe use Terraform to create the resources that Layer0 needs.\nIf you're unfamiliar with Terraform, you may want to check out our  introduction .\nIf you're ready to install Terraform, there are instructions in the  Terraform documentation .", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/setup/install/#part-1-download-and-extract-layer0", 
            "text": "In the  Downloads section of the home page , select the appropriate installation file for your operating system. Extract the zip file to a directory on your computer.  (Optional) Place the  l0  and  l0-setup  binaries into your system path. \nFor more information about adding directories to your system path, see the following resources:  (Windows):  How to Edit Your System PATH for Easy Command Line Access in Windows  (Linux/macOS):  Adding a Directory to the Path", 
            "title": "Part 1: Download and extract Layer0"
        }, 
        {
            "location": "/setup/install/#part-2-create-an-access-key", 
            "text": "This step will create an Identity   Access Management (IAM) access key for your AWS account. \nYou will use the credentials created in this section when creating, updating, or removing Layer0 instances.  To create an Access Key:    In a web browser, login to the  AWS Console .    Click the  Services  dropdown menu in the upper left portion of the console page, then type  IAM  in the text box that appears at the top of the page after you click  Services . As you type IAM, a search result will appear below the text box. Click on the IAM service result that appears below the text box.    In the left panel, click  Groups , and then confirm that you have a group called  Administrators .  Note If the  Administrators  group does not already exist, complete the following steps:  Click  Create New Group . Name the new group  Administrators , and then click  Next Step . Check the  AdministratorAccess  policy to attach the Administrator policy to your new group. Click  Next Step , and then click  Create Group .    In the left panel, click  Users .    Click the  New User  button and enter a unique user name you will use for Layer0. This user name can be used for multiple Layer0 installations. Check the box next to  Programmatic access , and then click the  Next: Permissions  button.    Make sure the  Add user to group  button is highlighted. Find and check the box next to the group  Administrators . Click  Next: Review  button to continue. This will make your newly created user an administrator for your AWS account, so be sure to keep your security credentials safe!    Review your choices and then click the  Create user  button.    Once your user account has been created, click the  Download .csv  button to save your access and secret key to a CSV file.", 
            "title": "Part 2: Create an Access Key"
        }, 
        {
            "location": "/setup/install/#part-3-create-a-new-layer0-instance", 
            "text": "Now that you have downloaded Layer0 and configured your AWS account, you can create your Layer0 instance.\nFrom a command prompt, run the following (replacing  instance_name  with a name for your Layer0 instance):  $ l0-setup init  instance_name   This command will prompt you for many different inputs. \nEnter the required values for  AWS Access Key ,  AWS Secret Key , and  AWS SSH Key  as they come up.\nAll remaining inputs are optional and can be set to their default by pressing enter.  ...\nAWS Access Key: The access_key input variable is used to provision the AWS resources\nrequired for Layer0. This corresponds to the Access Key ID portion of an AWS Access Key.\nIt is recommended this key has the  AdministratorAccess  policy. Note that Layer0 will\nonly use this key for  l0-setup  commands associated with this Layer0 instance; the\nLayer0 API will use its own key with limited permissions to provision AWS resources.\n\n[current:  none ]\nPlease enter a value and press  enter .\n        Input: ABC123xzy\n\nAWS Secret Key: The secret_key input variable is used to provision the AWS resources\nrequired for Layer0. This corresponds to the Secret Access Key portion of an AWS Access Key.\nIt is recommended this key has the  AdministratorAccess  policy. Note that Layer0 will\nonly use this key for  l0-setup  commands associated with this Layer0 instance; the\nLayer0 API will use its own key with limited permissions to provision AWS resources.\n\n[current:  none ]\nPlease enter a value and press  enter .\n        Input: ZXY987cba\n\nAWS SSH Key Pair: The ssh_key_pair input variable specifies the name of the\nssh key pair to include in EC2 instances provisioned by Layer0. This key pair must\nalready exist in the AWS account. The names of existing key pairs can be found\nin the EC2 dashboard. Note that changing this value will not effect instances\nthat have already been provisioned.\n\n[current:  none ]\nPlease enter a value and press  enter .\n        Input: mySSHKey\n...  Once the  init  command has successfully completed, you're ready to actually create the resources needed to use Layer0.\nRun the following command (again, replace  instance_name  with the name you've chosen for your Layer0 instance):  l0-setup apply  instance_name   The first time you run the  apply  command, it may take around 5 minutes to complete. \nThis command is idempotent; it is safe to run multiple times if it fails the first.  At the end of the apply command, your Layer0 instance's configuration and state will be automatically backed up to an S3 bucket. You can manually back up your configuration at any time using the  push  command. It's a good idea to run this command regularly ( l0-setup push  instance_name ) to ensure that your configuration is backed up.\nThese files can be downloaded at any time using the  pull  command ( l0-setup pull  instance_name ).   Using a Private Docker Registry  The procedures in this section are optional, but are highly recommended for production use.   If you require authentication to a private Docker registry, you will need a Docker configuration file present on your machine with access to private repositories (typically located at  ~/.docker/config.json ).   If you don't have a config file yet, you can generate one by running  docker login [registry-address] . \nA configuration file will be generated at  ~/.docker/config.json .  To add this authentication to your Layer0 instance, run:  $ l0-setup init --docker-path = path/to/config.json   instance_name   This will add a rendered file into your Layer0 instance's directory at  ~/.layer0/ instance_name /dockercfg.json .  You can modify a Layer0 instance's  dockercfg.json  file and re-run the  apply  command ( l0-setup apply  instance_name ) to make changes to your authentication.  Note:  Any EC2 instances created prior to changing your  dockercfg.json  file will need to be manually terminated since they only grab the authentication file during instance creation. \nTerminated EC2 instances will be automatically re-created by autoscaling.   Using an Existing VPC  The procedures in this section must be followed precisely to properly install Layer0 into an existing VPC   By default, l0-setup creates a new VPC to place resources. \nHowever, l0-setup can place resources in an existing VPC if the VPC meets all of the following conditions:   Has access to the public internet (through a NAT instance or gateway)  Has at least 1 public and 1 private subnet  The public and private subnets have the tag  Tier: Public  or  Tier: Private , respectively.\nFor information on how to tag AWS resources, please visit the  AWS documentation .    Once you are sure the existing VPC satisfies these requirements, run the  init  command, \nplacing the VPC ID when prompted:  $ l0-setup init  instance_name \n...\nVPC ID  ( optional ) : The vpc_id input variable specifies an existing AWS VPC to provision\nthe AWS resources required  for  Layer0. If no input is specified, a new VPC will be\ncreated  for  you. Existing VPCs must satisfy the following constraints:\n\n    - Have access to the public internet  ( through a NAT instance or gateway ) \n    - Have at least  1  public and  1  private subnet\n    - Each subnet must be tagged with  [ Tier :  Private ]  or  [ Tier :  Public ] \n\nNote that changing this value will destroy and recreate any existing resources. [ current:  ] \nPlease enter a new value, or press  enter  to keep the current value.\n        Input: vpc123  Once the command has completed, it is safe to run  apply  to provision the resources.", 
            "title": "Part 3: Create a new Layer0 Instance"
        }, 
        {
            "location": "/setup/install/#part-4-connect-to-a-layer0-instance", 
            "text": "Once the  apply  command has run successfully, you can configure the environment variables needed to connect to the Layer0 API using the  endpoint  command.  $ l0-setup endpoint --insecure  instance_name  export   LAYER0_API_ENDPOINT = https://l0-instance_name-api-123456.us-west-2.elb.amazonaws.com  export   LAYER0_AUTH_TOKEN = abcDEFG123  export   LAYER0_SKIP_SSL_VERIFY = 1  export   LAYER0_SKIP_VERSION_VERIFY = 1   The  --insecure  flag shows configurations that bypass SSL and version verifications. \nThis is required as the Layer0 API created uses a self-signed SSL certificate by default.\nThese settings are  not  recommended for production use!  The  endpoint  command supports a  --syntax  option, which can be used to turn configuration into a single line:   Bash (default) -  $ eval \"$(l0-setup endpoint --insecure  instance_name )\"  Powershell -  $ l0-setup endpoint --insecure --syntax=powershell  instance_name  | Out-String | Invoke-Expression", 
            "title": "Part 4: Connect to a Layer0 Instance"
        }, 
        {
            "location": "/setup/upgrade/", 
            "text": "Upgrade a Layer0 Instance\n#\n\n\nThis section provides procedures for upgrading your Layer0 installation to the latest version.\nThis assumes you are using Layer0 version \nv0.10.0\n or later. \n\n\n\n\nWarning\n\n\nLayer0 does not support updating MAJOR or MINOR versions in place unless explicitly stated otherwise.\nUsers will either need to create a new Layer0 instance and migrate to it or destroy and re-create their Layer0 instance in these circumstances.\n\n\n\n\nRun the \nupgrade\n command, replacing \ninstance_name\n and \nversion\n with the name of the Layer0 instance and new version, respectively:\n\n\n$ l0-setup upgrade \ninstance_name\n \nversion\n\n\n\n\n\n\nThis will prompt you about the updated \nsource\n and \nversion\n inputs changing. \nIf you are not satisfied with the changes, exit the application during the prompts. \nFor full control on changing inputs, please use the \nset\n command. \n\n\nExample Usage\n\n\n$ l0-setup upgrade mylayer0 v0.10.1\nThis will update the \nversion\n input\n        From: \n[\nv0.10.0\n]\n\n        To:   \n[\nv0.10.1\n]\n\n\n        Press \nenter\n to accept this change:\nThis will update the \nsource\n input\n        From: \n[\ngithub.com/quintilesims/layer0//setup/module?ref\n=\nv0.10.0\n]\n\n        To:   \n[\ngithub.com/quintilesims/layer0//setup/module?ref\n=\nv0.10.1\n]\n\n\n        Press \nenter\n to accept this change:\n        ...\n\nEverything looks good! You are now ready to run \nl0-setup apply mylayer0\n\n\n\n\n\n\nAs stated by the command output, run the \napply\n command to apply the changes to the Layer0 instance.\nIf any errors occur, please contact the Layer0 team.", 
            "title": "Upgrade"
        }, 
        {
            "location": "/setup/upgrade/#upgrade-a-layer0-instance", 
            "text": "This section provides procedures for upgrading your Layer0 installation to the latest version.\nThis assumes you are using Layer0 version  v0.10.0  or later.    Warning  Layer0 does not support updating MAJOR or MINOR versions in place unless explicitly stated otherwise.\nUsers will either need to create a new Layer0 instance and migrate to it or destroy and re-create their Layer0 instance in these circumstances.   Run the  upgrade  command, replacing  instance_name  and  version  with the name of the Layer0 instance and new version, respectively:  $ l0-setup upgrade  instance_name   version   This will prompt you about the updated  source  and  version  inputs changing. \nIf you are not satisfied with the changes, exit the application during the prompts. \nFor full control on changing inputs, please use the  set  command.   Example Usage  $ l0-setup upgrade mylayer0 v0.10.1\nThis will update the  version  input\n        From:  [ v0.10.0 ] \n        To:    [ v0.10.1 ] \n\n        Press  enter  to accept this change:\nThis will update the  source  input\n        From:  [ github.com/quintilesims/layer0//setup/module?ref = v0.10.0 ] \n        To:    [ github.com/quintilesims/layer0//setup/module?ref = v0.10.1 ] \n\n        Press  enter  to accept this change:\n        ...\n\nEverything looks good! You are now ready to run  l0-setup apply mylayer0   As stated by the command output, run the  apply  command to apply the changes to the Layer0 instance.\nIf any errors occur, please contact the Layer0 team.", 
            "title": "Upgrade a Layer0 Instance"
        }, 
        {
            "location": "/setup/destroy/", 
            "text": "Destroying a Layer0 Instance\n#\n\n\nThis section provides procedures for destroying (deleting) a Layer0 instance.\n\n\nPart 1: Clean Up Your Layer0 Environments\n#\n\n\nIn order to destroy a Layer0 instance, you must first delete all environments in the instance.\nList all environments with:\n\n\n$ l0 environment list\n\n\n\n\n\nFor each environment listed in the previous step, with the exception of the environment named \napi\n, \nissue the following command (replacing \nenvironment_name\n with the name of the environment to delete):\n\n\nl0 environment delete --wait \nenvironment_name\n\n\n\n\n\n\nPart 2: Destroy the Layer0 Instance\n#\n\n\nOnce all environments have been deleted, the Layer0 instance can be deleted using the \nl0-setup\n tool. \nRun the following command (replacing \ninstance_name\n with the name of the Layer0 instance):\n\n\n$ l0-setup destroy \ninstance_name\n\n\n\n\n\n\nThe \ndestroy\n command is idempotent; if it fails, it is safe to re-attempt multiple times. \nIf the  operation continues to fail, it is likely there are resources that were created outside of Layer0 that have dependencies on the resources \nl0-setup\n is attempting to destroy. \nYou will need to manually remove these dependencies in order to get the \ndestroy\n command to complete successfully.", 
            "title": "Destroy"
        }, 
        {
            "location": "/setup/destroy/#destroying-a-layer0-instance", 
            "text": "This section provides procedures for destroying (deleting) a Layer0 instance.", 
            "title": "Destroying a Layer0 Instance"
        }, 
        {
            "location": "/setup/destroy/#part-1-clean-up-your-layer0-environments", 
            "text": "In order to destroy a Layer0 instance, you must first delete all environments in the instance.\nList all environments with:  $ l0 environment list  For each environment listed in the previous step, with the exception of the environment named  api , \nissue the following command (replacing  environment_name  with the name of the environment to delete):  l0 environment delete --wait  environment_name", 
            "title": "Part 1: Clean Up Your Layer0 Environments"
        }, 
        {
            "location": "/setup/destroy/#part-2-destroy-the-layer0-instance", 
            "text": "Once all environments have been deleted, the Layer0 instance can be deleted using the  l0-setup  tool. \nRun the following command (replacing  instance_name  with the name of the Layer0 instance):  $ l0-setup destroy  instance_name   The  destroy  command is idempotent; if it fails, it is safe to re-attempt multiple times. \nIf the  operation continues to fail, it is likely there are resources that were created outside of Layer0 that have dependencies on the resources  l0-setup  is attempting to destroy. \nYou will need to manually remove these dependencies in order to get the  destroy  command to complete successfully.", 
            "title": "Part 2: Destroy the Layer0 Instance"
        }, 
        {
            "location": "/guides/walkthrough/intro/", 
            "text": "An Iterative Walkthrough\n#\n\n\nThis guide aims to take you through three increasingly-complex deployment examples using Layer0.\nSuccessive sections build upon the previous ones, and each deployment can be completed either through the Layer0 CLI directly, or through Terraform using our custom \nLayer0 Terraform Provider\n.\n\n\nWe assume that you're using Layer0 v0.9.0 or later.\nIf you have not already installed and configured Layer0, see the \ninstallation guide\n.\nIf you are running an older version of Layer0, you may need to \nupgrade\n.\n\n\nIf you intend to deploy services using the Layer0 Terraform Provider, you'll want to make sure that you've \ninstalled\n the provider correctly.\n\n\nRegardless of the deployment method you choose, we maintain a \nguides repository\n that you should clone/download.\nIt contains all the files you will need to progress through this walkthrough.\nAs you do so, we will assume that your working directory matches the part of the guide that you're following (for example, Deployment 1 of this guide will assume that your working directory is \n.../walkthrough/deployment-1/\n).\n\n\nTable of Contents\n:\n\n\n\n\nDeployment 1\n: Deploying a web service (Guestbook)\n\n\nDeployment 2\n: Deploying Guestbook and a data store service (Redis)\n\n\nDeployment 3\n: Deploying Guestbook, Redis, and a service discovery service (Consul)", 
            "title": "Walkthrough: Introduction"
        }, 
        {
            "location": "/guides/walkthrough/intro/#an-iterative-walkthrough", 
            "text": "This guide aims to take you through three increasingly-complex deployment examples using Layer0.\nSuccessive sections build upon the previous ones, and each deployment can be completed either through the Layer0 CLI directly, or through Terraform using our custom  Layer0 Terraform Provider .  We assume that you're using Layer0 v0.9.0 or later.\nIf you have not already installed and configured Layer0, see the  installation guide .\nIf you are running an older version of Layer0, you may need to  upgrade .  If you intend to deploy services using the Layer0 Terraform Provider, you'll want to make sure that you've  installed  the provider correctly.  Regardless of the deployment method you choose, we maintain a  guides repository  that you should clone/download.\nIt contains all the files you will need to progress through this walkthrough.\nAs you do so, we will assume that your working directory matches the part of the guide that you're following (for example, Deployment 1 of this guide will assume that your working directory is  .../walkthrough/deployment-1/ ).  Table of Contents :   Deployment 1 : Deploying a web service (Guestbook)  Deployment 2 : Deploying Guestbook and a data store service (Redis)  Deployment 3 : Deploying Guestbook, Redis, and a service discovery service (Consul)", 
            "title": "An Iterative Walkthrough"
        }, 
        {
            "location": "/guides/walkthrough/deployment-1/", 
            "text": "Deployment 1: A Simple Guestbook App\n#\n\n\nIn this section you'll learn how different Layer0 commands work together to deploy applications to the cloud.\nThe example application in this section is a guestbook -- a web application that acts as a simple message board.\nYou can choose to complete this section using either \nthe Layer0 CLI\n or \nTerraform\n.\n\n\n\n\nDeploy with Layer0 CLI\n#\n\n\nIf you're following along, you'll want to be working in the \nwalkthrough/deployment-1/\n directory of your clone of the \nguides\n repo.\n\n\nFiles used in this deployment:\n\n\n\n\n\n\n\n\nFilename\n\n\nPurpose\n\n\n\n\n\n\n\n\n\n\nGuestbook.Dockerrun.aws.json\n\n\nTemplate for running the Guestbook application\n\n\n\n\n\n\n\n\n\n\nPart 1: Create the Environment\n#\n\n\nThe first step in deploying an application with Layer0 is to create an environment.\nAn environment is a dedicated space in which one or more services can reside.\nHere, we'll create a new environment named \ndemo-env\n.\nAt the command prompt, execute the following:\n\n\nl0 environment create demo-env\n\n\nWe should see output like the following:\n\n\nENVIRONMENT ID  ENVIRONMENT NAME  CLUSTER COUNT  INSTANCE SIZE  LINKS\ndemo00e6aa9     demo-env          0              m3.medium\n\n\n\n\n\nWe can inspect our environments in a couple of different ways:\n\n\n\n\nl0 environment list\n will give us a brief summary of all environments:\n\n\n\n\nENVIRONMENT ID  ENVIRONMENT NAME\ndemo00e6aa9     demo-env\napi             api\n\n\n\n\n\n\n\nl0 environment get demo-env\n will show us more information about the \ndemo-env\n environment we just created:\n\n\n\n\nENVIRONMENT ID  ENVIRONMENT NAME  CLUSTER COUNT  INSTANCE SIZE  LINKS\ndemo00e6aa9     demo-env          0              m3.medium\n\n\n\n\n\n\n\nl0 environment get \\*\n illustrates wildcard matching (you could also have used \ndemo*\n in the above command), and it will return detailed information for \neach\n environment, not just one - it's like a detailed \nlist\n:\n\n\n\n\nENVIRONMENT ID  ENVIRONMENT NAME  CLUSTER COUNT  INSTANCE SIZE  LINKS\ndemo00e6aa9     demo-env          0              m3.medium\napi             api               2              m3.medium\n\n\n\n\n\n\n\nPart 2: Create the Load Balancer\n#\n\n\nIn order to expose a web application to the public internet, we need to create a load balancer.\nA load balancer listens for web traffic at a specific address and directs that traffic to a Layer0 service.\n\n\nA load balancer also has a notion of a health check - a way to assess whether or not the service is healthy and running properly.\nBy default, Layer0 configures the health check of a load balancer based upon a simple TCP ping to port 80 every thirty seconds.\nAlso by default, this ping will timeout after five seconds of no response from the service, and two consecutive successes or failures are required for the service to be considered healthy or unhealthy.\n\n\nHere, we'll create a new load balancer named \nguestbook-lb\n inside of our environment named \ndemo-env\n.\nThe load balancer will listen on port 80, and forward that traffic along to port 80 in the Docker container using the HTTP protocol.\nSince the port configuration is already aligned with the default health check, we don't need to specify any health check configuration when we create this load balancer.\nAt the command prompt, execute the following:\n\n\nl0 loadbalancer create --port 80:80/http demo-env guestbook-lb\n\n\nWe should see output like the following:\n\n\nLOADBALANCER ID  LOADBALANCER NAME  ENVIRONMENT  SERVICE  PORTS       PUBLIC  URL\nguestbodb65a     guestbook-lb       demo-env              80:80/HTTP  true\n\n\n\n\n\nThe following is a summary of the arguments passed in the above command:\n\n\n\n\nloadbalancer create\n: creates a new load balancer\n\n\n--port 80:80/HTTP\n: instructs the load balancer to forward requests from port 80 on the load balancer to port 80 in the EC2 instance using the HTTP protocol\n\n\ndemo-env\n: the name of the environment in which you are creating the load balancer\n\n\nguestbook-lb\n: a name for the load balancer itself\n\n\n\n\nYou can inspect load balancers in the same way that you inspected environments in Part 1.\nTry running the following commands to get an idea of the information available to you:\n\n\n\n\nl0 loadbalancer list\n\n\nl0 loadbalancer get guestbook-lb\n\n\nl0 loadbalancer get gues*\n\n\nl0 loadbalancer get \\*\n\n\n\n\n\n\nNote\n\n\nNotice that the load balancer \nlist\n and \nget\n outputs list an \nENVIRONMENT\n field - if you ever have load balancers (or other Layer0 entities) with the same name but in different environments, you can target a specific load balancer by qualifying it with its environment name:\n\n\n`l0 loadbalancer get demo-env:guestbook-lb`\n\n\n\n\n\n\nPart 3: Deploy the ECS Task Definition\n#\n\n\nThe \ndeploy\n command is used to specify the ECS task definition that outlines a web application.\nA deploy, once created, can be applied to multiple services - even across different environments!\n\n\nHere, we'll create a new deploy called \nguestbook-dpl\n that refers to the \nGuestbook.Dockerrun.aws.json\n file found in the guides reposiory.\nAt the command prompt, execute the following:\n\n\nl0 deploy create Guestbook.Dockerrun.aws.json guestbook-dpl\n\n\nWe should see output like the following:\n\n\nDEPLOY ID        DEPLOY NAME    VERSION\nguestbook-dpl.1  guestbook-dpl  1\n\n\n\n\n\nThe following is a summary of the arguments passed in the above command:\n\n\n\n\ndeploy create\n: creates a new deployment and allows you to specify an ECS task definition\n\n\nGuestbook.Dockerrun.aws.json\n: the file name of the ECS task definition (use the full path of the file if it is not in your current working directory)\n\n\nguestbook-dpl\n: a name for the deploy, which you will use later when you create the service\n\n\n\n\n\n\nDeploy Versioning\n\n\nThe \nDEPLOY NAME\n and \nVERSION\n are combined to create a unique identifier for a deploy.\nIf you create additional deploys named \nguestbook-dpl\n, they will be assigned different version numbers.\n\n\nYou can always specify the latest version when targeting a deploy by using \ndeploy name\n:latest\n -- for example, \nguestbook-dpl:latest\n.\n\n\n\n\nDeploys support the same methods of inspection as environments and load balancers:\n\n\n\n\nl0 deploy list\n\n\nl0 deploy get guestbook*\n\n\nl0 deploy get guestbook:1\n\n\nl0 deploy get guestbook:latest\n\n\nl0 deploy get \\*\n\n\n\n\n\n\nPart 4: Create the Service\n#\n\n\nThe final stage of the deployment process involves using the \nservice\n command to create a new service and associate it with the environment, load balancer, and deploy that we created in the previous sections.\nThe service will execute the Docker containers which have been described in the deploy.\n\n\nHere, we'll create a new service called \nguestbook-svc\n. At the command prompt, execute the following:\n\n\nl0 service create --loadbalancer demo-env:guestbook-lb demo-env guestbook-svc guestbook-dpl:latest\n\n\nWe should see output like the following:\n\n\nSERVICE ID    SERVICE NAME   ENVIRONMENT  LOADBALANCER  DEPLOYMENTS       SCALE\nguestbo9364b  guestbook-svc  demo-env     guestbook-lb  guestbook-dpl:1*  0/1\n\n\n\n\n\nThe following is a summary of the arguments passed in the above command:\n\n\n\n\nservice create\n: creates a new service\n\n\n--loadbalancer demo-env:guestbook-lb\n: the fully-qualified name of the load balancer; in this case, the load balancer named \nguestbook-lb\n in the environment named \ndemo-env\n. \n\n\n(It is not strictly necessary to use the fully qualified name of the load balancer, unless another load balancer with exactly the same name exists in a different environment.)\n\n\n\n\n\n\ndemo-env\n: the name of the environment you created in Part 1\n\n\nguestbook-svc\n: a name for the service you are creating\n\n\nguestbook-dpl\n: the name of the deploy that you created in Part 3\n\n\n\n\nLayer0 services can be queried using the same \nget\n and \nlist\n commands that we've come to expect by now.\n\n\n\n\nCheck the Status of the Service\n#\n\n\nAfter a service has been created, it may take several minutes for that service to completely finish deploying.\nA service's status may be checked by using the \nservice get\n command.\n\n\nLet's take a peek at our \nguestbook-svc\n service.\nAt the command prompt, execute the following:\n\n\nl0 service get demo-env:guestbook-svc\n\n\nIf we're quick enough, we'll be able to see the first stage of the process (this is what was output after running the \nservice create\n command up in Part 4).\nWe should see an asterisk (*) next to the name of the \nguestbook-dpl:1\n deploy, which indicates that the service is in a transitional state:\n\n\nSERVICE ID    SERVICE NAME   ENVIRONMENT  LOADBALANCER  DEPLOYMENTS       SCALE\nguestbo9364b  guestbook-svc  demo-env     guestbook-lb  guestbook-dpl:1*  0/1\n\n\n\n\n\nIn the next phase of deployment, if we execute the \nservice get\n command again, we will see \n(1)\n in the \nScale\n column; this indicates that 1 copy of the service is transitioning to an active state:\n\n\nSERVICE ID    SERVICE NAME   ENVIRONMENT  LOADBALANCER  DEPLOYMENTS       SCALE\nguestbo9364b  guestbook-svc  demo-env     guestbook-lb  guestbook-dpl:1*  0/1 (1)\n\n\n\n\n\nIn the final phase of deployment, we will see \n1/1\n in the \nScale\n column; this indicates that the service is running 1 copy:\n\n\nSERVICE ID    SERVICE NAME   ENVIRONMENT  LOADBALANCER  DEPLOYMENTS       SCALE\nguestbo9364b  guestbook-svc  demo-env     guestbook-lb  guestbook-dpl:1   1/1\n\n\n\n\n\n\n\nGet the Application's URL\n#\n\n\nOnce the service has been completely deployed, we can obtain the URL for the application and launch it in a browser.\n\n\nAt the command prompt, execute the following:\n\n\nl0 loadbalancer get demo-env:guestbook-lb\n\n\nWe should see output like the following:\n\n\nLOADBALANCER ID  LOADBALANCER NAME  ENVIRONMENT  SERVICE        PORTS       PUBLIC  URL\nguestbodb65a     guestbook-lb       demo-env     guestbook-svc  80:80/HTTP  true    \nurl\n\n\n\n\n\n\nCopy the value shown in the \nURL\n column and paste it into a web browser.\nThe guestbook application will appear (once the service has completely finished deploying).\n\n\n\n\nLogs\n#\n\n\nOutput from a Service's docker containers may be acquired by running the following command:\n\n\nl0 service logs \nSERVICE\n\n\n\n\n\n\n\n\nCleanup\n#\n\n\nIf you're finished with the example and don't want to continue with this walkthrough, you can instruct Layer0 to delete the environment and terminate the application.\n\n\nl0 environment delete demo-env\n\n\nHowever, if you intend to continue through \nDeployment 2\n, you will want to keep the resources you made in this section.\n\n\n\n\nDeploy with Terraform\n#\n\n\nInstead of using the Layer0 CLI directly, you can instead use our Terraform provider, and deploy using Terraform \n(\nlearn more\n)\n.\nYou can use Terraform with Layer0 and AWS to create \"fire-and-forget\" deployments for your applications.\n\n\nIf you're following along, you'll want to be working in the \nwalkthrough/deployment-1/\n directory of your clone of the \nguides\n repo.\n\n\nWe use these files to set up a Layer0 environment with Terraform:\n\n\n\n\n\n\n\n\nFilename\n\n\nPurpose\n\n\n\n\n\n\n\n\n\n\nmain.tf\n\n\nProvisions resources; populates resources in template files\n\n\n\n\n\n\noutputs.tf\n\n\nValues that Terraform will yield during deployment\n\n\n\n\n\n\nterraform.tfstate\n\n\nTracks status of deployment \n(created and managed by Terraform)\n\n\n\n\n\n\nterraform.tfvars\n\n\nVariables specific to the environment and application(s)\n\n\n\n\n\n\nvariables.tf\n\n\nValues that Terraform will use during deployment\n\n\n\n\n\n\n\n\n*.tf\n: A Brief Aside\n#\n\n\nLet's take a moment to discuss the \n.tf\n files.\nThe names of these files (and even the fact that they are separated out into multiple files at all) are completely arbitrary and exist soley for human-readability.\nTerraform understands all \n.tf\n files in a directory all together.\n\n\nIn \nvariables.tf\n, you'll see \n\"endpoint\"\n and \n\"token\"\n variables.\n\n\nIn \noutputs.tf\n, you'll see that Terraform should spit out the url of the guestbook's load balancer once deployment has finished.\n\n\nIn \nmain.tf\n, you'll see the bulk of the deployment process.\nIf you've followed along with the Layer0 CLI deployment above, it should be fairly easy to see how blocks in this file map to steps in the CLI process.\nWhen we began the CLI deployment, our first step was to create an environment:\n\n\nl0 environment create demo-env\n\n\nThis command is recreated in \nmain.tf\n like so:\n\n\n# walkthrough/deployment-1/main.tf\n\nresource \nlayer0_environment\n \ndemo-env\n {\n    name = \ndemo-env\n\n}\n\n\n\n\n\nWe've bundled up the heart of the Guestbook deployment (load balancer, deploy, service, etc.) into a \nTerraform module\n.\nTo use it, we declare a \nmodule\n block and pass in the source of the module as well as any configuration or variables that the module needs.\n\n\n# walkthrough/deployment-1/main.tf\n\nmodule \nguestbook\n {\n    source         = \ngithub.com/quintilesims/guides//guestbook/module\n\n    environment_id = \n${\nlayer0_environment\n.\ndemo\n.\nid\n}\n\n}\n\n\n\n\n\nYou can see that we pass in the ID of the environment we create.\nAll variables declared in this block are passed to the module, so the next file we should look at is \nvariables.tf\n inside of the module to get an idea of what the module is expecting.\n\n\nThere are a lot of variables here, but only one of them doesn't have a default value.\n\n\n# guestbook/module/variables.tf\n\nvariable \nenvironment_id\n {\n    description = \nid of the layer0 environment in which to create resources\n\n}\n\n\n\n\n\nYou'll notice that this is the variable that we're passing in.\nFor this particular deployment of the Guestbook, all of the default options are fine.\nWe could override any of them if we wanted to, just by specifying a new value for them back in \ndeployment-1/main.tf\n.\n\n\nNow that we've seen the variables that the module will have, let's take a look at part of \nmodule/main.tf\n and see how some of them might be used:\n\n\n# guestbook/module/main.tf\n\nresource \nlayer0_load_balancer\n \nguestbook-lb\n {\n    name = \n${\nvar\n.\nload_balancer_name\n}\n\n    environment = \n${\nvar\n.\nenvironment_id\n}\n\n    port {\n        host_port = 80\n        container_port = 80\n        protocol = \nhttp\n\n    }\n}\n\n...\n\n\n\n\n\nYou can follow \nthis link\n to learn more about Layer0 resources in Terraform.\n\n\n\n\nPart 1: Terraform Get\n#\n\n\nThis deployment uses modules, so we'll need to fetch those source materials.\nAt the command prompt, execute the following command:\n\n\nterraform get\n\n\nWe should see output like the following:\n\n\nGet\n:\n \ngit\n::\nhttps\n://\ngithub\n.\ncom\n/quintilesims/g\nuides\n.\ngit\n\n\n\n\n\n\nWe should now have a new local directory called \n.terraform/\n.\nWe don't need to do anything with it; we just want to make sure it's there.\n\n\n\n\nPart 2: Terraform Init\n#\n\n\nThis deployment has provider dependencies so an init call must be made. \n(Terraform v0.11~ requries init)\nAt the command prompt, execute the following command:\n\n\nterraform init\n\n\nWe should see output like the following:\n\n\nInitializing modules...\n- module.guestbook\n\nInitializing provider plugins...\n- Checking for available provider plugins on https://releases.hashicorp.com...\n- Downloading plugin for provider \ntemplate\n (1.0.0)...\n\nThe following providers do not have any version constraints in configuration,\nso the latest version was installed.\n\nTo prevent automatic upgrades to new major versions that may contain breaking\nchanges, it is recommended to add version = \n...\n constraints to the\ncorresponding provider blocks in configuration, with the constraint strings\nsuggested below.\n\n* provider.template: version = \n~\n 1.0\n\n\nTerraform has been successfully initialized!\n\nYou may now begin working with Terraform. Try running \nterraform plan\n to see\nany changes that are required for your infrastructure. All Terraform commands\nshould now work.\n\nIf you ever set or change modules or backend configuration for Terraform,\nrerun this command to reinitialize your working directory. If you forget, other\ncommands will detect it and remind you to do so if necessary.\n\n\n\n\n\n\n\nPart 3: Terraform Plan\n#\n\n\nBefore we actually create/update/delete any resources, it's a good idea to find out what Terraform intends to do.\n\n\nRun \nterraform plan\n. Terraform will prompt you for configuration values that it does not have:\n\n\nvar.endpoint\n    Enter a value:\n\nvar.token\n    Enter a value:\n\n\n\n\n\nYou can find these values by running \nl0-setup endpoint \nyour layer0 prefix\n.\n\n\n\n\nNote\n\n\nThere are a few ways to configure Terraform so that you don't have to keep entering these values every time you run a Terraform command (editing the \nterraform.tfvars\n file, or exporting evironment variables like \nTF_VAR_endpoint\n and \nTF_VAR_token\n, for example). See the \nTerraform Docs\n for more.\n\n\n\n\nThe \nplan\n command should give us output like the following:\n\n\nRefreshing Terraform state in-memory prior to plan...\nThe refreshed state will be used to calculate this plan, but will not be\npersisted to local or remote state storage.\n\ndata.template_file.guestbook: Refreshing state...\nThe Terraform execution plan has been generated and is shown below.\nResources are shown in alphabetical order for quick scanning. Green resources\nwill be created (or destroyed and then created if an existing resource\nexists), yellow resources are being changed in-place, and red resources\nwill be destroyed. Cyan entries are data sources to be read.\n\nNote: You didn\nt specify an \n-out\n parameter to save this plan, so when\n\napply\n is called, Terraform can\nt guarantee this is what will execute.\n\n+ layer0_environment.demo\n    ami:               \ncomputed\n\n    cluster_count:     \ncomputed\n\n    links:             \ncomputed\n\n    name:              \ndemo\n\n    os:                \nlinux\n\n    security_group_id: \ncomputed\n\n    size:              \nm3.medium\n\n\n+ module.guestbook.layer0_deploy.guestbook\n    content: \n{\\n    \\\nAWSEBDockerrunVersion\\\n: 2,\\n    \\\ncontainerDefinitions\\\n: [\\n        {\\n            \\\nname\\\n: \\\nguestbook\\\n,\\n            \\\nimage\\\n: \\\nquintilesims/guestbook\\\n,\\n            \\\nessential\\\n: true,\\n      \\\nmemory\\\n: 128,\\n            \\\nenvironment\\\n: [\\n                {\\n                    \\\nname\\\n: \\\nGUESTBOOK_BACKEND_TYPE\\\n,\\n                    \\\nvalue\\\n: \\\nmemory\\\n\\n                },\\n                {\\n          \\\nname\\\n: \\\nGUESTBOOK_BACKEND_CONFIG\\\n,\\n                    \\\nvalue\\\n: \\\n\\\n\\n                },\\n           {\\n                    \\\nname\\\n: \\\nAWS_ACCESS_KEY_ID\\\n,\\n                    \\\nvalue\\\n: \\\n\\\n\\n        },\\n                {\\n                    \\\nname\\\n: \\\nAWS_SECRET_ACCESS_KEY\\\n,\\n                    \\\nvalue\\\n: \\\n\\\n\\n                },\\n                {\\n                    \\\nname\\\n: \\\nAWS_REGION\\\n,\\n      \\\nvalue\\\n: \\\nus-west-2\\\n\\n                }\\n            ],\\n            \\\nportMappings\\\n: [\\n   {\\n                    \\\nhostPort\\\n: 80,\\n                    \\\ncontainerPort\\\n: 80\\n                }\\n      ]\\n        }\\n    ]\\n}\\n\n\n    name:    \nguestbook\n\n\n+ module.guestbook.layer0_load_balancer.guestbook\n    environment:                    \n${\nvar\n.\nenvironment_id\n}\n\n    health_check.#:                 \ncomputed\n\n    name:                           \nguestbook\n\n    port.#:                         \n1\n\n    port.2027667003.certificate:    \n\n    port.2027667003.container_port: \n80\n\n    port.2027667003.host_port:      \n80\n\n    port.2027667003.protocol:       \nhttp\n\n    url:                            \ncomputed\n\n\n+ module.guestbook.layer0_service.guestbook\n    deploy:        \n${\n \nvar\n.\ndeploy_id\n \n==\n \\\n\\\n ? layer0_deploy.guestbook.id : var.deploy_id \n}\n\n    environment:   \n${\nvar\n.\nenvironment_id\n}\n\n    load_balancer: \n${\nlayer0_load_balancer\n.\nguestbook\n.\nid\n}\n\n    name:          \nguestbook\n\n    scale:         \n1\n\n    wait:          \ntrue\n\n\n\nPlan: 4 to add, 0 to change, 0 to destroy.\n\n\n\n\n\nThis shows you that Terraform intends to create a deploy, an environment, a load balancer, and a service, all through Layer0.\n\n\nIf you've gone through this deployment using the \nLayer0 CLI\n, you may notice that these resources appear out of order - that's fine. Terraform presents these resources in alphabetical order, but underneath, it knows the correct order in which to create them.\n\n\nOnce we're satisfied that Terraform will do what we want it to do, we can move on to actually making these things exist!\n\n\n\n\nPart 4: Terraform Apply\n#\n\n\nRun \nterraform apply\n to begin the process.\n\n\nWe should see output like the following:\n\n\nlayer0_environment.demo: Refreshing state...\n...\n...\n...\nlayer0_service.guestbook: Creation complete\n\nApply complete! Resources: 7 added, 0 changed, 0 destroyed.\n\nThe state of your infrastructure has been saved to the path\nbelow. This state is required to modify and destroy your\ninfrastructure, so keep it safe. To inspect the complete state\nuse the `terraform show` command.\n\nState path: terraform.tfstate\n\nOutputs:\n\nguestbook_url = \nhttp endpoint for the sample application\n\n\n\n\n\n\n\n\nNote\n\n\nIt may take a few minutes for the guestbook service to launch and the load balancer to become available. During that time you may get HTTP 503 errors when making HTTP requests against the load balancer URL.\n\n\n\n\nWhat's Happening\n#\n\n\nTerraform provisions the AWS resources through Layer0, configures environment variables for the application, and deploys the application into a Layer0 environment. Terraform also writes the state of your deployment to the \nterraform.tfstate\n file (creating a new one if it's not already there).\n\n\nCleanup\n#\n\n\nWhen you're finished with the example, you can instruct Terraform to destroy the Layer0 environment, and terminate the application. Execute the following command (in the same directory):\n\n\nterraform destroy\n\n\nIt's also now safe to remove the \n.terraform/\n directory and the \n*.tfstate*\n files.", 
            "title": "Walkthrough: Deployment 1"
        }, 
        {
            "location": "/guides/walkthrough/deployment-1/#deployment-1-a-simple-guestbook-app", 
            "text": "In this section you'll learn how different Layer0 commands work together to deploy applications to the cloud.\nThe example application in this section is a guestbook -- a web application that acts as a simple message board.\nYou can choose to complete this section using either  the Layer0 CLI  or  Terraform .", 
            "title": "Deployment 1: A Simple Guestbook App"
        }, 
        {
            "location": "/guides/walkthrough/deployment-1/#deploy-with-layer0-cli", 
            "text": "If you're following along, you'll want to be working in the  walkthrough/deployment-1/  directory of your clone of the  guides  repo.  Files used in this deployment:     Filename  Purpose      Guestbook.Dockerrun.aws.json  Template for running the Guestbook application", 
            "title": "Deploy with Layer0 CLI"
        }, 
        {
            "location": "/guides/walkthrough/deployment-1/#part-1-create-the-environment", 
            "text": "The first step in deploying an application with Layer0 is to create an environment.\nAn environment is a dedicated space in which one or more services can reside.\nHere, we'll create a new environment named  demo-env .\nAt the command prompt, execute the following:  l0 environment create demo-env  We should see output like the following:  ENVIRONMENT ID  ENVIRONMENT NAME  CLUSTER COUNT  INSTANCE SIZE  LINKS\ndemo00e6aa9     demo-env          0              m3.medium  We can inspect our environments in a couple of different ways:   l0 environment list  will give us a brief summary of all environments:   ENVIRONMENT ID  ENVIRONMENT NAME\ndemo00e6aa9     demo-env\napi             api   l0 environment get demo-env  will show us more information about the  demo-env  environment we just created:   ENVIRONMENT ID  ENVIRONMENT NAME  CLUSTER COUNT  INSTANCE SIZE  LINKS\ndemo00e6aa9     demo-env          0              m3.medium   l0 environment get \\*  illustrates wildcard matching (you could also have used  demo*  in the above command), and it will return detailed information for  each  environment, not just one - it's like a detailed  list :   ENVIRONMENT ID  ENVIRONMENT NAME  CLUSTER COUNT  INSTANCE SIZE  LINKS\ndemo00e6aa9     demo-env          0              m3.medium\napi             api               2              m3.medium", 
            "title": "Part 1: Create the Environment"
        }, 
        {
            "location": "/guides/walkthrough/deployment-1/#part-2-create-the-load-balancer", 
            "text": "In order to expose a web application to the public internet, we need to create a load balancer.\nA load balancer listens for web traffic at a specific address and directs that traffic to a Layer0 service.  A load balancer also has a notion of a health check - a way to assess whether or not the service is healthy and running properly.\nBy default, Layer0 configures the health check of a load balancer based upon a simple TCP ping to port 80 every thirty seconds.\nAlso by default, this ping will timeout after five seconds of no response from the service, and two consecutive successes or failures are required for the service to be considered healthy or unhealthy.  Here, we'll create a new load balancer named  guestbook-lb  inside of our environment named  demo-env .\nThe load balancer will listen on port 80, and forward that traffic along to port 80 in the Docker container using the HTTP protocol.\nSince the port configuration is already aligned with the default health check, we don't need to specify any health check configuration when we create this load balancer.\nAt the command prompt, execute the following:  l0 loadbalancer create --port 80:80/http demo-env guestbook-lb  We should see output like the following:  LOADBALANCER ID  LOADBALANCER NAME  ENVIRONMENT  SERVICE  PORTS       PUBLIC  URL\nguestbodb65a     guestbook-lb       demo-env              80:80/HTTP  true  The following is a summary of the arguments passed in the above command:   loadbalancer create : creates a new load balancer  --port 80:80/HTTP : instructs the load balancer to forward requests from port 80 on the load balancer to port 80 in the EC2 instance using the HTTP protocol  demo-env : the name of the environment in which you are creating the load balancer  guestbook-lb : a name for the load balancer itself   You can inspect load balancers in the same way that you inspected environments in Part 1.\nTry running the following commands to get an idea of the information available to you:   l0 loadbalancer list  l0 loadbalancer get guestbook-lb  l0 loadbalancer get gues*  l0 loadbalancer get \\*    Note  Notice that the load balancer  list  and  get  outputs list an  ENVIRONMENT  field - if you ever have load balancers (or other Layer0 entities) with the same name but in different environments, you can target a specific load balancer by qualifying it with its environment name:  `l0 loadbalancer get demo-env:guestbook-lb`", 
            "title": "Part 2: Create the Load Balancer"
        }, 
        {
            "location": "/guides/walkthrough/deployment-1/#part-3-deploy-the-ecs-task-definition", 
            "text": "The  deploy  command is used to specify the ECS task definition that outlines a web application.\nA deploy, once created, can be applied to multiple services - even across different environments!  Here, we'll create a new deploy called  guestbook-dpl  that refers to the  Guestbook.Dockerrun.aws.json  file found in the guides reposiory.\nAt the command prompt, execute the following:  l0 deploy create Guestbook.Dockerrun.aws.json guestbook-dpl  We should see output like the following:  DEPLOY ID        DEPLOY NAME    VERSION\nguestbook-dpl.1  guestbook-dpl  1  The following is a summary of the arguments passed in the above command:   deploy create : creates a new deployment and allows you to specify an ECS task definition  Guestbook.Dockerrun.aws.json : the file name of the ECS task definition (use the full path of the file if it is not in your current working directory)  guestbook-dpl : a name for the deploy, which you will use later when you create the service    Deploy Versioning  The  DEPLOY NAME  and  VERSION  are combined to create a unique identifier for a deploy.\nIf you create additional deploys named  guestbook-dpl , they will be assigned different version numbers.  You can always specify the latest version when targeting a deploy by using  deploy name :latest  -- for example,  guestbook-dpl:latest .   Deploys support the same methods of inspection as environments and load balancers:   l0 deploy list  l0 deploy get guestbook*  l0 deploy get guestbook:1  l0 deploy get guestbook:latest  l0 deploy get \\*", 
            "title": "Part 3: Deploy the ECS Task Definition"
        }, 
        {
            "location": "/guides/walkthrough/deployment-1/#part-4-create-the-service", 
            "text": "The final stage of the deployment process involves using the  service  command to create a new service and associate it with the environment, load balancer, and deploy that we created in the previous sections.\nThe service will execute the Docker containers which have been described in the deploy.  Here, we'll create a new service called  guestbook-svc . At the command prompt, execute the following:  l0 service create --loadbalancer demo-env:guestbook-lb demo-env guestbook-svc guestbook-dpl:latest  We should see output like the following:  SERVICE ID    SERVICE NAME   ENVIRONMENT  LOADBALANCER  DEPLOYMENTS       SCALE\nguestbo9364b  guestbook-svc  demo-env     guestbook-lb  guestbook-dpl:1*  0/1  The following is a summary of the arguments passed in the above command:   service create : creates a new service  --loadbalancer demo-env:guestbook-lb : the fully-qualified name of the load balancer; in this case, the load balancer named  guestbook-lb  in the environment named  demo-env .   (It is not strictly necessary to use the fully qualified name of the load balancer, unless another load balancer with exactly the same name exists in a different environment.)    demo-env : the name of the environment you created in Part 1  guestbook-svc : a name for the service you are creating  guestbook-dpl : the name of the deploy that you created in Part 3   Layer0 services can be queried using the same  get  and  list  commands that we've come to expect by now.", 
            "title": "Part 4: Create the Service"
        }, 
        {
            "location": "/guides/walkthrough/deployment-1/#check-the-status-of-the-service", 
            "text": "After a service has been created, it may take several minutes for that service to completely finish deploying.\nA service's status may be checked by using the  service get  command.  Let's take a peek at our  guestbook-svc  service.\nAt the command prompt, execute the following:  l0 service get demo-env:guestbook-svc  If we're quick enough, we'll be able to see the first stage of the process (this is what was output after running the  service create  command up in Part 4).\nWe should see an asterisk (*) next to the name of the  guestbook-dpl:1  deploy, which indicates that the service is in a transitional state:  SERVICE ID    SERVICE NAME   ENVIRONMENT  LOADBALANCER  DEPLOYMENTS       SCALE\nguestbo9364b  guestbook-svc  demo-env     guestbook-lb  guestbook-dpl:1*  0/1  In the next phase of deployment, if we execute the  service get  command again, we will see  (1)  in the  Scale  column; this indicates that 1 copy of the service is transitioning to an active state:  SERVICE ID    SERVICE NAME   ENVIRONMENT  LOADBALANCER  DEPLOYMENTS       SCALE\nguestbo9364b  guestbook-svc  demo-env     guestbook-lb  guestbook-dpl:1*  0/1 (1)  In the final phase of deployment, we will see  1/1  in the  Scale  column; this indicates that the service is running 1 copy:  SERVICE ID    SERVICE NAME   ENVIRONMENT  LOADBALANCER  DEPLOYMENTS       SCALE\nguestbo9364b  guestbook-svc  demo-env     guestbook-lb  guestbook-dpl:1   1/1", 
            "title": "Check the Status of the Service"
        }, 
        {
            "location": "/guides/walkthrough/deployment-1/#get-the-applications-url", 
            "text": "Once the service has been completely deployed, we can obtain the URL for the application and launch it in a browser.  At the command prompt, execute the following:  l0 loadbalancer get demo-env:guestbook-lb  We should see output like the following:  LOADBALANCER ID  LOADBALANCER NAME  ENVIRONMENT  SERVICE        PORTS       PUBLIC  URL\nguestbodb65a     guestbook-lb       demo-env     guestbook-svc  80:80/HTTP  true     url   Copy the value shown in the  URL  column and paste it into a web browser.\nThe guestbook application will appear (once the service has completely finished deploying).", 
            "title": "Get the Application's URL"
        }, 
        {
            "location": "/guides/walkthrough/deployment-1/#logs", 
            "text": "Output from a Service's docker containers may be acquired by running the following command:  l0 service logs  SERVICE", 
            "title": "Logs"
        }, 
        {
            "location": "/guides/walkthrough/deployment-1/#cleanup", 
            "text": "If you're finished with the example and don't want to continue with this walkthrough, you can instruct Layer0 to delete the environment and terminate the application.  l0 environment delete demo-env  However, if you intend to continue through  Deployment 2 , you will want to keep the resources you made in this section.", 
            "title": "Cleanup"
        }, 
        {
            "location": "/guides/walkthrough/deployment-1/#deploy-with-terraform", 
            "text": "Instead of using the Layer0 CLI directly, you can instead use our Terraform provider, and deploy using Terraform  ( learn more ) .\nYou can use Terraform with Layer0 and AWS to create \"fire-and-forget\" deployments for your applications.  If you're following along, you'll want to be working in the  walkthrough/deployment-1/  directory of your clone of the  guides  repo.  We use these files to set up a Layer0 environment with Terraform:     Filename  Purpose      main.tf  Provisions resources; populates resources in template files    outputs.tf  Values that Terraform will yield during deployment    terraform.tfstate  Tracks status of deployment  (created and managed by Terraform)    terraform.tfvars  Variables specific to the environment and application(s)    variables.tf  Values that Terraform will use during deployment", 
            "title": "Deploy with Terraform"
        }, 
        {
            "location": "/guides/walkthrough/deployment-1/#tf-a-brief-aside", 
            "text": "Let's take a moment to discuss the  .tf  files.\nThe names of these files (and even the fact that they are separated out into multiple files at all) are completely arbitrary and exist soley for human-readability.\nTerraform understands all  .tf  files in a directory all together.  In  variables.tf , you'll see  \"endpoint\"  and  \"token\"  variables.  In  outputs.tf , you'll see that Terraform should spit out the url of the guestbook's load balancer once deployment has finished.  In  main.tf , you'll see the bulk of the deployment process.\nIf you've followed along with the Layer0 CLI deployment above, it should be fairly easy to see how blocks in this file map to steps in the CLI process.\nWhen we began the CLI deployment, our first step was to create an environment:  l0 environment create demo-env  This command is recreated in  main.tf  like so:  # walkthrough/deployment-1/main.tf\n\nresource  layer0_environment   demo-env  {\n    name =  demo-env \n}  We've bundled up the heart of the Guestbook deployment (load balancer, deploy, service, etc.) into a  Terraform module .\nTo use it, we declare a  module  block and pass in the source of the module as well as any configuration or variables that the module needs.  # walkthrough/deployment-1/main.tf\n\nmodule  guestbook  {\n    source         =  github.com/quintilesims/guides//guestbook/module \n    environment_id =  ${ layer0_environment . demo . id } \n}  You can see that we pass in the ID of the environment we create.\nAll variables declared in this block are passed to the module, so the next file we should look at is  variables.tf  inside of the module to get an idea of what the module is expecting.  There are a lot of variables here, but only one of them doesn't have a default value.  # guestbook/module/variables.tf\n\nvariable  environment_id  {\n    description =  id of the layer0 environment in which to create resources \n}  You'll notice that this is the variable that we're passing in.\nFor this particular deployment of the Guestbook, all of the default options are fine.\nWe could override any of them if we wanted to, just by specifying a new value for them back in  deployment-1/main.tf .  Now that we've seen the variables that the module will have, let's take a look at part of  module/main.tf  and see how some of them might be used:  # guestbook/module/main.tf\n\nresource  layer0_load_balancer   guestbook-lb  {\n    name =  ${ var . load_balancer_name } \n    environment =  ${ var . environment_id } \n    port {\n        host_port = 80\n        container_port = 80\n        protocol =  http \n    }\n}\n\n...  You can follow  this link  to learn more about Layer0 resources in Terraform.", 
            "title": "*.tf: A Brief Aside"
        }, 
        {
            "location": "/guides/walkthrough/deployment-1/#part-1-terraform-get", 
            "text": "This deployment uses modules, so we'll need to fetch those source materials.\nAt the command prompt, execute the following command:  terraform get  We should see output like the following:  Get :   git :: https :// github . com /quintilesims/g uides . git   We should now have a new local directory called  .terraform/ .\nWe don't need to do anything with it; we just want to make sure it's there.", 
            "title": "Part 1: Terraform Get"
        }, 
        {
            "location": "/guides/walkthrough/deployment-1/#part-2-terraform-init", 
            "text": "This deployment has provider dependencies so an init call must be made. \n(Terraform v0.11~ requries init)\nAt the command prompt, execute the following command:  terraform init  We should see output like the following:  Initializing modules...\n- module.guestbook\n\nInitializing provider plugins...\n- Checking for available provider plugins on https://releases.hashicorp.com...\n- Downloading plugin for provider  template  (1.0.0)...\n\nThe following providers do not have any version constraints in configuration,\nso the latest version was installed.\n\nTo prevent automatic upgrades to new major versions that may contain breaking\nchanges, it is recommended to add version =  ...  constraints to the\ncorresponding provider blocks in configuration, with the constraint strings\nsuggested below.\n\n* provider.template: version =  ~  1.0 \n\nTerraform has been successfully initialized!\n\nYou may now begin working with Terraform. Try running  terraform plan  to see\nany changes that are required for your infrastructure. All Terraform commands\nshould now work.\n\nIf you ever set or change modules or backend configuration for Terraform,\nrerun this command to reinitialize your working directory. If you forget, other\ncommands will detect it and remind you to do so if necessary.", 
            "title": "Part 2: Terraform Init"
        }, 
        {
            "location": "/guides/walkthrough/deployment-1/#part-3-terraform-plan", 
            "text": "Before we actually create/update/delete any resources, it's a good idea to find out what Terraform intends to do.  Run  terraform plan . Terraform will prompt you for configuration values that it does not have:  var.endpoint\n    Enter a value:\n\nvar.token\n    Enter a value:  You can find these values by running  l0-setup endpoint  your layer0 prefix .   Note  There are a few ways to configure Terraform so that you don't have to keep entering these values every time you run a Terraform command (editing the  terraform.tfvars  file, or exporting evironment variables like  TF_VAR_endpoint  and  TF_VAR_token , for example). See the  Terraform Docs  for more.   The  plan  command should give us output like the following:  Refreshing Terraform state in-memory prior to plan...\nThe refreshed state will be used to calculate this plan, but will not be\npersisted to local or remote state storage.\n\ndata.template_file.guestbook: Refreshing state...\nThe Terraform execution plan has been generated and is shown below.\nResources are shown in alphabetical order for quick scanning. Green resources\nwill be created (or destroyed and then created if an existing resource\nexists), yellow resources are being changed in-place, and red resources\nwill be destroyed. Cyan entries are data sources to be read.\n\nNote: You didn t specify an  -out  parameter to save this plan, so when apply  is called, Terraform can t guarantee this is what will execute.\n\n+ layer0_environment.demo\n    ami:                computed \n    cluster_count:      computed \n    links:              computed \n    name:               demo \n    os:                 linux \n    security_group_id:  computed \n    size:               m3.medium \n\n+ module.guestbook.layer0_deploy.guestbook\n    content:  {\\n    \\ AWSEBDockerrunVersion\\ : 2,\\n    \\ containerDefinitions\\ : [\\n        {\\n            \\ name\\ : \\ guestbook\\ ,\\n            \\ image\\ : \\ quintilesims/guestbook\\ ,\\n            \\ essential\\ : true,\\n      \\ memory\\ : 128,\\n            \\ environment\\ : [\\n                {\\n                    \\ name\\ : \\ GUESTBOOK_BACKEND_TYPE\\ ,\\n                    \\ value\\ : \\ memory\\ \\n                },\\n                {\\n          \\ name\\ : \\ GUESTBOOK_BACKEND_CONFIG\\ ,\\n                    \\ value\\ : \\ \\ \\n                },\\n           {\\n                    \\ name\\ : \\ AWS_ACCESS_KEY_ID\\ ,\\n                    \\ value\\ : \\ \\ \\n        },\\n                {\\n                    \\ name\\ : \\ AWS_SECRET_ACCESS_KEY\\ ,\\n                    \\ value\\ : \\ \\ \\n                },\\n                {\\n                    \\ name\\ : \\ AWS_REGION\\ ,\\n      \\ value\\ : \\ us-west-2\\ \\n                }\\n            ],\\n            \\ portMappings\\ : [\\n   {\\n                    \\ hostPort\\ : 80,\\n                    \\ containerPort\\ : 80\\n                }\\n      ]\\n        }\\n    ]\\n}\\n \n    name:     guestbook \n\n+ module.guestbook.layer0_load_balancer.guestbook\n    environment:                     ${ var . environment_id } \n    health_check.#:                  computed \n    name:                            guestbook \n    port.#:                          1 \n    port.2027667003.certificate:     \n    port.2027667003.container_port:  80 \n    port.2027667003.host_port:       80 \n    port.2027667003.protocol:        http \n    url:                             computed \n\n+ module.guestbook.layer0_service.guestbook\n    deploy:         ${   var . deploy_id   ==  \\ \\  ? layer0_deploy.guestbook.id : var.deploy_id  } \n    environment:    ${ var . environment_id } \n    load_balancer:  ${ layer0_load_balancer . guestbook . id } \n    name:           guestbook \n    scale:          1 \n    wait:           true \n\n\nPlan: 4 to add, 0 to change, 0 to destroy.  This shows you that Terraform intends to create a deploy, an environment, a load balancer, and a service, all through Layer0.  If you've gone through this deployment using the  Layer0 CLI , you may notice that these resources appear out of order - that's fine. Terraform presents these resources in alphabetical order, but underneath, it knows the correct order in which to create them.  Once we're satisfied that Terraform will do what we want it to do, we can move on to actually making these things exist!", 
            "title": "Part 3: Terraform Plan"
        }, 
        {
            "location": "/guides/walkthrough/deployment-1/#part-4-terraform-apply", 
            "text": "Run  terraform apply  to begin the process.  We should see output like the following:  layer0_environment.demo: Refreshing state...\n...\n...\n...\nlayer0_service.guestbook: Creation complete\n\nApply complete! Resources: 7 added, 0 changed, 0 destroyed.\n\nThe state of your infrastructure has been saved to the path\nbelow. This state is required to modify and destroy your\ninfrastructure, so keep it safe. To inspect the complete state\nuse the `terraform show` command.\n\nState path: terraform.tfstate\n\nOutputs:\n\nguestbook_url =  http endpoint for the sample application    Note  It may take a few minutes for the guestbook service to launch and the load balancer to become available. During that time you may get HTTP 503 errors when making HTTP requests against the load balancer URL.", 
            "title": "Part 4: Terraform Apply"
        }, 
        {
            "location": "/guides/walkthrough/deployment-1/#whats-happening", 
            "text": "Terraform provisions the AWS resources through Layer0, configures environment variables for the application, and deploys the application into a Layer0 environment. Terraform also writes the state of your deployment to the  terraform.tfstate  file (creating a new one if it's not already there).", 
            "title": "What's Happening"
        }, 
        {
            "location": "/guides/walkthrough/deployment-1/#cleanup_1", 
            "text": "When you're finished with the example, you can instruct Terraform to destroy the Layer0 environment, and terminate the application. Execute the following command (in the same directory):  terraform destroy  It's also now safe to remove the  .terraform/  directory and the  *.tfstate*  files.", 
            "title": "Cleanup"
        }, 
        {
            "location": "/guides/walkthrough/deployment-2/", 
            "text": "Deployment 2: Guestbook + Redis\n#\n\n\nIn this section, we're going to add some complexity to the previous deployment.\n\nDeployment 1\n saw us create a simple guestbook application which kept its data in memory.\nBut what if that ever came down, either by intention or accident?\nIt would be easy enough to redeploy it, but all of the entered data would be lost.\nWhat if we wanted to scale the application to run more than one copy?\nFor this deployment, we're going to separate the data store from the guestbook application by creating a second Layer0 service which will house a Redis database server and linking it to the first.\nYou can choose to complete this section using either \nthe Layer0 CLI\n or \nTerraform\n.\n\n\n\n\nDeploy with Layer0 CLI\n#\n\n\nFor this example, we'll be working in the \nwalkthrough/deployment-2/\n directory of the \nguides\n repo.\nWe assume that you've completed the \nLayer0 CLI\n section of Deployment 1.\n\n\nFiles used in this deployment:\n\n\n\n\n\n\n\n\nFilename\n\n\nPurpose\n\n\n\n\n\n\n\n\n\n\nGuestbook.Dockerrun.aws.json\n\n\nTemplate for running the Guestbook application\n\n\n\n\n\n\nRedis.Dockerrun.aws.json\n\n\nTemplate for running a Redis server\n\n\n\n\n\n\n\n\n\n\nPart 1: Create the Redis Load Balancer\n#\n\n\nBoth the Guestbook service and the Redis service will live in the same Layer0 environment, so we don't need to create one like we did in the first deployment.\nWe'll start by making a load balancer behind which the Redis service will be deployed.\n\n\nThe \nRedis.Dockerrun.aws.json\n task definition file we'll use is very simple - it just spins up a Redis server with the default configuration, which means that it will be serving on port 6379.\nOur load balancer needs to be able to forward TCP traffic to and from this port.\nAnd since we don't want the Redis server to be exposed to the public internet, we'll put it behind a private load balancer; private load balancers only accept traffic that originates from within their own environment.\nWe'll also need to specify a non-default healthcheck target, since the load balancer won't expose port 80.\nAt the command prompt, execute the following:\n\n\nl0 loadbalancer create --port 6379:6379/tcp --private --healthcheck-target tcp:6379 demo-env redis-lb\n\n\nWe should see output like the following:\n\n\nLOADBALANCER ID  LOADBALANCER NAME  ENVIRONMENT  SERVICE  PORTS          PUBLIC  URL\nredislb16ae6     redis-lb           demo-env              6378:6379:TCP  false\n\n\n\n\n\nThe following is a summary of the arguments passed in the above command:\n\n\n\n\nloadbalancer create\n: creates a new load balancer\n\n\n--port 6379:6379/TCP\n: instructs the load balancer to forward requests from port 6379 on the load balancer to port 6379 in the EC2 instance using the TCP protocol\n\n\n--private\n: instructs the load balancer to ignore external traffic\n\n\n--healthcheck-target tcp:6379\n: instructs the load balancer to check the health of the service via TCP pings to port 6379\n\n\ndemo-env\n: the name of the environment in which the load balancer is being created\n\n\nredis-lb\n: a name for the load balancer itself\n\n\n\n\n\n\nPart 2: Deploy the ECS Task Definition\n#\n\n\nHere, we just need to create the deploy using the \nRedis.Dockerrun.aws.json\n task definition file.\nAt the command prompt, execute the following:\n\n\nl0 deploy create Redis.Dockerrun.aws.json redis-dpl\n\n\nWe should see output like the following:\n\n\nDEPLOY ID    DEPLOY NAME  VERSION\nredis-dpl.1  redis-dpl    1\n\n\n\n\n\nThe following is a summary of the arguments passed in the above command:\n\n\n\n\ndeploy create\n: creates a new Layer0 Deploy and allows you to specify an ECS task definition\n\n\nRedis.Dockerrun.aws.json\n: the file name of the ECS task definition (use the full path of the file if it is not in your current working directory)\n\n\nredis-dpl\n: a name for the deploy, which we will use later when we create the service\n\n\n\n\n\n\nPart 3: Create the Redis Service\n#\n\n\nHere, we just need to pull the previous resources together into a service.\nAt the command prompt, execute the following:\n\n\nl0 service create --wait --loadbalancer demo-env:redis-lb demo-env redis-svc redis-dpl:latest\n\n\nWe should see output like the following:\n\n\nSERVICE ID    SERVICE NAME  ENVIRONMENT  LOADBALANCER  DEPLOYMENTS  SCALE\nredislb16ae6  redis-svc     demo-env     redis-lb      redis-dpl:1  0/1\n\n\n\n\n\nThe following is a summary of the arguments passed in the above commands:\n\n\n\n\nservice create\n: creates a new Layer0 Service\n\n\n--wait\n:  instructs the CLI to keep hold of the shell until the service has been successfully deployed\n\n\n--loadbalancer demo-env:redis-lb\n: the fully-qualified name of the load balancer; in this case, the load balancer named \nredis-lb\n in the environment named \ndemo-env\n\n\n(Again, it's not strictly necessary to use the fully-qualified name of the load balancer as long as there isn't another load balancer with the same name in a different environment)\n\n\n\n\n\n\ndemo-env\n: the name of the environment in which the service is to reside\n\n\nredis-svc\n: a name for the service we're creating\n\n\nredis-dpl:latest\n: the name of the deploy the service will put into action\n\n\n(We use \n:\n to specify which deploy we want - \n:latest\n will always give us the most recently-created one.)\n\n\n\n\n\n\n\n\n\n\nPart 4: Check the Status of the Redis Service\n#\n\n\nAs in the first deployment, we can keep an eye on our service by using the \nservice get\n command:\n\n\nl0 service get redis-svc\n\n\nOnce the service has finished scaling, try looking at the service's logs to see the output that the Redis server creates:\n\n\nl0 service logs redis-svc\n\n\nAmong some warnings and information not important to this exercise and a fun bit of ASCII art, you should see something like the following:\n\n\n... # words and ASCII art\n1:M 05 Apr 23:29:47.333 * The server is now ready to accept connections on port 6379\n\n\n\n\n\nNow we just need to teach the Guestbook application how to talk with our Redis service.\n\n\n\n\nPart 5: Update the Guestbook Deploy\n#\n\n\nYou should see in \nwalkthrough/deployment-2/\n another \nGuestbook.Dockerrun.aws.json\n file.\nThis file is very similar to but not the same as the one in \ndeployment-1/\n - if you open it up, you can see the following additions:\n\n\n    ...\n    \nenvironment\n: [\n        {\n            \nname\n: \nGUESTBOOK_BACKEND_TYPE\n,\n            \nvalue\n: \nredis\n\n        },\n        {\n            \nname\n: \nGUESTBOOK_BACKEND_CONFIG\n,\n            \nvalue\n: \nredis host and port here\n\n        }\n    ],\n    ...\n\n\n\n\n\nThe \n\"GUESTBOOK_BACKEND_CONFIG\"\n variable is what will point the Guestbook application towards the Redis server.\nThe \nredis host and port here\n section needs to be replaced and populated in the following format:\n\n\nvalue\n: \nADDRESS_OF_REDIS_SERVER:PORT_THE_SERVER_IS_SERVING_ON\n\n\n\n\n\n\nWe already know that Redis is serving on port 6379, so let's go find the server's address.\nRemember, it lives behind a load balancer that we made, so run the following command:\n\n\nl0 loadbalancer get redis-lb\n\n\nWe should see output like the following:\n\n\nLOADBALANCER ID  LOADBALANCER NAME  ENVIRONMENT  SERVICE    PORTS          PUBLIC  URL\nredislb16ae6     redis-lb           demo-env     redis-svc  6379:6379/TCP  false   internal-l0-\nyadda-yadda\n.elb.amazonaws.com\n\n\n\n\n\nCopy that \nURL\n value, replace \nredis host and port here\n with the \nURL\n value in \nGuestbook.Dockerrun.aws.json\n, append \n:6379\n to it, and save the file.\nIt should look something like the following:\n\n\n    ...\n    \nenvironment\n: [\n        {\n            \nname\n: \nGUESTBOOK_BACKEND_CONFIG\n,\n            \nvalue\n: \ninternal-l0-\nyadda-yadda\n.elb.amazonaws.com:6379\n\n        }\n    ],\n    ...\n\n\n\n\n\nNow, we can create an updated deploy:\n\n\nl0 deploy create Guestbook.Dockerrun.aws.json guestbook-dpl\n\n\nWe should see output like the following:\n\n\nDEPLOY ID        DEPLOY NAME    VERSION\nguestbook-dpl.2  guestbook-dpl  2\n\n\n\n\n\n\n\nPart 6: Update the Guestbook Service\n#\n\n\nAlmost all the pieces are in place!\nNow we just need to apply the new Guestbook deploy to the running Guestbook service:\n\n\nl0 service update guestbook-svc guestbook-dpl:latest\n\n\nAs the Guestbook service moves through the phases of its update process, we should see outputs like the following (if we keep an eye on the service with \nl0 service get guestbook-svc\n, that is):\n\n\nSERVICE ID    SERVICE NAME   ENVIRONMENT  LOADBALANCER  DEPLOYMENTS       SCALE\nguestbo5fadd  guestbook-svc  demo-env     guestbook-lb  guestbook-dpl:2*  1/1\n                                                        guestbook-dpl:1\n\n\n\n\n\nabove: \nguestbook-dpl:2\n is in a transitional state\n\n\nSERVICE ID    SERVICE NAME   ENVIRONMENT  LOADBALANCER  DEPLOYMENTS      SCALE\nguestbo5fadd  guestbook-svc  demo-env     guestbook-lb  guestbook-dpl:2  2/1\n                                                        guestbook-dpl:1\n\n\n\n\n\nabove: both versions of the deployment are running at scale\n\n\nSERVICE ID    SERVICE NAME   ENVIRONMENT  LOADBALANCER  DEPLOYMENTS       SCALE\nguestbo5fadd  guestbook-svc  demo-env     guestbook-lb  guestbook-dpl:2   1/1\n                                                        guestbook-dpl:1*\n\n\n\n\n\nabove: \nguestbook-dpl:1\n is in a transitional state\n\n\nSERVICE ID    SERVICE NAME   ENVIRONMENT  LOADBALANCER  DEPLOYMENTS      SCALE\nguestbo5fadd  guestbook-svc  demo-env     guestbook-lb  guestbook-dpl:2  1/1\n\n\n\n\n\nabove: \nguestbook-dpl:1\n has been removed, and only \nguestbook-dpl:2\n remains\n\n\n\n\nPart 7: Prove It\n#\n\n\nYou should now be able to point your browser at the URL for the Guestbook load balancer (run \nl0 loadbalancer get guestbook-lb\n to find it) and see what looks like the same Guestbook application you deployed in the first section of the walkthrough.\nGo ahead and add a few entries, make sure it's functioning properly.\nWe'll wait.\n\n\nNow, let's prove that we've actually separated the data from the application by deleting and redeploying the Guestbook application:\n\n\nl0 service delete --wait guestbook-svc\n\n\n(We'll leave the \ndeploy\n intact so we can spin up a new service easily, and we'll leave the environment untouched because it also contained the Redis server.\nWe'll also pass the \n--wait\n flag so that we don't need to keep checking on the status of the job to know when it's complete.)\n\n\nOnce those resources have been deleted, we can recreate them!\n\n\nCreate another service, using the \nguestbook-dpl\n deploy we kept around:\n\n\nl0 service create --loadbalancer demo-env:guestbook-lb demo-env guestbook-svc guestbook-dpl:latest\n\n\nWait for everything to spin up, and hit that new load balancer's url (\nl0 loadbalancer get guestbook-lb\n) with your browser.\nYour data should still be there!\n\n\n\n\nCleanup\n#\n\n\nIf you're finished with the example and don't want to continue with this walkthrough, you can instruct Layer0 to delete the environment and terminate the application.\n\n\nl0 environment delete demo-env\n\n\nHowever, if you intend to continue through \nDeployment 3\n, you will want to keep the resources you made in this section.\n\n\n\n\nDeploy with Terraform\n#\n\n\nAs before, we can complete this deployment using Terraform and the Layer0 provider instead of the Layer0 CLI. As before, we will assume that you've cloned the \nguides\n repo and are working in the \nwalkthrough/deployment-2/\n directory.\n\n\nWe'll use these files to manage our deployment with Terraform:\n\n\n\n\n\n\n\n\nFilename\n\n\nPurpose\n\n\n\n\n\n\n\n\n\n\nmain.tf\n\n\nProvisions resources; populates variables in template files\n\n\n\n\n\n\noutputs.tf\n\n\nValues that Terraform will yield during deployment\n\n\n\n\n\n\nterraform.tfstate\n\n\nTracks status of deployment \n(created and managed by Terraform)\n\n\n\n\n\n\nterraform.tfvars\n\n\nVariables specific to the environment and application(s)\n\n\n\n\n\n\nvariables.tf\n\n\nValues that Terraform will use during deployment\n\n\n\n\n\n\n\n\n\n\n*.tf\n: A Brief Aside: Revisited\n#\n\n\nNot much is changed from \nDeployment 1\n.\nIn \nmain.tf\n, we pull in a new, second module that will deploy Redis for us.\nWe maintain this module as well; you can inspect \nthe repo\n if you'd like.\n\n\nIn \nmain.tf\n where we pull in the Guestbook module, you'll see that we're supplying more values than we did last time, because we need some additional configuration to let the Guestbook application use a Redis backend instead of its default in-memory storage.\n\n\n\n\nPart 1: Terraform Get\n#\n\n\nRun \nterraform get\n to pull down the source materials Terraform will use for deployment.\nThis will create a local \n.terraform/\n directory.\n\n\n\n\nPart 2: Terraform Init\n#\n\n\nThis deployment has provider dependencies so an init call must be made. \n(Terraform v0.11~ requries init)\nAt the command prompt, execute the following command:\n\n\nterraform init\n\n\nWe should see output like the following:\n\n\nInitializing modules...\n- module.redis\n  Getting source \ngithub.com/quintilesims/redis//terraform\n\n- module.guestbook\n  Getting source \ngithub.com/quintilesims/guides//guestbook/module\n\n\nInitializing provider plugins...\n- Checking for available provider plugins on https://releases.hashicorp.com...\n- Downloading plugin for provider \ntemplate\n (1.0.0)...\n\nThe following providers do not have any version constraints in configuration,\nso the latest version was installed.\n\nTo prevent automatic upgrades to new major versions that may contain breaking\nchanges, it is recommended to add version = \n...\n constraints to the\ncorresponding provider blocks in configuration, with the constraint strings\nsuggested below.\n\n* provider.template: version = \n~\n 1.0\n\n\nTerraform has been successfully initialized!\n\nYou may now begin working with Terraform. Try running \nterraform plan\n to see\nany changes that are required for your infrastructure. All Terraform commands\nshould now work.\n\nIf you ever set or change modules or backend configuration for Terraform,\nrerun this command to reinitialize your working directory. If you forget, other\ncommands will detect it and remind you to do so if necessary.\n\n\n\n\n\n\n\nPart 3: Terraform Plan\n#\n\n\nIt's always a good idea to find out what Terraform intends to do, so let's do that:\n\n\nterraform plan\n\n\nAs before, we'll be prompted for any variables Terraform needs and doesn't have (see the note in \nDeployment 1\n for configuring Terraform variables).\nWe'll see output similar to the following:\n\n\nRefreshing Terraform state in-memory prior to plan...\nThe refreshed state will be used to calculate this plan, but will not be\npersisted to local or remote state storage.\n\ndata.template_file.redis: Refreshing state...\nThe Terraform execution plan has been generated and is shown below.\nResources are shown in alphabetical order for quick scanning. Green resources\nwill be created (or destroyed and then created if an existing resource\nexists), yellow resources are being changed in-place, and red resources\nwill be destroyed. Cyan entries are data sources to be read.\n\nNote: You didn\nt specify an \n-out\n parameter to save this plan, so when\n\napply\n is called, Terraform can\nt guarantee this is what will execute.\n\n+ layer0_environment.demo\n    ami:               \ncomputed\n\n    cluster_count:     \ncomputed\n\n    links:             \ncomputed\n\n    name:              \ndemo\n\n    os:                \nlinux\n\n    security_group_id: \ncomputed\n\n    size:              \nm3.medium\n\n\n+ module.redis.layer0_deploy.redis\n    content: \n{\\n    \\\nAWSEBDockerrunVersion\\\n: 2,\\n    \\\ncontainerDefinitions\\\n: [\\n        {\\n            \\\nname\\\n: \\\nredis\\\n,\\n            \\\nimage\\\n: \\\nredis:3.2-alpine\\\n,\\n            \\\nessential\\\n: true,\\n            \\\nmemory\\\n: 128,\\n            \\\nportMappings\\\n: [\\n                {\\n                    \\\nhostPort\\\n: 6379,\\n              \\\ncontainerPort\\\n: 6379\\n                }\\n            ]\\n        }\\n    ]\\n}\\n\\n\n\n    name:    \nredis\n\n\n+ module.redis.layer0_load_balancer.redis\n    environment:                    \n${\nvar\n.\nenvironment_id\n}\n\n    health_check.#:                 \ncomputed\n\n    name:                           \nredis\n\n    port.#:                         \n1\n\n    port.1072619732.certificate:    \n\n    port.1072619732.container_port: \n6379\n\n    port.1072619732.host_port:      \n6379\n\n    port.1072619732.protocol:       \ntcp\n\n    private:                        \ntrue\n\n    url:                            \ncomputed\n\n\n+ module.redis.layer0_service.redis\n    deploy:        \n${\n \nvar\n.\ndeploy_id\n \n==\n \\\n\\\n ? layer0_deploy.redis.id : var.deploy_id \n}\n\n    environment:   \n${\nvar\n.\nenvironment_id\n}\n\n    load_balancer: \n${\nlayer0_load_balancer\n.\nredis\n.\nid\n}\n\n    name:          \nredis\n\n    scale:         \n1\n\n    wait:          \ntrue\n\n\n\n= module.guestbook.data.template_file.guestbook\n    rendered: \ncomputed\n\n    template: \n{\\n    \\\nAWSEBDockerrunVersion\\\n: 2,\\n    \\\ncontainerDefinitions\\\n: [\\n        {\\n            \\\nname\\\n: \\\nguestbook\\\n,\\n            \\\nimage\\\n: \\\nquintilesims/guestbook\\\n,\\n            \\\nessential\\\n: true,\\n       \\\nmemory\\\n: 128,\\n            \\\nenvironment\\\n: [\\n                {\\n                    \\\nname\\\n: \\\nGUESTBOOK_BACKEND_TYPE\\\n,\\n                    \\\nvalue\\\n: \\\n${\nbackend_type\n}\n\\\n\\n                },\\n                {\\n                    \\\nname\\\n: \\\nGUESTBOOK_BACKEND_CONFIG\\\n,\\n                    \\\nvalue\\\n: \\\n${\nbackend_config\n}\n\\\n\\n                },\\n                {\\n                    \\\nname\\\n: \\\nAWS_ACCESS_KEY_ID\\\n,\\n  \\\nvalue\\\n: \\\n${\naccess_key\n}\n\\\n\\n                },\\n                {\\n                    \\\nname\\\n: \\\nAWS_SECRET_ACCESS_KEY\\\n,\\n                    \\\nvalue\\\n: \\\n${\nsecret_key\n}\n\\\n\\n                },\\n                {\\n            \\\nname\\\n: \\\nAWS_REGION\\\n,\\n                    \\\nvalue\\\n: \\\n${\nregion\n}\n\\\n\\n                }\\n   ],\\n            \\\nportMappings\\\n: [\\n                {\\n                    \\\nhostPort\\\n: 80,\\n     \\\ncontainerPort\\\n: 80\\n                }\\n            ]\\n        }\\n    ]\\n}\\n\n\n    vars.%:   \ncomputed\n\n\n+ module.guestbook.layer0_deploy.guestbook\n    content: \n${\ndata\n.\ntemplate_file\n.\nguestbook\n.\nrendered\n}\n\n    name:    \nguestbook\n\n\n+ module.guestbook.layer0_load_balancer.guestbook\n    environment:                    \n${\nvar\n.\nenvironment_id\n}\n\n    health_check.#:                 \ncomputed\n\n    name:                           \nguestbook\n\n    port.#:                         \n1\n\n    port.2027667003.certificate:    \n\n    port.2027667003.container_port: \n80\n\n    port.2027667003.host_port:      \n80\n\n    port.2027667003.protocol:       \nhttp\n\n    url:                            \ncomputed\n\n\n+ module.guestbook.layer0_service.guestbook\n    deploy:        \n${\n \nvar\n.\ndeploy_id\n \n==\n \\\n\\\n ? layer0_deploy.guestbook.id : var.deploy_id \n}\n\n    environment:   \n${\nvar\n.\nenvironment_id\n}\n\n    load_balancer: \n${\nlayer0_load_balancer\n.\nguestbook\n.\nid\n}\n\n    name:          \nguestbook\n\n    scale:         \n2\n\n    wait:          \ntrue\n\n\n\nPlan: 7 to add, 0 to change, 0 to destroy.\n\n\n\n\n\nWe should see that Terraform intends to add 7 new resources, some of which are for the Guestbook deployment and some of which are for the Redis deployment.\n\n\n\n\nPart 4: Terraform Apply\n#\n\n\nRun \nterraform apply\n, and we should see output similar to the following:\n\n\ndata.template_file.redis: Refreshing state...\nlayer0_deploy.redis-dpl: Creating...\n\n...\n...\n...\n\nlayer0_service.guestbook-svc: Creation complete\n\nApply complete! Resources: 7 added, 0 changed, 0 destroyed.\n\nThe state of your infrastructure has been saved to the path\nbelow. This state is required to modify and destroy your\ninfrastructure, so keep it safe. To inspect the complete state\nuse the `terraform show` command.\n\nState path: terraform.tfstate\n\nOutputs:\n\nguestbook_url = \nhttp endpoint for the sample application\n\n\n\n\n\n\n\n\nNote\n\n\nIt may take a few minutes for the guestbook service to launch and the load balancer to become available.\nDuring that time you may get HTTP 503 errors when making HTTP requests against the load balancer URL.\n\n\n\n\nWhat's Happening\n#\n\n\nTerraform provisions the AWS resources through Layer0, configures environment variables for the application, and deploys the application into a Layer0 environment.\nTerraform also writes the state of your deployment to the \nterraform.tfstate\n file (creating a new one if it's not already there).\n\n\nCleanup\n#\n\n\nWhen you're finished with the example, you can instruct Terraform to destroy the Layer0 environment, and terminate the application.\nExecute the following command (in the same directory):\n\n\nterraform destroy\n\n\nIt's also now safe to remove the \n.terraform/\n directory and the \n*.tfstate*\n files.", 
            "title": "Walkthrough: Deployment 2"
        }, 
        {
            "location": "/guides/walkthrough/deployment-2/#deployment-2-guestbook-redis", 
            "text": "In this section, we're going to add some complexity to the previous deployment. Deployment 1  saw us create a simple guestbook application which kept its data in memory.\nBut what if that ever came down, either by intention or accident?\nIt would be easy enough to redeploy it, but all of the entered data would be lost.\nWhat if we wanted to scale the application to run more than one copy?\nFor this deployment, we're going to separate the data store from the guestbook application by creating a second Layer0 service which will house a Redis database server and linking it to the first.\nYou can choose to complete this section using either  the Layer0 CLI  or  Terraform .", 
            "title": "Deployment 2: Guestbook + Redis"
        }, 
        {
            "location": "/guides/walkthrough/deployment-2/#deploy-with-layer0-cli", 
            "text": "For this example, we'll be working in the  walkthrough/deployment-2/  directory of the  guides  repo.\nWe assume that you've completed the  Layer0 CLI  section of Deployment 1.  Files used in this deployment:     Filename  Purpose      Guestbook.Dockerrun.aws.json  Template for running the Guestbook application    Redis.Dockerrun.aws.json  Template for running a Redis server", 
            "title": "Deploy with Layer0 CLI"
        }, 
        {
            "location": "/guides/walkthrough/deployment-2/#part-1-create-the-redis-load-balancer", 
            "text": "Both the Guestbook service and the Redis service will live in the same Layer0 environment, so we don't need to create one like we did in the first deployment.\nWe'll start by making a load balancer behind which the Redis service will be deployed.  The  Redis.Dockerrun.aws.json  task definition file we'll use is very simple - it just spins up a Redis server with the default configuration, which means that it will be serving on port 6379.\nOur load balancer needs to be able to forward TCP traffic to and from this port.\nAnd since we don't want the Redis server to be exposed to the public internet, we'll put it behind a private load balancer; private load balancers only accept traffic that originates from within their own environment.\nWe'll also need to specify a non-default healthcheck target, since the load balancer won't expose port 80.\nAt the command prompt, execute the following:  l0 loadbalancer create --port 6379:6379/tcp --private --healthcheck-target tcp:6379 demo-env redis-lb  We should see output like the following:  LOADBALANCER ID  LOADBALANCER NAME  ENVIRONMENT  SERVICE  PORTS          PUBLIC  URL\nredislb16ae6     redis-lb           demo-env              6378:6379:TCP  false  The following is a summary of the arguments passed in the above command:   loadbalancer create : creates a new load balancer  --port 6379:6379/TCP : instructs the load balancer to forward requests from port 6379 on the load balancer to port 6379 in the EC2 instance using the TCP protocol  --private : instructs the load balancer to ignore external traffic  --healthcheck-target tcp:6379 : instructs the load balancer to check the health of the service via TCP pings to port 6379  demo-env : the name of the environment in which the load balancer is being created  redis-lb : a name for the load balancer itself", 
            "title": "Part 1: Create the Redis Load Balancer"
        }, 
        {
            "location": "/guides/walkthrough/deployment-2/#part-2-deploy-the-ecs-task-definition", 
            "text": "Here, we just need to create the deploy using the  Redis.Dockerrun.aws.json  task definition file.\nAt the command prompt, execute the following:  l0 deploy create Redis.Dockerrun.aws.json redis-dpl  We should see output like the following:  DEPLOY ID    DEPLOY NAME  VERSION\nredis-dpl.1  redis-dpl    1  The following is a summary of the arguments passed in the above command:   deploy create : creates a new Layer0 Deploy and allows you to specify an ECS task definition  Redis.Dockerrun.aws.json : the file name of the ECS task definition (use the full path of the file if it is not in your current working directory)  redis-dpl : a name for the deploy, which we will use later when we create the service", 
            "title": "Part 2: Deploy the ECS Task Definition"
        }, 
        {
            "location": "/guides/walkthrough/deployment-2/#part-3-create-the-redis-service", 
            "text": "Here, we just need to pull the previous resources together into a service.\nAt the command prompt, execute the following:  l0 service create --wait --loadbalancer demo-env:redis-lb demo-env redis-svc redis-dpl:latest  We should see output like the following:  SERVICE ID    SERVICE NAME  ENVIRONMENT  LOADBALANCER  DEPLOYMENTS  SCALE\nredislb16ae6  redis-svc     demo-env     redis-lb      redis-dpl:1  0/1  The following is a summary of the arguments passed in the above commands:   service create : creates a new Layer0 Service  --wait :  instructs the CLI to keep hold of the shell until the service has been successfully deployed  --loadbalancer demo-env:redis-lb : the fully-qualified name of the load balancer; in this case, the load balancer named  redis-lb  in the environment named  demo-env  (Again, it's not strictly necessary to use the fully-qualified name of the load balancer as long as there isn't another load balancer with the same name in a different environment)    demo-env : the name of the environment in which the service is to reside  redis-svc : a name for the service we're creating  redis-dpl:latest : the name of the deploy the service will put into action  (We use  :  to specify which deploy we want -  :latest  will always give us the most recently-created one.)", 
            "title": "Part 3: Create the Redis Service"
        }, 
        {
            "location": "/guides/walkthrough/deployment-2/#part-4-check-the-status-of-the-redis-service", 
            "text": "As in the first deployment, we can keep an eye on our service by using the  service get  command:  l0 service get redis-svc  Once the service has finished scaling, try looking at the service's logs to see the output that the Redis server creates:  l0 service logs redis-svc  Among some warnings and information not important to this exercise and a fun bit of ASCII art, you should see something like the following:  ... # words and ASCII art\n1:M 05 Apr 23:29:47.333 * The server is now ready to accept connections on port 6379  Now we just need to teach the Guestbook application how to talk with our Redis service.", 
            "title": "Part 4: Check the Status of the Redis Service"
        }, 
        {
            "location": "/guides/walkthrough/deployment-2/#part-5-update-the-guestbook-deploy", 
            "text": "You should see in  walkthrough/deployment-2/  another  Guestbook.Dockerrun.aws.json  file.\nThis file is very similar to but not the same as the one in  deployment-1/  - if you open it up, you can see the following additions:      ...\n     environment : [\n        {\n             name :  GUESTBOOK_BACKEND_TYPE ,\n             value :  redis \n        },\n        {\n             name :  GUESTBOOK_BACKEND_CONFIG ,\n             value :  redis host and port here \n        }\n    ],\n    ...  The  \"GUESTBOOK_BACKEND_CONFIG\"  variable is what will point the Guestbook application towards the Redis server.\nThe  redis host and port here  section needs to be replaced and populated in the following format:  value :  ADDRESS_OF_REDIS_SERVER:PORT_THE_SERVER_IS_SERVING_ON   We already know that Redis is serving on port 6379, so let's go find the server's address.\nRemember, it lives behind a load balancer that we made, so run the following command:  l0 loadbalancer get redis-lb  We should see output like the following:  LOADBALANCER ID  LOADBALANCER NAME  ENVIRONMENT  SERVICE    PORTS          PUBLIC  URL\nredislb16ae6     redis-lb           demo-env     redis-svc  6379:6379/TCP  false   internal-l0- yadda-yadda .elb.amazonaws.com  Copy that  URL  value, replace  redis host and port here  with the  URL  value in  Guestbook.Dockerrun.aws.json , append  :6379  to it, and save the file.\nIt should look something like the following:      ...\n     environment : [\n        {\n             name :  GUESTBOOK_BACKEND_CONFIG ,\n             value :  internal-l0- yadda-yadda .elb.amazonaws.com:6379 \n        }\n    ],\n    ...  Now, we can create an updated deploy:  l0 deploy create Guestbook.Dockerrun.aws.json guestbook-dpl  We should see output like the following:  DEPLOY ID        DEPLOY NAME    VERSION\nguestbook-dpl.2  guestbook-dpl  2", 
            "title": "Part 5: Update the Guestbook Deploy"
        }, 
        {
            "location": "/guides/walkthrough/deployment-2/#part-6-update-the-guestbook-service", 
            "text": "Almost all the pieces are in place!\nNow we just need to apply the new Guestbook deploy to the running Guestbook service:  l0 service update guestbook-svc guestbook-dpl:latest  As the Guestbook service moves through the phases of its update process, we should see outputs like the following (if we keep an eye on the service with  l0 service get guestbook-svc , that is):  SERVICE ID    SERVICE NAME   ENVIRONMENT  LOADBALANCER  DEPLOYMENTS       SCALE\nguestbo5fadd  guestbook-svc  demo-env     guestbook-lb  guestbook-dpl:2*  1/1\n                                                        guestbook-dpl:1  above:  guestbook-dpl:2  is in a transitional state  SERVICE ID    SERVICE NAME   ENVIRONMENT  LOADBALANCER  DEPLOYMENTS      SCALE\nguestbo5fadd  guestbook-svc  demo-env     guestbook-lb  guestbook-dpl:2  2/1\n                                                        guestbook-dpl:1  above: both versions of the deployment are running at scale  SERVICE ID    SERVICE NAME   ENVIRONMENT  LOADBALANCER  DEPLOYMENTS       SCALE\nguestbo5fadd  guestbook-svc  demo-env     guestbook-lb  guestbook-dpl:2   1/1\n                                                        guestbook-dpl:1*  above:  guestbook-dpl:1  is in a transitional state  SERVICE ID    SERVICE NAME   ENVIRONMENT  LOADBALANCER  DEPLOYMENTS      SCALE\nguestbo5fadd  guestbook-svc  demo-env     guestbook-lb  guestbook-dpl:2  1/1  above:  guestbook-dpl:1  has been removed, and only  guestbook-dpl:2  remains", 
            "title": "Part 6: Update the Guestbook Service"
        }, 
        {
            "location": "/guides/walkthrough/deployment-2/#part-7-prove-it", 
            "text": "You should now be able to point your browser at the URL for the Guestbook load balancer (run  l0 loadbalancer get guestbook-lb  to find it) and see what looks like the same Guestbook application you deployed in the first section of the walkthrough.\nGo ahead and add a few entries, make sure it's functioning properly.\nWe'll wait.  Now, let's prove that we've actually separated the data from the application by deleting and redeploying the Guestbook application:  l0 service delete --wait guestbook-svc  (We'll leave the  deploy  intact so we can spin up a new service easily, and we'll leave the environment untouched because it also contained the Redis server.\nWe'll also pass the  --wait  flag so that we don't need to keep checking on the status of the job to know when it's complete.)  Once those resources have been deleted, we can recreate them!  Create another service, using the  guestbook-dpl  deploy we kept around:  l0 service create --loadbalancer demo-env:guestbook-lb demo-env guestbook-svc guestbook-dpl:latest  Wait for everything to spin up, and hit that new load balancer's url ( l0 loadbalancer get guestbook-lb ) with your browser.\nYour data should still be there!", 
            "title": "Part 7: Prove It"
        }, 
        {
            "location": "/guides/walkthrough/deployment-2/#cleanup", 
            "text": "If you're finished with the example and don't want to continue with this walkthrough, you can instruct Layer0 to delete the environment and terminate the application.  l0 environment delete demo-env  However, if you intend to continue through  Deployment 3 , you will want to keep the resources you made in this section.", 
            "title": "Cleanup"
        }, 
        {
            "location": "/guides/walkthrough/deployment-2/#deploy-with-terraform", 
            "text": "As before, we can complete this deployment using Terraform and the Layer0 provider instead of the Layer0 CLI. As before, we will assume that you've cloned the  guides  repo and are working in the  walkthrough/deployment-2/  directory.  We'll use these files to manage our deployment with Terraform:     Filename  Purpose      main.tf  Provisions resources; populates variables in template files    outputs.tf  Values that Terraform will yield during deployment    terraform.tfstate  Tracks status of deployment  (created and managed by Terraform)    terraform.tfvars  Variables specific to the environment and application(s)    variables.tf  Values that Terraform will use during deployment", 
            "title": "Deploy with Terraform"
        }, 
        {
            "location": "/guides/walkthrough/deployment-2/#tf-a-brief-aside-revisited", 
            "text": "Not much is changed from  Deployment 1 .\nIn  main.tf , we pull in a new, second module that will deploy Redis for us.\nWe maintain this module as well; you can inspect  the repo  if you'd like.  In  main.tf  where we pull in the Guestbook module, you'll see that we're supplying more values than we did last time, because we need some additional configuration to let the Guestbook application use a Redis backend instead of its default in-memory storage.", 
            "title": "*.tf: A Brief Aside: Revisited"
        }, 
        {
            "location": "/guides/walkthrough/deployment-2/#part-1-terraform-get", 
            "text": "Run  terraform get  to pull down the source materials Terraform will use for deployment.\nThis will create a local  .terraform/  directory.", 
            "title": "Part 1: Terraform Get"
        }, 
        {
            "location": "/guides/walkthrough/deployment-2/#part-2-terraform-init", 
            "text": "This deployment has provider dependencies so an init call must be made. \n(Terraform v0.11~ requries init)\nAt the command prompt, execute the following command:  terraform init  We should see output like the following:  Initializing modules...\n- module.redis\n  Getting source  github.com/quintilesims/redis//terraform \n- module.guestbook\n  Getting source  github.com/quintilesims/guides//guestbook/module \n\nInitializing provider plugins...\n- Checking for available provider plugins on https://releases.hashicorp.com...\n- Downloading plugin for provider  template  (1.0.0)...\n\nThe following providers do not have any version constraints in configuration,\nso the latest version was installed.\n\nTo prevent automatic upgrades to new major versions that may contain breaking\nchanges, it is recommended to add version =  ...  constraints to the\ncorresponding provider blocks in configuration, with the constraint strings\nsuggested below.\n\n* provider.template: version =  ~  1.0 \n\nTerraform has been successfully initialized!\n\nYou may now begin working with Terraform. Try running  terraform plan  to see\nany changes that are required for your infrastructure. All Terraform commands\nshould now work.\n\nIf you ever set or change modules or backend configuration for Terraform,\nrerun this command to reinitialize your working directory. If you forget, other\ncommands will detect it and remind you to do so if necessary.", 
            "title": "Part 2: Terraform Init"
        }, 
        {
            "location": "/guides/walkthrough/deployment-2/#part-3-terraform-plan", 
            "text": "It's always a good idea to find out what Terraform intends to do, so let's do that:  terraform plan  As before, we'll be prompted for any variables Terraform needs and doesn't have (see the note in  Deployment 1  for configuring Terraform variables).\nWe'll see output similar to the following:  Refreshing Terraform state in-memory prior to plan...\nThe refreshed state will be used to calculate this plan, but will not be\npersisted to local or remote state storage.\n\ndata.template_file.redis: Refreshing state...\nThe Terraform execution plan has been generated and is shown below.\nResources are shown in alphabetical order for quick scanning. Green resources\nwill be created (or destroyed and then created if an existing resource\nexists), yellow resources are being changed in-place, and red resources\nwill be destroyed. Cyan entries are data sources to be read.\n\nNote: You didn t specify an  -out  parameter to save this plan, so when apply  is called, Terraform can t guarantee this is what will execute.\n\n+ layer0_environment.demo\n    ami:                computed \n    cluster_count:      computed \n    links:              computed \n    name:               demo \n    os:                 linux \n    security_group_id:  computed \n    size:               m3.medium \n\n+ module.redis.layer0_deploy.redis\n    content:  {\\n    \\ AWSEBDockerrunVersion\\ : 2,\\n    \\ containerDefinitions\\ : [\\n        {\\n            \\ name\\ : \\ redis\\ ,\\n            \\ image\\ : \\ redis:3.2-alpine\\ ,\\n            \\ essential\\ : true,\\n            \\ memory\\ : 128,\\n            \\ portMappings\\ : [\\n                {\\n                    \\ hostPort\\ : 6379,\\n              \\ containerPort\\ : 6379\\n                }\\n            ]\\n        }\\n    ]\\n}\\n\\n \n    name:     redis \n\n+ module.redis.layer0_load_balancer.redis\n    environment:                     ${ var . environment_id } \n    health_check.#:                  computed \n    name:                            redis \n    port.#:                          1 \n    port.1072619732.certificate:     \n    port.1072619732.container_port:  6379 \n    port.1072619732.host_port:       6379 \n    port.1072619732.protocol:        tcp \n    private:                         true \n    url:                             computed \n\n+ module.redis.layer0_service.redis\n    deploy:         ${   var . deploy_id   ==  \\ \\  ? layer0_deploy.redis.id : var.deploy_id  } \n    environment:    ${ var . environment_id } \n    load_balancer:  ${ layer0_load_balancer . redis . id } \n    name:           redis \n    scale:          1 \n    wait:           true  = module.guestbook.data.template_file.guestbook\n    rendered:  computed \n    template:  {\\n    \\ AWSEBDockerrunVersion\\ : 2,\\n    \\ containerDefinitions\\ : [\\n        {\\n            \\ name\\ : \\ guestbook\\ ,\\n            \\ image\\ : \\ quintilesims/guestbook\\ ,\\n            \\ essential\\ : true,\\n       \\ memory\\ : 128,\\n            \\ environment\\ : [\\n                {\\n                    \\ name\\ : \\ GUESTBOOK_BACKEND_TYPE\\ ,\\n                    \\ value\\ : \\ ${ backend_type } \\ \\n                },\\n                {\\n                    \\ name\\ : \\ GUESTBOOK_BACKEND_CONFIG\\ ,\\n                    \\ value\\ : \\ ${ backend_config } \\ \\n                },\\n                {\\n                    \\ name\\ : \\ AWS_ACCESS_KEY_ID\\ ,\\n  \\ value\\ : \\ ${ access_key } \\ \\n                },\\n                {\\n                    \\ name\\ : \\ AWS_SECRET_ACCESS_KEY\\ ,\\n                    \\ value\\ : \\ ${ secret_key } \\ \\n                },\\n                {\\n            \\ name\\ : \\ AWS_REGION\\ ,\\n                    \\ value\\ : \\ ${ region } \\ \\n                }\\n   ],\\n            \\ portMappings\\ : [\\n                {\\n                    \\ hostPort\\ : 80,\\n     \\ containerPort\\ : 80\\n                }\\n            ]\\n        }\\n    ]\\n}\\n \n    vars.%:    computed \n\n+ module.guestbook.layer0_deploy.guestbook\n    content:  ${ data . template_file . guestbook . rendered } \n    name:     guestbook \n\n+ module.guestbook.layer0_load_balancer.guestbook\n    environment:                     ${ var . environment_id } \n    health_check.#:                  computed \n    name:                            guestbook \n    port.#:                          1 \n    port.2027667003.certificate:     \n    port.2027667003.container_port:  80 \n    port.2027667003.host_port:       80 \n    port.2027667003.protocol:        http \n    url:                             computed \n\n+ module.guestbook.layer0_service.guestbook\n    deploy:         ${   var . deploy_id   ==  \\ \\  ? layer0_deploy.guestbook.id : var.deploy_id  } \n    environment:    ${ var . environment_id } \n    load_balancer:  ${ layer0_load_balancer . guestbook . id } \n    name:           guestbook \n    scale:          2 \n    wait:           true \n\n\nPlan: 7 to add, 0 to change, 0 to destroy.  We should see that Terraform intends to add 7 new resources, some of which are for the Guestbook deployment and some of which are for the Redis deployment.", 
            "title": "Part 3: Terraform Plan"
        }, 
        {
            "location": "/guides/walkthrough/deployment-2/#part-4-terraform-apply", 
            "text": "Run  terraform apply , and we should see output similar to the following:  data.template_file.redis: Refreshing state...\nlayer0_deploy.redis-dpl: Creating...\n\n...\n...\n...\n\nlayer0_service.guestbook-svc: Creation complete\n\nApply complete! Resources: 7 added, 0 changed, 0 destroyed.\n\nThe state of your infrastructure has been saved to the path\nbelow. This state is required to modify and destroy your\ninfrastructure, so keep it safe. To inspect the complete state\nuse the `terraform show` command.\n\nState path: terraform.tfstate\n\nOutputs:\n\nguestbook_url =  http endpoint for the sample application    Note  It may take a few minutes for the guestbook service to launch and the load balancer to become available.\nDuring that time you may get HTTP 503 errors when making HTTP requests against the load balancer URL.", 
            "title": "Part 4: Terraform Apply"
        }, 
        {
            "location": "/guides/walkthrough/deployment-2/#whats-happening", 
            "text": "Terraform provisions the AWS resources through Layer0, configures environment variables for the application, and deploys the application into a Layer0 environment.\nTerraform also writes the state of your deployment to the  terraform.tfstate  file (creating a new one if it's not already there).", 
            "title": "What's Happening"
        }, 
        {
            "location": "/guides/walkthrough/deployment-2/#cleanup_1", 
            "text": "When you're finished with the example, you can instruct Terraform to destroy the Layer0 environment, and terminate the application.\nExecute the following command (in the same directory):  terraform destroy  It's also now safe to remove the  .terraform/  directory and the  *.tfstate*  files.", 
            "title": "Cleanup"
        }, 
        {
            "location": "/guides/walkthrough/deployment-3/", 
            "text": "Deployment 3: Guestbook + Redis + Consul\n#\n\n\nIn \nDeployment 2\n, we created two services in the same environment and linked them together manually.\nWhile that can work for a small system, it's not really feasible for a system with a lot of moving parts - we would need to look up load balancer endpoints for all of our services and manually link them all together.\nTo that end, here we're going to to redeploy our two-service system using \nConsul\n, a service discovery tool.\n\n\nFor this deployment, we'll create a cluster of Consul servers which will be responsible for keeping track of the state of our system.\nWe'll also deploy new versions of the Guestbook and Redis task definition files - in addition to creating a container for its respective application, each task definition creates two other containers:\n\n\n\n\na container for a Consul agent, which is in charge of communicating with the Consul server cluster\n\n\na container for \nRegistrator\n, which is charge of talking to the local Consul agent when a service comes up or goes down.\n\n\n\n\nYou can choose to complete this section using either the \nLayer0 CLI\n or \nTerraform\n.\n\n\nDeploy with Layer0 CLI\n#\n\n\nIf you're following along, you'll want to be working in the \nwalkthrough/deployment-3/\n directory of your clone of the \nguides\n repo.\n\n\nFiles used in this deployment:\n\n\n\n\n\n\n\n\nFilename\n\n\nPurpose\n\n\n\n\n\n\n\n\n\n\nCLI.Consul.Dockerrun.aws.json\n\n\nTemplate for running a Consul server\n\n\n\n\n\n\nCLI.Guestbook.Dockerrun.aws.json\n\n\nTemplate for running the Guestbook application with Registrator and Consul agent\n\n\n\n\n\n\nCLI.Redis.Dockerrun.aws.json\n\n\nTemplate for running a Redis server with Registrator and Consul agent\n\n\n\n\n\n\n\n\n\n\nPart 1: Create the Consul Load Balancer\n#\n\n\nThe Consul server cluster will live in the same environment as our Guestbook and Redis services - if you've completed \nDeployment 1\n and \nDeployment 2\n, this environment already exists as \ndemo-env\n.\nWe'll start by creating a load balancer for the Consul cluster.\nThe load balancer will be private since only Layer0 services need to communicate with the Consul cluster.\nAt the command prompt, execute the following:\n\n\nl0 loadbalancer create --port 8500:8500/tcp --port 8301:8301/tcp --private --healthcheck-target tcp:8500 demo-env consul-lb\n\n\nWe should see output like the following:\n\n\nLOADBALANCER ID  LOADBALANCER NAME  ENVIRONMENT  SERVICE  PORTS          PUBLIC  URL\nconsull66b23     consul-lb          consul-env            8500:8500/TCP  false\n                                                          8301:8301/TCP\n\n\n\n\n\nThe following is a summary of the arguments passed in the above command:\n\n\n\n\nloadbalancer create\n: creates a new load balancer\n\n\n--port 8500:8500/tcp\n: instructs the load balancer to forward requests from port 8500 on the load balancer to port 8500 in the EC2 instance using the TCP protocol\n\n\n--port 8301:8301/tcp\n: instructs the load balancer to forward requests from port 8301 on the load balancer to port 8301 in the EC2 instance using the TCP protocol\n\n\n--private\n: instructs the load balancer to ignore outside traffic\n\n\n--healthcheck-target\n: instructs the load balancer to use a TCP ping on port 8500 as the basis for deciding whether the service is healthy\n\n\ndemo-env\n: the name of the environment in which the load balancer is being created\n\n\nconsul-lb\n: a name for the load balancer itself\n\n\n\n\nWhile we're touching on the Consul load balancer, we should grab its URL - this is the one value that we'll need to know in order to deploy the rest of our system, no matter how large it may get.\nAt the command prompt, execute the following:\n\n\nl0 loadbalancer get consul-lb\n\n\nWe should see output that looks like the output we just received above after creating the load balancer, but this time there is something in the \nURL\n column.\nThat URL is the value we're looking for.\nMake note of it for when we reference it later.\n\n\n\n\nPart 2: Deploy the Consul Task Definition\n#\n\n\nBefore we can create the deploy, we need to supply the URL of the Consul load balancer that we got in Part 1.\nIn \nCLI.Consul.Dockerrun.aws.json\n, find the entry in the \nenvironment\n block that looks like this:\n\n\n{\n    \nname\n: \nCONSUL_SERVER_URL\n,\n    \nvalue\n: \n\n}\n\n\n\n\n\nUpdate the \"value\" with the Consul load balancer's URL into and save the file.\nWe can then create the deploy.\nAt the command prompt, execute the following:\n\n\nl0 deploy create CLI.Consul.Dockerrun.aws.json consul-dpl\n\n\nWe should see output like the following:\n\n\nDEPLOY ID     DEPLOY NAME  VERSION\nconsul-dpl.1  consul-dpl   1\n\n\n\n\n\nThe following is a summary of the arguments passed in the above command:\n\n\n\n\ndeploy create\n: creates a new Layer0 Deploy and allows you to specifiy an ECS task definition\n\n\nCLI.Consul.Dockerrun.aws.json\n: the file name of the ECS task definition (use the full path of the file if it is not in the current working directory)\n\n\nconsul-dpl\n: a name for the deploy, which will later be used in creating the service\n\n\n\n\n\n\nPart 3: Create the Consul Service\n#\n\n\nHere, we pull the previous resources together to create a service.\nAt the command prompt, execute the following:\n\n\nl0 service create --wait --loadbalancer demo-env:consul-lb demo-env consul-svc consul-dpl:latest\n\n\nWe should see output like the following:\n\n\nWaiting for Deployment...\nSERVICE ID    SERVICE NAME  ENVIRONMENT  LOADBALANCER  DEPLOYMENTS   SCALE\nconsuls2f3c6  consul-svc    demo-env     consul-lb     consul-dpl:1  1/1\n\n\n\n\n\nThe following is a summary of the arguments passed in the above commands:\n\n\n\n\nservice create\n: creates a new Layer0 Service\n\n\n--wait\n: instructs the CLI to keep hold of the shell until the service has been successfully deployed\n\n\n--loadbalancer demo-env:consul-lb\n: the fully-qualified name of the load balancer behind which the service should live; in this case, the load balancer named \nconsul-lb\n in the environment named \ndemo-env\n\n\ndemo-env\n: the name of the environment in which the service is to reside\n\n\nconsul-svc\n: a name for the service itself\n\n\nconsul-dpl:latest\n: the name and version of the deploy that the service should put into action\n\n\n\n\nOnce the service has finished being deployed (and \n--wait\n has returned our shell to us), we need to scale the service.\n\n\nCurrently, we only have one Consul server running in the cluster.\nFor best use, we should have at least 3 servers running (see \nthis link\n for more details on Consul servers and their concensus protocol).\nIndeed, if we inspect the \ncommand\n block of the task definition file, we can find the following parameter: \n-bootstrap-expect=3\n.\nThis tells the Consul server that we have just deployed that it should be expecting a total of three servers.\nWe still need to fulfill that expectation, so we'll scale our service up to three.\nAt the command prompt, execute the following:\n\n\nl0 service scale --wait consul-svc 3\n\n\nWe should see output like the following:\n\n\nWaiting for Deployment...\nSERVICE ID    SERVICE NAME  ENVIRONMENT  LOADBALANCER  DEPLOYMENTS   SCALE\nconsuls2f3c6  consul-svc    demo-env     consul-lb     consul-dpl:1  3/3\n\n\n\n\n\n\n\nImportant!\n\n\nThe successful completion of the \nscale\n command doesn't mean that we're ready to move on just yet!\nWe need to check in on the logs (\nl0 service logs consul-svc\n) until we can confirm that all three of the Consul servers have synced up with each other.\nEach \nconsul-server\n section in the logs should be ending with \nconsul: Adding LAN server [ip address]\n or \nagent: Join completed\n.\nIf you see one of the sections ending with \nagent: Join failed, retrying in 30s\n, you need to wait for that server to join the cluster before continuing.\n\n\n\n\n\n\nPart 4: Update and Redeploy the Redis and Guestbook Applications\n#\n\n\nWe're going to need the URL of the Consul load balancer again.\nIn each of the CLI.Redis and CLI.Guestbook task definition files, look for the \nCONSUL_SERVER_URL\n block in the \nconsul-agent\n container and populate the value field with the Consul load balancer's URL, then save the file.\nAt the command prompt, execute the two following commands to create new versions of the deploys for the Redis and Guestbook applications:\n\n\nl0 deploy create CLI.Redis.Dockerrun.aws.json redis-dpl\n\n\nl0 deploy create CLI.Guestbook.Dockerrun.aws.json guestbook-dpl\n\n\nThen, execute the two following commands to redeploy the existing Redis and Guestbook services using those new deploys:\n\n\nl0 service update --wait redis-svc redis-dpl:latest\n\n\nl0 service update --wait guestbook-svc guestbook-dpl:latest\n\n\n\n\nNote\n\n\nHere, we should run \nl0 service logs consul-svc\n again and confirm that the Consul cluster has discovered these two services.\n\n\n\n\nWe can use \nl0 loadbalancer get guestbook-lb\n to obtain the guestbook application's URL, and then navigate to it with a web browser.\nOur guestbook app should be up and running - this time, it's been deployed without needing to know the address of the Redis backend!\n\n\nOf course, this is a simple example; in both this deployment and \nDeployment 2\n, we needed to use \nl0 loadbalancer get\n to obtain the URL of a load balancer.\nHowever, in a system with many services that uses Consul like this example, we only ever need to find the URL of the Consul cluster - not the URLs of every service that needs to talk to another of our services.\n\n\n\n\nPart 5: Inspect the Consul Universe (Optional)\n#\n\n\nLet's take a glimpse into how this system that we've deployed works.\n\nThis requires that we have access to the key pair we've told Layer0 about when we \nset it up\n.\n\n\nOpen Ports for SSH\n#\n\n\nWe want to SSH into the Guestbook EC2 instance, which means that we need to tell the Guestbook load balancer to allow SSH traffic through.\nAt the command prompt, execute the following:\n\n\nl0 loadbalancer addport guestbook-lb 22:22/tcp\n\n\nWe should see output like the following:\n\n\nLOADBALANCER ID  LOADBALANCER NAME  ENVIRONMENT  SERVICE        PORTS       PUBLIC  URL\nguestbodb65a     guestbook-lb       demo-env     guestbook-svc  80:80/HTTP  true    \nurl\n\n                                                                22:22/TCP\n\n\n\n\n\nWe need to take note of the load balancer's URL here, too.\n\n\nSSH Into the Instance\n#\n\n\nAt the command prompt, execute the following:\n\n\nssh -i /path/to/keypair ec2-user@\nguestbook_load_balancer_url\n -o ServerAliveInterval=30\n\n\n(We'll probably be asked if we want to continue connecting - we do, so we'll enter \nyes\n.)\n\n\nSummary of arguments passed into the above command:\n\n\n\n\n-i /path/to/keypair\n: this allows us to specify an identity file for use when connecting to the remote machine - in this case, we want to replace \n/path/to/keypair\n with the actual path to the keypair we created when we set up Layer0\n\n\nec2-user@\nguestbook_load_balancer_url\n: the address (here we want to replace \nguestbook_load_balancer_url\n with the actual URL of the guestbook load balancer) of the machine to which we want to connect and the name of the user (\nec2-user\n) that we'd like to connect as\n\n\n-o\n: allows us to set parameters on the \nssh\n command\n\n\nServerAliveInterval=30\n: one of those \nssh\n parameters - AWS imposes an automatic disconnect if a connection is not active for a certain amount of time, so we use this option to ping every 30 seconds to prevent that automatic disconnect\n\n\n\n\nLook Around You\n#\n\n\nWe're now inside of the EC2 instance!\nIf we run \ndocker ps\n, we should see that our three Docker containers (the Guestbook app, a Consul agent, and Registrator) are up and running, as well as an \namazon-ecs-agent\n image.\nBut that's not the Consul universe that we came here to see.\nAt the EC2 instance's command prompt, execute the following:\n\n\necho $(curl -s localhost:8500/v1/catalog/services) | jq '.'\n\n\nWe should see output like the following:\n\n\n{\n  \nconsul\n: [],\n  \nconsul-8301\n: [\n    \nudp\n\n  ],\n  \nconsul-8500\n: [],\n  \nconsul-8600\n: [\n    \nudp\n\n  ],\n  \nguestbook-redis\n: [],\n  \nredis\n: []\n}\n\n\n\n\n\nSummary of commands passed in the above command:\n\n\n\n\ncurl -s localhost:8500/v1/catalog/services\n: use \ncurl\n to send a GET request to the specified URL, where \nlocalhost:8500\n is an HTTP connection to the local Consul agent in this EC2 instance (the \n-s\n flag just silences excess output from \ncurl\n)\n\n\n| jq '.'\n: use a pipe (\n|\n) to take whatever returns from the left side of the pipe and pass it to the \njq\n program, which we use here simply to pretty-print the JSON response\n\n\necho $(...)\n: print out whatever returns from running the stuff inside of the parens; not necessary, but it gives us a nice newline after we get our response\n\n\n\n\nIn that output, we can see all of the things that our local Consul agent knows about.\nIn addition to a few connections to the Consul server cluster, we can see that it knows about the Guestbook application running in this EC2 instance, as well as the Redis application running in a different instance with its own Consul agent and Registrator.\n\n\nLet's take a closer look at the Redis service and see how our Guestbook application is locating our Redis application.\nAt the EC2 instance's command prompt, execute the following:\n\n\necho $(curl -s http://localhost:8500/v1/catalog/service/redis) | jq '.'\n\n\nWe should see output like the following:\n\n\n[\n  {\n    \nID\n: \nb4bb81e6-fe6a-c630-2553-7f6492ae5275\n,\n    \nNode\n: \nip-10-100-230-97.us-west-2.compute.internal\n,\n    \nAddress\n: \n10.100.230.97\n,\n    \nDatacenter\n: \ndc1\n,\n    \nTaggedAddresses\n: {\n      \nlan\n: \n10.100.230.97\n,\n      \nwan\n: \n10.100.230.97\n\n    },\n    \nNodeMeta\n: {},\n    \nServiceID\n: \n562aceee6935:ecs-l0-tlakedev-redis-dpl-20-redis-e0f989e5af97cdfd0e00:6379\n,\n    \nServiceName\n: \nredis\n,\n    \nServiceTags\n: [],\n    \nServiceAddress\n: \n10.100.230.97\n,\n    \nServicePort\n: 6379,\n    \nServiceEnableTagOverride\n: false,\n    \nCreateIndex\n: 761,\n    \nModifyIndex\n: 761\n  }\n]\n\n\n\n\n\nTo \nreally\n see how the Guestbook application connects to Redis, we can take an \neven closer\n look!\n\n\nRun \ndocker ps\n to generate a listing of all the containers that Docker is running on the EC2 instance, and note the Container ID for the Guestbook container. Then run the following command to connect to the Guestbook container:\n\n\ndocker exec -it [container_id] /bin/sh\n\n\nOnce we've gotten inside the container, we'll run a similar command to the previous \ncurl\n:\n\n\ncurl -s consul-agent:8500/v1/catalog/service/redis\n\n\nOur Guestbook application makes a call like this one and figures out how to connect to the Redis service by mushing together the information from the \nServiceAddress\n and \nServicePort\n fields!\n\n\nTo close the \nssh\n connection to the EC2 instance, run \nexit\n in the command prompt.\n\n\n\n\nCleanup\n#\n\n\nWhen you're finished with the example, we can instruct Layer0 to terminate the applications and delete the environment.\n\n\nl0 environment delete demo-env\n\n\n\n\nDeploy with Terraform\n#\n\n\nAs before, we can complete this deployment using Terraform and the Layer0 provider instead of the Layer0 CLI.\nAs before, we will assume that you've cloned the \nguides\n repo and are working in the \niterative-walkthrough/deployment-3/\n directory.\n\n\nWe'll use these files to manage our deployment with Terraform:\n\n\n\n\n\n\n\n\nFilename\n\n\nPurpose\n\n\n\n\n\n\n\n\n\n\nGuestbook.Dockerrun.aws.json\n\n\nTemplate for running the Guestbook application\n\n\n\n\n\n\nmain.tf\n\n\nProvisions resources; populates variables in template files\n\n\n\n\n\n\noutputs.tf\n\n\nValues that Terraform will yield during deployment\n\n\n\n\n\n\nRedis.Dockerrun.aws.json\n\n\nTemplate for running the Redis application\n\n\n\n\n\n\nterraform.tfstate\n\n\nTracks status of deployment \n(created and managed by Terraform)\n\n\n\n\n\n\nterraform.tfvars\n\n\nVariables specific to the environment and application(s)\n\n\n\n\n\n\nvariables.tf\n\n\nValues that Terraform will use during deployment\n\n\n\n\n\n\n\n\n\n\n*.tf\n: A Brief Aside: Revisited: Redux\n#\n\n\nIn looking at \nmain.tf\n, you can see that we're pulling in a Consul module that we maintain (here's the \nrepo\n); this removes the need for a local task definition file.\n\n\nWe also are continuing to use modules for Redis and Guestbook.\nHowever, instead of just sourcing the module and passing in a value or two, you can see that we actually create new deploys from local task definition files and pass those deploys in to the module.\nThis design allows us to use pre-made modules while also offering a great deal of flexibility.\nIf you'd like to follow along the Redis deployment logic chain (the other applications/services work similarly), it goes something like this:\n\n\n\n\nmain.tf\n creates a deploy for the Redis server by rendering a local task definition and populating it with certain values\n\n\nmain.tf\n passes the ID of the deploy into the Redis module, along with other values the module requires\n\n\nthe Redis module\n pulls all the variables it knows about (both the defaults in \nvariables.tf\n as well as the ones passed in)\n\n\namong other Layer0/AWS resources, the module spins up a Redis service; since a deploy ID has been provided, it uses that deploy to create the service instead of a deploy made from a \ndefault task definition\n contained within the module\n\n\n\n\n\n\nPart 1: Terraform Get\n#\n\n\nRun \nterraform get\n to pull down all the source materials Terraform needs for our deployment.\n\n\n\n\nPart 2: Terraform Init\n#\n\n\nThis deployment has provider dependencies so an init call must be made. \n(Terraform v0.11~ requries init)\nAt the command prompt, execute the following command:\n\n\nterraform init\n\n\nWe should see output like the following:\n\n\nInitializing modules...\n- module.consul\n- module.redis\n- module.guestbook\n\nInitializing provider plugins...\n- Checking for available provider plugins on https://releases.hashicorp.com...\n- Downloading plugin for provider \ntemplate\n (1.0.0)...\n\nThe following providers do not have any version constraints in configuration,\nso the latest version was installed.\n\nTo prevent automatic upgrades to new major versions that may contain breaking\nchanges, it is recommended to add version = \n...\n constraints to the\ncorresponding provider blocks in configuration, with the constraint strings\nsuggested below.\n\n* provider.template: version = \n~\n 1.0\n\n\nTerraform has been successfully initialized!\n\nYou may now begin working with Terraform. Try running \nterraform plan\n to see\nany changes that are required for your infrastructure. All Terraform commands\nshould now work.\n\nIf you ever set or change modules or backend configuration for Terraform,\nrerun this command to reinitialize your working directory. If you forget, other\ncommands will detect it and remind you to do so if necessary.\n\n\n\n\n\n\n\nPart 2: Terraform Plan\n#\n\n\nAs before, we can run \nterraform plan\n to see what's going to happen.\nWe should see that there are 12 new resources to be created:\n\n\n\n\nthe environment\n\n\nthe two local deploys which will be used for Guestbook and Redis\n\n\nthe load balancer, deploy, and service from each of the Consul, Guestbook, and Redis modules\n\n\nnote that even though the default modules' deploys are created, they won't actually be used to deploy services\n\n\n\n\n\n\n\n\n\n\nPart 3: Terraform Apply\n#\n\n\nRun \nterraform apply\n, and we should see output similar to the following:\n\n\ndata.template_file.consul: Refreshing state...\nlayer0_deploy.consul-dpl: Creating...\n\n...\n...\n...\n\nlayer0_service.guestbook-svc: Creation complete\n\nApply complete! Resources: 10 added, 0 changed, 0 destroyed.\n\nThe state of your infrastructure has been saved to the path\nbelow. This state is required to modify and destroy your\ninfrastructure, so keep it safe. To inspect the complete state\nuse the `terraform show` command.\n\nState path: terraform.tfstate\n\nOutputs:\n\nguestbook_url = \nhttp endpoint for the guestbook application\n\n\n\n\n\n\n\n\nNote\n\n\nIt may take a few minutes for the guestbook service to launch and the load balancer to become available.\nDuring that time you may get HTTP 503 errors when making HTTP requests against the load balancer URL.\n\n\n\n\nWhat's Happening\n#\n\n\nTerraform provisions the AWS resources through Layer0, configures environment variables for the application, and deploys the application into a Layer0 environment.\nTerraform also writes the state of your deployment to the \nterraform.tfstate\n file (creating a new one if it's not already there).\n\n\nCleanup\n#\n\n\nWhen you're finished with the example, you can instruct Terraform to destroy the Layer0 environment, and terminate the application.\nExecute the following command (in the same directory):\n\n\nterraform destroy\n\n\nIt's also now safe to remove the \n.terraform/\n directory and the \n*.tfstate*\n files.", 
            "title": "Walkthrough: Deployment 3"
        }, 
        {
            "location": "/guides/walkthrough/deployment-3/#deployment-3-guestbook-redis-consul", 
            "text": "In  Deployment 2 , we created two services in the same environment and linked them together manually.\nWhile that can work for a small system, it's not really feasible for a system with a lot of moving parts - we would need to look up load balancer endpoints for all of our services and manually link them all together.\nTo that end, here we're going to to redeploy our two-service system using  Consul , a service discovery tool.  For this deployment, we'll create a cluster of Consul servers which will be responsible for keeping track of the state of our system.\nWe'll also deploy new versions of the Guestbook and Redis task definition files - in addition to creating a container for its respective application, each task definition creates two other containers:   a container for a Consul agent, which is in charge of communicating with the Consul server cluster  a container for  Registrator , which is charge of talking to the local Consul agent when a service comes up or goes down.   You can choose to complete this section using either the  Layer0 CLI  or  Terraform .", 
            "title": "Deployment 3: Guestbook + Redis + Consul"
        }, 
        {
            "location": "/guides/walkthrough/deployment-3/#deploy-with-layer0-cli", 
            "text": "If you're following along, you'll want to be working in the  walkthrough/deployment-3/  directory of your clone of the  guides  repo.  Files used in this deployment:     Filename  Purpose      CLI.Consul.Dockerrun.aws.json  Template for running a Consul server    CLI.Guestbook.Dockerrun.aws.json  Template for running the Guestbook application with Registrator and Consul agent    CLI.Redis.Dockerrun.aws.json  Template for running a Redis server with Registrator and Consul agent", 
            "title": "Deploy with Layer0 CLI"
        }, 
        {
            "location": "/guides/walkthrough/deployment-3/#part-1-create-the-consul-load-balancer", 
            "text": "The Consul server cluster will live in the same environment as our Guestbook and Redis services - if you've completed  Deployment 1  and  Deployment 2 , this environment already exists as  demo-env .\nWe'll start by creating a load balancer for the Consul cluster.\nThe load balancer will be private since only Layer0 services need to communicate with the Consul cluster.\nAt the command prompt, execute the following:  l0 loadbalancer create --port 8500:8500/tcp --port 8301:8301/tcp --private --healthcheck-target tcp:8500 demo-env consul-lb  We should see output like the following:  LOADBALANCER ID  LOADBALANCER NAME  ENVIRONMENT  SERVICE  PORTS          PUBLIC  URL\nconsull66b23     consul-lb          consul-env            8500:8500/TCP  false\n                                                          8301:8301/TCP  The following is a summary of the arguments passed in the above command:   loadbalancer create : creates a new load balancer  --port 8500:8500/tcp : instructs the load balancer to forward requests from port 8500 on the load balancer to port 8500 in the EC2 instance using the TCP protocol  --port 8301:8301/tcp : instructs the load balancer to forward requests from port 8301 on the load balancer to port 8301 in the EC2 instance using the TCP protocol  --private : instructs the load balancer to ignore outside traffic  --healthcheck-target : instructs the load balancer to use a TCP ping on port 8500 as the basis for deciding whether the service is healthy  demo-env : the name of the environment in which the load balancer is being created  consul-lb : a name for the load balancer itself   While we're touching on the Consul load balancer, we should grab its URL - this is the one value that we'll need to know in order to deploy the rest of our system, no matter how large it may get.\nAt the command prompt, execute the following:  l0 loadbalancer get consul-lb  We should see output that looks like the output we just received above after creating the load balancer, but this time there is something in the  URL  column.\nThat URL is the value we're looking for.\nMake note of it for when we reference it later.", 
            "title": "Part 1: Create the Consul Load Balancer"
        }, 
        {
            "location": "/guides/walkthrough/deployment-3/#part-2-deploy-the-consul-task-definition", 
            "text": "Before we can create the deploy, we need to supply the URL of the Consul load balancer that we got in Part 1.\nIn  CLI.Consul.Dockerrun.aws.json , find the entry in the  environment  block that looks like this:  {\n     name :  CONSUL_SERVER_URL ,\n     value :  \n}  Update the \"value\" with the Consul load balancer's URL into and save the file.\nWe can then create the deploy.\nAt the command prompt, execute the following:  l0 deploy create CLI.Consul.Dockerrun.aws.json consul-dpl  We should see output like the following:  DEPLOY ID     DEPLOY NAME  VERSION\nconsul-dpl.1  consul-dpl   1  The following is a summary of the arguments passed in the above command:   deploy create : creates a new Layer0 Deploy and allows you to specifiy an ECS task definition  CLI.Consul.Dockerrun.aws.json : the file name of the ECS task definition (use the full path of the file if it is not in the current working directory)  consul-dpl : a name for the deploy, which will later be used in creating the service", 
            "title": "Part 2: Deploy the Consul Task Definition"
        }, 
        {
            "location": "/guides/walkthrough/deployment-3/#part-3-create-the-consul-service", 
            "text": "Here, we pull the previous resources together to create a service.\nAt the command prompt, execute the following:  l0 service create --wait --loadbalancer demo-env:consul-lb demo-env consul-svc consul-dpl:latest  We should see output like the following:  Waiting for Deployment...\nSERVICE ID    SERVICE NAME  ENVIRONMENT  LOADBALANCER  DEPLOYMENTS   SCALE\nconsuls2f3c6  consul-svc    demo-env     consul-lb     consul-dpl:1  1/1  The following is a summary of the arguments passed in the above commands:   service create : creates a new Layer0 Service  --wait : instructs the CLI to keep hold of the shell until the service has been successfully deployed  --loadbalancer demo-env:consul-lb : the fully-qualified name of the load balancer behind which the service should live; in this case, the load balancer named  consul-lb  in the environment named  demo-env  demo-env : the name of the environment in which the service is to reside  consul-svc : a name for the service itself  consul-dpl:latest : the name and version of the deploy that the service should put into action   Once the service has finished being deployed (and  --wait  has returned our shell to us), we need to scale the service.  Currently, we only have one Consul server running in the cluster.\nFor best use, we should have at least 3 servers running (see  this link  for more details on Consul servers and their concensus protocol).\nIndeed, if we inspect the  command  block of the task definition file, we can find the following parameter:  -bootstrap-expect=3 .\nThis tells the Consul server that we have just deployed that it should be expecting a total of three servers.\nWe still need to fulfill that expectation, so we'll scale our service up to three.\nAt the command prompt, execute the following:  l0 service scale --wait consul-svc 3  We should see output like the following:  Waiting for Deployment...\nSERVICE ID    SERVICE NAME  ENVIRONMENT  LOADBALANCER  DEPLOYMENTS   SCALE\nconsuls2f3c6  consul-svc    demo-env     consul-lb     consul-dpl:1  3/3   Important!  The successful completion of the  scale  command doesn't mean that we're ready to move on just yet!\nWe need to check in on the logs ( l0 service logs consul-svc ) until we can confirm that all three of the Consul servers have synced up with each other.\nEach  consul-server  section in the logs should be ending with  consul: Adding LAN server [ip address]  or  agent: Join completed .\nIf you see one of the sections ending with  agent: Join failed, retrying in 30s , you need to wait for that server to join the cluster before continuing.", 
            "title": "Part 3: Create the Consul Service"
        }, 
        {
            "location": "/guides/walkthrough/deployment-3/#part-4-update-and-redeploy-the-redis-and-guestbook-applications", 
            "text": "We're going to need the URL of the Consul load balancer again.\nIn each of the CLI.Redis and CLI.Guestbook task definition files, look for the  CONSUL_SERVER_URL  block in the  consul-agent  container and populate the value field with the Consul load balancer's URL, then save the file.\nAt the command prompt, execute the two following commands to create new versions of the deploys for the Redis and Guestbook applications:  l0 deploy create CLI.Redis.Dockerrun.aws.json redis-dpl  l0 deploy create CLI.Guestbook.Dockerrun.aws.json guestbook-dpl  Then, execute the two following commands to redeploy the existing Redis and Guestbook services using those new deploys:  l0 service update --wait redis-svc redis-dpl:latest  l0 service update --wait guestbook-svc guestbook-dpl:latest   Note  Here, we should run  l0 service logs consul-svc  again and confirm that the Consul cluster has discovered these two services.   We can use  l0 loadbalancer get guestbook-lb  to obtain the guestbook application's URL, and then navigate to it with a web browser.\nOur guestbook app should be up and running - this time, it's been deployed without needing to know the address of the Redis backend!  Of course, this is a simple example; in both this deployment and  Deployment 2 , we needed to use  l0 loadbalancer get  to obtain the URL of a load balancer.\nHowever, in a system with many services that uses Consul like this example, we only ever need to find the URL of the Consul cluster - not the URLs of every service that needs to talk to another of our services.", 
            "title": "Part 4: Update and Redeploy the Redis and Guestbook Applications"
        }, 
        {
            "location": "/guides/walkthrough/deployment-3/#part-5-inspect-the-consul-universe-optional", 
            "text": "Let's take a glimpse into how this system that we've deployed works. This requires that we have access to the key pair we've told Layer0 about when we  set it up .", 
            "title": "Part 5: Inspect the Consul Universe (Optional)"
        }, 
        {
            "location": "/guides/walkthrough/deployment-3/#open-ports-for-ssh", 
            "text": "We want to SSH into the Guestbook EC2 instance, which means that we need to tell the Guestbook load balancer to allow SSH traffic through.\nAt the command prompt, execute the following:  l0 loadbalancer addport guestbook-lb 22:22/tcp  We should see output like the following:  LOADBALANCER ID  LOADBALANCER NAME  ENVIRONMENT  SERVICE        PORTS       PUBLIC  URL\nguestbodb65a     guestbook-lb       demo-env     guestbook-svc  80:80/HTTP  true     url \n                                                                22:22/TCP  We need to take note of the load balancer's URL here, too.", 
            "title": "Open Ports for SSH"
        }, 
        {
            "location": "/guides/walkthrough/deployment-3/#ssh-into-the-instance", 
            "text": "At the command prompt, execute the following:  ssh -i /path/to/keypair ec2-user@ guestbook_load_balancer_url  -o ServerAliveInterval=30  (We'll probably be asked if we want to continue connecting - we do, so we'll enter  yes .)  Summary of arguments passed into the above command:   -i /path/to/keypair : this allows us to specify an identity file for use when connecting to the remote machine - in this case, we want to replace  /path/to/keypair  with the actual path to the keypair we created when we set up Layer0  ec2-user@ guestbook_load_balancer_url : the address (here we want to replace  guestbook_load_balancer_url  with the actual URL of the guestbook load balancer) of the machine to which we want to connect and the name of the user ( ec2-user ) that we'd like to connect as  -o : allows us to set parameters on the  ssh  command  ServerAliveInterval=30 : one of those  ssh  parameters - AWS imposes an automatic disconnect if a connection is not active for a certain amount of time, so we use this option to ping every 30 seconds to prevent that automatic disconnect", 
            "title": "SSH Into the Instance"
        }, 
        {
            "location": "/guides/walkthrough/deployment-3/#look-around-you", 
            "text": "We're now inside of the EC2 instance!\nIf we run  docker ps , we should see that our three Docker containers (the Guestbook app, a Consul agent, and Registrator) are up and running, as well as an  amazon-ecs-agent  image.\nBut that's not the Consul universe that we came here to see.\nAt the EC2 instance's command prompt, execute the following:  echo $(curl -s localhost:8500/v1/catalog/services) | jq '.'  We should see output like the following:  {\n   consul : [],\n   consul-8301 : [\n     udp \n  ],\n   consul-8500 : [],\n   consul-8600 : [\n     udp \n  ],\n   guestbook-redis : [],\n   redis : []\n}  Summary of commands passed in the above command:   curl -s localhost:8500/v1/catalog/services : use  curl  to send a GET request to the specified URL, where  localhost:8500  is an HTTP connection to the local Consul agent in this EC2 instance (the  -s  flag just silences excess output from  curl )  | jq '.' : use a pipe ( | ) to take whatever returns from the left side of the pipe and pass it to the  jq  program, which we use here simply to pretty-print the JSON response  echo $(...) : print out whatever returns from running the stuff inside of the parens; not necessary, but it gives us a nice newline after we get our response   In that output, we can see all of the things that our local Consul agent knows about.\nIn addition to a few connections to the Consul server cluster, we can see that it knows about the Guestbook application running in this EC2 instance, as well as the Redis application running in a different instance with its own Consul agent and Registrator.  Let's take a closer look at the Redis service and see how our Guestbook application is locating our Redis application.\nAt the EC2 instance's command prompt, execute the following:  echo $(curl -s http://localhost:8500/v1/catalog/service/redis) | jq '.'  We should see output like the following:  [\n  {\n     ID :  b4bb81e6-fe6a-c630-2553-7f6492ae5275 ,\n     Node :  ip-10-100-230-97.us-west-2.compute.internal ,\n     Address :  10.100.230.97 ,\n     Datacenter :  dc1 ,\n     TaggedAddresses : {\n       lan :  10.100.230.97 ,\n       wan :  10.100.230.97 \n    },\n     NodeMeta : {},\n     ServiceID :  562aceee6935:ecs-l0-tlakedev-redis-dpl-20-redis-e0f989e5af97cdfd0e00:6379 ,\n     ServiceName :  redis ,\n     ServiceTags : [],\n     ServiceAddress :  10.100.230.97 ,\n     ServicePort : 6379,\n     ServiceEnableTagOverride : false,\n     CreateIndex : 761,\n     ModifyIndex : 761\n  }\n]  To  really  see how the Guestbook application connects to Redis, we can take an  even closer  look!  Run  docker ps  to generate a listing of all the containers that Docker is running on the EC2 instance, and note the Container ID for the Guestbook container. Then run the following command to connect to the Guestbook container:  docker exec -it [container_id] /bin/sh  Once we've gotten inside the container, we'll run a similar command to the previous  curl :  curl -s consul-agent:8500/v1/catalog/service/redis  Our Guestbook application makes a call like this one and figures out how to connect to the Redis service by mushing together the information from the  ServiceAddress  and  ServicePort  fields!  To close the  ssh  connection to the EC2 instance, run  exit  in the command prompt.", 
            "title": "Look Around You"
        }, 
        {
            "location": "/guides/walkthrough/deployment-3/#cleanup", 
            "text": "When you're finished with the example, we can instruct Layer0 to terminate the applications and delete the environment.  l0 environment delete demo-env", 
            "title": "Cleanup"
        }, 
        {
            "location": "/guides/walkthrough/deployment-3/#deploy-with-terraform", 
            "text": "As before, we can complete this deployment using Terraform and the Layer0 provider instead of the Layer0 CLI.\nAs before, we will assume that you've cloned the  guides  repo and are working in the  iterative-walkthrough/deployment-3/  directory.  We'll use these files to manage our deployment with Terraform:     Filename  Purpose      Guestbook.Dockerrun.aws.json  Template for running the Guestbook application    main.tf  Provisions resources; populates variables in template files    outputs.tf  Values that Terraform will yield during deployment    Redis.Dockerrun.aws.json  Template for running the Redis application    terraform.tfstate  Tracks status of deployment  (created and managed by Terraform)    terraform.tfvars  Variables specific to the environment and application(s)    variables.tf  Values that Terraform will use during deployment", 
            "title": "Deploy with Terraform"
        }, 
        {
            "location": "/guides/walkthrough/deployment-3/#tf-a-brief-aside-revisited-redux", 
            "text": "In looking at  main.tf , you can see that we're pulling in a Consul module that we maintain (here's the  repo ); this removes the need for a local task definition file.  We also are continuing to use modules for Redis and Guestbook.\nHowever, instead of just sourcing the module and passing in a value or two, you can see that we actually create new deploys from local task definition files and pass those deploys in to the module.\nThis design allows us to use pre-made modules while also offering a great deal of flexibility.\nIf you'd like to follow along the Redis deployment logic chain (the other applications/services work similarly), it goes something like this:   main.tf  creates a deploy for the Redis server by rendering a local task definition and populating it with certain values  main.tf  passes the ID of the deploy into the Redis module, along with other values the module requires  the Redis module  pulls all the variables it knows about (both the defaults in  variables.tf  as well as the ones passed in)  among other Layer0/AWS resources, the module spins up a Redis service; since a deploy ID has been provided, it uses that deploy to create the service instead of a deploy made from a  default task definition  contained within the module", 
            "title": "*.tf: A Brief Aside: Revisited: Redux"
        }, 
        {
            "location": "/guides/walkthrough/deployment-3/#part-1-terraform-get", 
            "text": "Run  terraform get  to pull down all the source materials Terraform needs for our deployment.", 
            "title": "Part 1: Terraform Get"
        }, 
        {
            "location": "/guides/walkthrough/deployment-3/#part-2-terraform-init", 
            "text": "This deployment has provider dependencies so an init call must be made. \n(Terraform v0.11~ requries init)\nAt the command prompt, execute the following command:  terraform init  We should see output like the following:  Initializing modules...\n- module.consul\n- module.redis\n- module.guestbook\n\nInitializing provider plugins...\n- Checking for available provider plugins on https://releases.hashicorp.com...\n- Downloading plugin for provider  template  (1.0.0)...\n\nThe following providers do not have any version constraints in configuration,\nso the latest version was installed.\n\nTo prevent automatic upgrades to new major versions that may contain breaking\nchanges, it is recommended to add version =  ...  constraints to the\ncorresponding provider blocks in configuration, with the constraint strings\nsuggested below.\n\n* provider.template: version =  ~  1.0 \n\nTerraform has been successfully initialized!\n\nYou may now begin working with Terraform. Try running  terraform plan  to see\nany changes that are required for your infrastructure. All Terraform commands\nshould now work.\n\nIf you ever set or change modules or backend configuration for Terraform,\nrerun this command to reinitialize your working directory. If you forget, other\ncommands will detect it and remind you to do so if necessary.", 
            "title": "Part 2: Terraform Init"
        }, 
        {
            "location": "/guides/walkthrough/deployment-3/#part-2-terraform-plan", 
            "text": "As before, we can run  terraform plan  to see what's going to happen.\nWe should see that there are 12 new resources to be created:   the environment  the two local deploys which will be used for Guestbook and Redis  the load balancer, deploy, and service from each of the Consul, Guestbook, and Redis modules  note that even though the default modules' deploys are created, they won't actually be used to deploy services", 
            "title": "Part 2: Terraform Plan"
        }, 
        {
            "location": "/guides/walkthrough/deployment-3/#part-3-terraform-apply", 
            "text": "Run  terraform apply , and we should see output similar to the following:  data.template_file.consul: Refreshing state...\nlayer0_deploy.consul-dpl: Creating...\n\n...\n...\n...\n\nlayer0_service.guestbook-svc: Creation complete\n\nApply complete! Resources: 10 added, 0 changed, 0 destroyed.\n\nThe state of your infrastructure has been saved to the path\nbelow. This state is required to modify and destroy your\ninfrastructure, so keep it safe. To inspect the complete state\nuse the `terraform show` command.\n\nState path: terraform.tfstate\n\nOutputs:\n\nguestbook_url =  http endpoint for the guestbook application    Note  It may take a few minutes for the guestbook service to launch and the load balancer to become available.\nDuring that time you may get HTTP 503 errors when making HTTP requests against the load balancer URL.", 
            "title": "Part 3: Terraform Apply"
        }, 
        {
            "location": "/guides/walkthrough/deployment-3/#whats-happening", 
            "text": "Terraform provisions the AWS resources through Layer0, configures environment variables for the application, and deploys the application into a Layer0 environment.\nTerraform also writes the state of your deployment to the  terraform.tfstate  file (creating a new one if it's not already there).", 
            "title": "What's Happening"
        }, 
        {
            "location": "/guides/walkthrough/deployment-3/#cleanup_1", 
            "text": "When you're finished with the example, you can instruct Terraform to destroy the Layer0 environment, and terminate the application.\nExecute the following command (in the same directory):  terraform destroy  It's also now safe to remove the  .terraform/  directory and the  *.tfstate*  files.", 
            "title": "Cleanup"
        }, 
        {
            "location": "/guides/terraform_beyond_layer0/", 
            "text": "Deployment guide: Terraform beyond Layer0\n#\n\n\nIn this example, we'll learn how you can use Terraform to create a Layer0 service as well as a persistent data store. The main goal of this example is to explore how you can combine Layer0 with other Terraform providers and best practices.\n\n\nBefore you start\n#\n\n\nTo complete the procedures in this section, you must have the following installed and configured correctly:\n\n\n\n\nLayer0 v0.8.4 or later\n\n\nTerraform v0.9.0 or later\n\n\nLayer0 Terraform Provider\n\n\n\n\nIf you have not already configured Layer0, see the \nLayer0 installation guide\n. If you are running an older version of Layer0, see the \nLayer0 upgrade instructions\n.\n\n\nSee the \nTerraform installation guide\n to install Terraform and the Layer0 Terraform Plugin.\n\n\n\n\nDeploy with Terraform\n#\n\n\nUsing Terraform, you will deploy a simple guestbook application backed by AWS DynamoDB Table. The terraform configuration file will use both the Layer0 and AWS Terraform providers, to deploy the guestbook application and provision a new DynamoDB Table.\n\n\nPart 1: Clone the guides repository\n#\n\n\nRun this command to clone the \nquintilesims/guides\n repository:\n\n\ngit clone https://github.com/quintilesims/guides.git\n\n\nOnce you have cloned the repository, navigate to the \nguides/terraform-beyond-layer0/example-1\n folder for the rest of this example.\n\n\nPart 2: Terraform Plan\n#\n\n\n\n\nNote\n\n\nAs we're using modules in our Terraform configuration, we need to run \nterraform get\n command before performing other terraform operations. Running \nterraform get\n will download the modules to your local folder named \n.terraform\n. See here for more information on \nterraform get\n.\n\n\nterraform get\n\n\nGet: file:///Users/\n/go/src/github.com/quintilesims/guides/terraform-beyond-layer0/example-1/modules/guestbook_service\n\n\n\n\nBefore deploying, we can run the following command to see what changes Terraform will make to your infrastructure should you go ahead and apply. If you had any errors in your layer0.tf file, running \nterraform plan\n would output those errors so that you can address them. Also, Terraform will prompt you for configuration values that it does not have.\n\n\n\n\nTip\n\n\nThere are a few ways to configure Terraform so that you don't have to keep entering these values every time you run a Terraform command (editing the \nterraform.tfvars\n file, or exporting environment variables like \nTF_VAR_endpoint\n and \nTF_VAR_token\n, for example). See the \nTerraform Docs\n for more.\n\n\n\n\nterraform plan\n\n\nvar.endpoint\n  Enter a value: \nenter your Layer0 endpoint\n\n\nvar.token\n  Enter a value: \nenter your Layer0 token\n\n...\n+ aws_DynamoDB_table.guestbook\n    arn:                       \ncomputed\n\n    attribute.#:               \n1\n\n    attribute.4228504427.name: \nid\n\n    attribute.4228504427.type: \nS\n\n    hash_key:                  \nid\n\n    name:                      \nguestbook\n\n    read_capacity:             \n20\n\n    stream_arn:                \ncomputed\n\n    stream_enabled:            \ncomputed\n\n    stream_view_type:          \ncomputed\n\n    write_capacity:            \n20\n\n\n...\n\n\n\n\n\nPart 3: Terraform Apply\n#\n\n\nRun the following command to begin the deploy process.\n\n\nterraform apply\n\n\nlayer0_environment.demo: Refreshing state...\n...\n...\n...\nlayer0_service.guestbook: Creation complete\n\nApply complete! Resources: 7 added, 0 changed, 0 destroyed.\n\nThe state of your infrastructure has been saved to the path\nbelow. This state is required to modify and destroy your\ninfrastructure, so keep it safe. To inspect the complete state\nuse the `terraform show` command.\n\nState path: terraform.tfstate\n\nOutputs:\n\nguestbook_url = \nhttp endpoint for the sample application\n\n\n\n\n\n\n\n\nNote\n\n\nIt may take a few minutes for the guestbook service to launch and the load balancer to become available. During that time, you may get HTTP 503 errors when making HTTP requests against the load balancer URL.\n\n\n\n\nTerraform will set up the entire environment for you and then output a link to the application's load balancer.\n\n\nWhat's happening\n#\n\n\nTerraform using the \nAWS provider\n, provisions a new DynamoDB table. It also uses the \nLayer0 provider\n to provision the environment, deploy, load balancer and service required to run the entire guestbook application.\n\n\nLooking at an excerpt of the file \n./terraform-beyond-layer0/example-1/modules/guestbook_service/main.tf\n, we can see the following definitions:\n\n\nresource \naws_dynamodb_table\n \nguestbook\n {\n  name           = \n${\nvar\n.\ntable_name\n}\n\n  read_capacity  = 20\n  write_capacity = 20\n  hash_key       = \nid\n\n\n  attribute {\n    name = \nid\n\n    type = \nS\n\n  }\n}\n\nresource \nlayer0_deploy\n \nguestbook\n {\n  name    = \nguestbook\n\n  content = \n${\ndata\n.\ntemplate_file\n.\nguestbook\n.\nrendered\n}\n\n}\n\ndata \ntemplate_file\n \nguestbook\n {\n  template = \n${\nfile\n(\nDockerrun.aws.json\n)\n}\n\n\n  vars {\n    access_key = \n${\nvar\n.\naccess_key\n}\n\n    secret_key = \n${\nvar\n.\nsecret_key\n}\n\n    region     = \n${\nvar\n.\nregion\n}\n\n    table_name = \n${\naws_dynamodb_table\n.\nguestbook\n.\nname\n}\n\n  }\n}\n\n\n\n\n\nNote the resource definitions for \naws_dynamodb_table\n and \nlayer0_deploy\n. To configure the guestbook application to use the provisioned DynamoDB table, we reference the \nname\n property from the DynamoDB definition \ntable_name = \"${aws_dynamodb_table.guestbook.name}\"\n. \n\n\nThese \nvars\n are used to populate the template fields in our \nDockerrun.aws.json\n file. \n\n\n{\n    \nAWSEBDockerrunVersion\n: 2,\n    \ncontainerDefinitions\n: [\n        {\n            \nname\n: \nguestbook\n,\n            \nimage\n: \nquintilesims/guestbook-db\n,\n            \nessential\n: true,\n            \nmemory\n: 128,\n            \nenvironment\n: [\n                {\n                    \nname\n: \nDYNAMO_TABLE\n,\n                    \nvalue\n: \n${\ntable_name\n}\n\n                }\n                ...\n\n\n\n\n\nThe Layer0 configuration referencing the AWS DynamoDB configuration \ntable_name = \"${aws_DynamoDB_table.guestbook.name}\"\n, infers an implicit dependency. Before Terraform creates the infrastructure, it will use this information to order the resource creation and create resources in parallel, where there are no dependencies. In this example, the AWS DynamoDB table will be created before the Layer0 deploy. See \nTerraform Resource Dependencies\n for more information.\n\n\nPart 4: Scaling a Layer0 Service\n#\n\n\nThe workflow to make changes to your infrastructure generally involves updating your Terraform configuration file followed by a \nterraform plan\n and \nterraform apply\n.\n\n\nUpdate the Terraform configuration\n#\n\n\nOpen the file \n./example-1/modules/guestbook_service/main.tf\n in a text editor and make the change to add a \nscale\n property with a value of \n3\n to the \nlayer0_service\n section. For more information about the \nscale\n property, see \nLayer0 Terraform Plugin\n documentation. The result should look like the below:\n\n\nexample-1/modules/guestbook_service/main.tf\n\n\n# Create a service named \nguestbook\n\nresource \nlayer0_service\n \nguestbook\n {\n  name          = \nguestbook\n\n  environment   = \n${\nlayer0_environment\n.\ndemo\n.\nid\n}\n\n  deploy        = \n${\nlayer0_deploy\n.\nguestbook\n.\nid\n}\n\n  load_balancer = \n${\nlayer0_load_balancer\n.\nguestbook\n.\nid\n}\n\n  scale         = 3\n}\n\n\n\n\n\nPlan and Apply\n#\n\n\nExecute the \nterraform plan\n command to understand the changes that you will be making. Note that if you did not specify \nscale\n, it defaults to '1'.\n\n\nterraform plan\n\n\nOutputs:\n\n\n...\n\n~ module.guestbook.layer0_service.guestbook\n    scale: \n1\n =\n \n3\n\n\n\n\n\n\nNow run the following command to deploy your changes:\n\n\nterraform apply\n\n\nOutputs:\n\n\nlayer0_environment.demo: Refreshing state... (ID: demoenvbb9f6)\ndata.template_file.guestbook: Refreshing state...\nlayer0_deploy.guestbook: Refreshing state... (ID: guestbook.6)\nlayer0_load_balancer.guestbook: Refreshing state... (ID: guestbo43ab0)\nlayer0_service.guestbook: Refreshing state... (ID: guestboebca1)\nlayer0_service.guestbook: Modifying... (ID: guestboebca1)\n  scale: \n1\n =\n \n3\n\nlayer0_service.guestbook: Modifications complete (ID: guestboebca1)\n\nApply complete! Resources: 0 added, 1 changed, 0 destroyed.\n\nThe state of your infrastructure has been saved to the path\nbelow. This state is required to modify and destroy your\ninfrastructure, so keep it safe. To inspect the complete state\nuse the `terraform show` command.\n\nState path: \n\nOutputs:\n\nservices = \nguestbook_service_url\n\n\n\n\n\n\nTo confirm your service has been updated to the desired scale, you can run the following layer0 command. Note that the desired scale for the guestbook service should be eventually be 3/3.\n\n\nl0 service get guestbook1_guestbook_svc\n\nOutputs:\n\n\nSERVICE ID    SERVICE NAME  ENVIRONMENT  LOADBALANCER  DEPLOYMENTS  SCALE\nSERVICE ID    SERVICE NAME              ENVIRONMENT  LOADBALANCER             DEPLOYMENTS                  SCALE\nguestbo4fd3b  guestbook1_guestbook_svc  demo         guestbook1_guestbook_lb  guestbook1_guestbook_dpl:3*  1/3 (2)\n\n\n\n\n\nAs scale is a parameter we are likely to change in the future, rather than hardcoding it to 3 as we have done just now, it would be better to use a variable to store  \nservice_scale\n. The following Best Practices sections will show how you can achieve this.\n\n\n\n\nBest Practices with Terraform + Layer0\n\n\nThe following sections outline some of the best practices and tips to take into consideration, when using Layer0 with Terraform.\n\n\n\n\nPart 5: Terraform Remote State\n#\n\n\nTerraform stores the state of the deployed infrastructure in a local file named \nterraform.tfstate\n by default. To find out more about why Terraform needs to store state, see \nPurpose of Terraform State\n. \n\n\nHow state is loaded and used for operations such as \nterraform apply\n is determined by a \nBackend\n. As mentioned, by default the state is stored locally which is enabled by a \"local\" backend.\n\n\nRemote State\n#\n\n\nBy default, Terraform stores state locally but it can also be configured to store state in a remote backend. This can prove useful when you are working as part of a team to provision and manage services deployed by Terraform. All the members of the team will need access to the state file to apply new changes and be able to do so without overwriting each others' changes. See here for more information on the different \nbackend types\n supported by Terraform.\n\n\nTo configure a remote backend, append the \nterraform\n section below to your terraform file \n./example-1/main.tf\n. Populate the \nbucket\n property to an existing s3 bucket.\n\n\n\n\nTip\n\n\nIf you have been following along with the guide, \n./example-1/main.tf\n should already have the below section commented out. You can uncomment the \nterraform\n section and populate the bucket property with an appropriate value.\n\n\n\n\nterraform {\n  backend \ns3\n {\n    bucket     = \nmy-bucket-name\n\n    key        = \ndemo-env/remote-backend/terraform.tfstate\n\n    region     = \nus-west-2\n\n  }\n}\n\n\n\n\n\nOnce you have modified \nmain.tf\n, you will need to initialize the newly configured backend by running the following command.\n\n\nterraform init\n\n\nOutputs:\n\n\nInitializing the backend...\n\nDo you want to copy state from \nlocal\n to \nconsul\n?\n  ...\n  Do you want to copy the state from \nlocal\n to \nconsul\n? Enter \nyes\n to copy\n  and \nno\n to start with the existing state in \nconsul\n.\n\n  Enter a value: \n\n\n\n\n\nGo ahead and enter: \nyes\n.\n\n\nSuccessfully configured the backend \nconsul\n! Terraform will automatically\nuse this backend unless the backend configuration changes.\n\nTerraform has been successfully initialized!\n...\n\n\n\n\n\nWhat's happening\n#\n\n\nAs you are configuring a backend for the first time, Terraform will give you an option to migrate your state to the new backend. From now on, any further changes to your infrastructure made by Terraform will result in the remote state file being updated. For more information see \nTerraform backends\n.\n\n\nA new team member can use the \nmain.tf\n from their own machine without obtaining a copy of the state file \nterraform.tfstate\n as the configuration will retrieve the state file from the remote backend.\n\n\nLocking\n#\n\n\nNot all remote backends support locking (locking ensures only one person is able to change the state at a time). The \nS3\n backend we used earlier in the example supports locking which is disabled by default. The \nS3\n backend uses a DynamoDB table to acquire a lock before making a change to the state file. To enable locking, you need to specify \nlocking_table\n property with the name of an existing DynamoDB table. The DynamoDB table also needs primary key named \nLockID\n of type \nString\n.\n\n\nSecurity\n#\n\n\nA Terraform state file is written in plain text. This can lead to a situation where deploying resources that require sensitive data can result in the sensitive data being stored in the state file. To minimize exposure of sensitive data, you can enable \nserver side encryption\n of the state file by adding property \nencrypt\n set to \ntrue\n.\n\n\nThis will ensure that the file is encrypted in S3 and by using a remote backend, you will also have the added benefit of the state file not being persisted to disk locally as it will only ever be held in memory by Terraform.\n\n\nFor securing the state file further, you can also enable access logging on the S3 bucket you are using for the remote backend, which can help track down invalid access should it occur.\n\n\nPart 6: Terraform Configuration Structure\n#\n\n\nWhile there are many different approaches to organizing your Terraform code, we suggest using the following file structure:\n\n\nexample1/  # contains overarching Terraform deployment, pulls in any modules that might exist\n  \u2500 main.tf  \n  \u2500 variables.tf  \n  \u2500 output.tf  \n  + modules/  # if you can break up deployment into smaller modules, keep the modules in here\n      + guestbook_service/  # contains Terraform configuration for a module\n        \u2500 main.tf  \n        \u2500 variables.tf  \n        \u2500 output.tf\n      + service2/  # contains another module\n      + service3/  # contains another module\n\n\n\n\n\nHere we are making use of Terraform \nModules\n. Modules in Terraform are self-contained packages of Terraform configurations, that are managed as a group. Modules are used to create reusable components in Terraform as well as for basic code organization. In this example, we are using modules to separate each service and making it consumable as a module.\n\n\nIf you wanted to add a new service, you can create a new service folder inside the ./modules. If you wanted to you could even run multiple copies of the same service. See here for more information about \nCreating Modules\n.\n\n\nAlso see the below repositories for ideas on different ways you can organize your Terraform configuration files for the needs of your specific project: \n\n\n\n\nTerraform Community Modules\n\n\nBest Pratices Ops\n\n\n\n\nPart 7: State Environments\n#\n\n\nLayer0 recommends that you typically make a single environment for each tier of your application, such as \ndev\n, \nstaging\n and \nproduction\n. That recommendation still holds when using Terraform with Layer0. Using Layer0 CLI, you can target a specific environment for most CLI commands. This enables you to service each tier relatively easily. In Terraform, there a few approaches you can take to enable a similar workflow.\n\n\nSingle Terraform Configuration\n#\n\n\nYou can use a single Terraform configuration to create and maintain multiple environments by making use of the \nCount\n parameter, inside a Resource. Count enables you to create multiple copies of a given resource. \n\n\nFor example\n\n\nvariable \nenvironments\n {\n  type = \nlist\n\n\n  default = [\n    \ndev\n,\n    \nstaging\n\n    \nproduction\n\n  ]\n}\n\nresource \nlayer0_environment\n \ndemo\n {\n  count = \n${\nlength\n(\nvar\n.\nenvironments\n)\n}\n\n\n  name = \n${\nvar\n.\nenvironments\n[\ncount\n.\nindex\n]\n}\n_demo\n\n}\n\n\n\n\n\nLet's have a more in-depth look in how this works. You can start by navigating to `./terraform-beyond-layer0/example-2' folder. Start by running the plan command.\n\n\nterraform plan\n\n\nOutputs:\n\n\n+ module.environment.aws_dynamodb_table.guestbook.0\n    ...\n    name:                      \ndev_guestbook\n\n...\n+ module.environment.aws_dynamodb_table.guestbook.1\n    ..\n    name:                      \nstaging_guestbook\n\n...\n\n\n\n\n\nNote that you will see a copy of each resource for each environment specified in your environments file in \n./example-2/variables.tf\n. Go ahead and run apply.\n\n\nterraform apply\n\n\nOutputs:\n\n\nApply complete! Resources: 10 added, 0 changed, 0 destroyed.\n\nOutputs:\n\nguestbook_urls = \n\ndev_url\n\n\nstaging_url\n\n\n\n\n\n\nYou have now created two separate environments using a single terraform configuration: dev \n staging. You can navigate to both the urls output and you should note that they are separate instances of the guestbook application backed with their own separate data store.\n\n\nA common use case for maintaining different environments is to configure each environment slightly differently. For example, you might want to scale your Layer0 service to 3 for staging and leave it as 1 for the dev environment. This can be done easily by using conditional logic to set our \nscale\n parameter in the layer0 service configuration in \n./example-2/main.tf\n. Go ahead and open \nmain.tf\n in a text editor. Navigate to the \nlayer0_service guestbook\n section. Uncomment the scale parameter so that your configuration looks like below.\n\n\nresource \nlayer0_service\n \nguestbook\n {\n  count = \n${\nlength\n(\nvar\n.\nenvironments\n)\n}\n\n\n  name          = \n${\nelement\n(\nlayer0_environment\n.\ndemo\n.*.\nname\n,\n \ncount\n.\nindex\n)\n}\n_guestbook_svc\n\n  environment   = \n${\nelement\n(\nlayer0_environment\n.\ndemo\n.*.\nid\n,\n \ncount\n.\nindex\n)\n}\n\n  deploy        = \n${\nelement\n(\nlayer0_deploy\n.\nguestbook\n.*.\nid\n,\n \ncount\n.\nindex\n)\n}\n\n  load_balancer = \n${\nelement\n(\nlayer0_load_balancer\n.\nguestbook\n.*.\nid\n,\n \ncount\n.\nindex\n)\n}\n\n  scale         = scale         = \n${\nlookup\n(\nvar\n.\nservice_scale\n,\n \nvar\n.\nenvironments\n[\ncount\n.\nindex\n]),\n \n1\n)\n}\n\n}\n\n\n\n\n\nThe variable \nservice_scale\n is already defined in \nvariables.tf\n. If you now go ahead and run plan, you will see that the \nguestbook\n service for only the \nstaging\n environment will be scaled up.\n\n\nterraform plan\n\n\nOutputs:\n\n\n~ layer0_service.guestbook.1\n    scale: \n1\n =\n \n3\n\n\n\n\n\n\nA potential downside of this approach however is that all your environments are using the same state file. Sharing a state file breaks some of the resource encapsulation between environments. Should there ever be a situation where your state file becomes corrupt, it would affect your ability to service all the environments till you resolve the issue by potentially rolling back to a previous copy of the state file. \n\n\nThe next section will show you how you can separate your Terraform environment configuration such that each environment will have its own state file.\n\n\n\n\nNote\n\n\nAs previously mentioned, you will want to avoid hardcoding resource parameter configuration values as much as possible. As an example the scale property of a layer0 service. But this extends to other properties as well like docker image version etc. You should avoid using \nlatest\n and specify a explicit version via configurable variable when possible.\n\n\n\n\nMultiple Terraform Configurations\n#\n\n\nThe previous example used a single set of Terraform Configuration files to create and maintain multiple environments. This resulted in a single state file which had the state information for all the environments. To avoid all environments sharing a single state file, you can split your Terraform configuration so that you a state file for each environment.\n\n\nGo ahead and navigate to \n./terraform-beyond-layer0/example-3\n folder. Here we are using a folder to separate each environment. So \nenv-dev\n and \nenv-staging\n represent a \ndev\n and \nstaging\n environment. To work with either of the environments, you will need to navigate into the desired environment's folder and run Terraform commands. This will ensure that each environment will have its own state file.\n\n\nOpen the env-dev folder inside a text editor. Note that \nmain.tf\n doesn't contain any resource definitions. Instead, we only have one module definition which has various variables being passed in, which is also how we are passing in the \nenvironment\n variable. To create a \ndev\n and \nstaging\n environments for our guestbook application, go ahead and run terraform plan and apply commands from \nenv-dev\n and \nenv-staging\n folders.\n\n\n# assuming you are in the terraform-beyond-layer0/example-3 folder\ncd env-dev\nterraform get\nterraform plan\nterraform apply\n\ncd ../env-staging\nterraform get\nterraform plan\nterraform apply\n\n\n\n\n\nYou should now have two instances of the guestbook application running. Note that our guestbook service in our staging environment has been scaled to 3. We have done this by specifying a map variable \nservice_scale\n in \n./example-3/dev-staging/variables.tf\n which can have different scale values for each environment.\n\n\nPart 8: Multiple Provider Instances\n#\n\n\nYou can define multiple instances of the same provider that is uniquely customized. For example, you can have an \naws\n provider to support multiple regions, different roles etc or in the case of the \nlayer0\n provider, to support multiple layer0 endpoints.\n\n\nFor example:\n\n\n# aws provider\nprovider \naws\n {\n  alias = \neast\n\n  region = \nus-east-1\n\n  # ...\n}\n\n# aws provider configured to a west region\nprovider \naws\n {\n  alias = \nwest\n\n  region = \nus-west-1\n\n  # ...\n}\n\n\n\n\n\nThis will now allow you to reference aws providers configured to a different region. You can do so by referencing the provider using the naming scheme \nTYPE.ALIAS\n, which in the above example results in \naws.west\n. See \nProvider Configuration\n for more information.\n\n\nresource \naws.east_instance\n \nfoo\n {\n  # ...\n}\n\nresource \naws.west_instance\n \nbar\n {\n  # ...\n}\n\n\n\n\n\nPart 9: Cleanup\n#\n\n\nWhen you're finished with the examples in this guide, run the following destroy command in all the following directories to destroy the Layer0 environment, application and the DynamoDB Table.\n\n\nDirectories:  \n\n\n\n\n/example-1  \n\n\n/example-2  \n\n\n/example-3/env-dev  \n\n\n/example-3/env-staging  \n\n\n\n\nterraform destroy\n\n\n\n\nRemote Backend Resources\n\n\nIf you created additional resources (S3 bucket and a DynamoDB Table) separately when configuring a \nRemote Backend\n, do not forget to delete those if they are no longer needed. You should be able to look at your Terraform configuration file \nlayer0.tf\n to determine the name of the bucket and table.", 
            "title": "Terraform beyond Layer0"
        }, 
        {
            "location": "/guides/terraform_beyond_layer0/#deployment-guide-terraform-beyond-layer0", 
            "text": "In this example, we'll learn how you can use Terraform to create a Layer0 service as well as a persistent data store. The main goal of this example is to explore how you can combine Layer0 with other Terraform providers and best practices.", 
            "title": "Deployment guide: Terraform beyond Layer0"
        }, 
        {
            "location": "/guides/terraform_beyond_layer0/#before-you-start", 
            "text": "To complete the procedures in this section, you must have the following installed and configured correctly:   Layer0 v0.8.4 or later  Terraform v0.9.0 or later  Layer0 Terraform Provider   If you have not already configured Layer0, see the  Layer0 installation guide . If you are running an older version of Layer0, see the  Layer0 upgrade instructions .  See the  Terraform installation guide  to install Terraform and the Layer0 Terraform Plugin.", 
            "title": "Before you start"
        }, 
        {
            "location": "/guides/terraform_beyond_layer0/#deploy-with-terraform", 
            "text": "Using Terraform, you will deploy a simple guestbook application backed by AWS DynamoDB Table. The terraform configuration file will use both the Layer0 and AWS Terraform providers, to deploy the guestbook application and provision a new DynamoDB Table.", 
            "title": "Deploy with Terraform"
        }, 
        {
            "location": "/guides/terraform_beyond_layer0/#part-1-clone-the-guides-repository", 
            "text": "Run this command to clone the  quintilesims/guides  repository:  git clone https://github.com/quintilesims/guides.git  Once you have cloned the repository, navigate to the  guides/terraform-beyond-layer0/example-1  folder for the rest of this example.", 
            "title": "Part 1: Clone the guides repository"
        }, 
        {
            "location": "/guides/terraform_beyond_layer0/#part-2-terraform-plan", 
            "text": "Note  As we're using modules in our Terraform configuration, we need to run  terraform get  command before performing other terraform operations. Running  terraform get  will download the modules to your local folder named  .terraform . See here for more information on  terraform get .  terraform get  Get: file:///Users/ /go/src/github.com/quintilesims/guides/terraform-beyond-layer0/example-1/modules/guestbook_service   Before deploying, we can run the following command to see what changes Terraform will make to your infrastructure should you go ahead and apply. If you had any errors in your layer0.tf file, running  terraform plan  would output those errors so that you can address them. Also, Terraform will prompt you for configuration values that it does not have.   Tip  There are a few ways to configure Terraform so that you don't have to keep entering these values every time you run a Terraform command (editing the  terraform.tfvars  file, or exporting environment variables like  TF_VAR_endpoint  and  TF_VAR_token , for example). See the  Terraform Docs  for more.   terraform plan  var.endpoint\n  Enter a value:  enter your Layer0 endpoint \n\nvar.token\n  Enter a value:  enter your Layer0 token \n...\n+ aws_DynamoDB_table.guestbook\n    arn:                        computed \n    attribute.#:                1 \n    attribute.4228504427.name:  id \n    attribute.4228504427.type:  S \n    hash_key:                   id \n    name:                       guestbook \n    read_capacity:              20 \n    stream_arn:                 computed \n    stream_enabled:             computed \n    stream_view_type:           computed \n    write_capacity:             20 \n\n...", 
            "title": "Part 2: Terraform Plan"
        }, 
        {
            "location": "/guides/terraform_beyond_layer0/#part-3-terraform-apply", 
            "text": "Run the following command to begin the deploy process.  terraform apply  layer0_environment.demo: Refreshing state...\n...\n...\n...\nlayer0_service.guestbook: Creation complete\n\nApply complete! Resources: 7 added, 0 changed, 0 destroyed.\n\nThe state of your infrastructure has been saved to the path\nbelow. This state is required to modify and destroy your\ninfrastructure, so keep it safe. To inspect the complete state\nuse the `terraform show` command.\n\nState path: terraform.tfstate\n\nOutputs:\n\nguestbook_url =  http endpoint for the sample application    Note  It may take a few minutes for the guestbook service to launch and the load balancer to become available. During that time, you may get HTTP 503 errors when making HTTP requests against the load balancer URL.   Terraform will set up the entire environment for you and then output a link to the application's load balancer.", 
            "title": "Part 3: Terraform Apply"
        }, 
        {
            "location": "/guides/terraform_beyond_layer0/#whats-happening", 
            "text": "Terraform using the  AWS provider , provisions a new DynamoDB table. It also uses the  Layer0 provider  to provision the environment, deploy, load balancer and service required to run the entire guestbook application.  Looking at an excerpt of the file  ./terraform-beyond-layer0/example-1/modules/guestbook_service/main.tf , we can see the following definitions:  resource  aws_dynamodb_table   guestbook  {\n  name           =  ${ var . table_name } \n  read_capacity  = 20\n  write_capacity = 20\n  hash_key       =  id \n\n  attribute {\n    name =  id \n    type =  S \n  }\n}\n\nresource  layer0_deploy   guestbook  {\n  name    =  guestbook \n  content =  ${ data . template_file . guestbook . rendered } \n}\n\ndata  template_file   guestbook  {\n  template =  ${ file ( Dockerrun.aws.json ) } \n\n  vars {\n    access_key =  ${ var . access_key } \n    secret_key =  ${ var . secret_key } \n    region     =  ${ var . region } \n    table_name =  ${ aws_dynamodb_table . guestbook . name } \n  }\n}  Note the resource definitions for  aws_dynamodb_table  and  layer0_deploy . To configure the guestbook application to use the provisioned DynamoDB table, we reference the  name  property from the DynamoDB definition  table_name = \"${aws_dynamodb_table.guestbook.name}\" .   These  vars  are used to populate the template fields in our  Dockerrun.aws.json  file.   {\n     AWSEBDockerrunVersion : 2,\n     containerDefinitions : [\n        {\n             name :  guestbook ,\n             image :  quintilesims/guestbook-db ,\n             essential : true,\n             memory : 128,\n             environment : [\n                {\n                     name :  DYNAMO_TABLE ,\n                     value :  ${ table_name } \n                }\n                ...  The Layer0 configuration referencing the AWS DynamoDB configuration  table_name = \"${aws_DynamoDB_table.guestbook.name}\" , infers an implicit dependency. Before Terraform creates the infrastructure, it will use this information to order the resource creation and create resources in parallel, where there are no dependencies. In this example, the AWS DynamoDB table will be created before the Layer0 deploy. See  Terraform Resource Dependencies  for more information.", 
            "title": "What's happening"
        }, 
        {
            "location": "/guides/terraform_beyond_layer0/#part-4-scaling-a-layer0-service", 
            "text": "The workflow to make changes to your infrastructure generally involves updating your Terraform configuration file followed by a  terraform plan  and  terraform apply .", 
            "title": "Part 4: Scaling a Layer0 Service"
        }, 
        {
            "location": "/guides/terraform_beyond_layer0/#update-the-terraform-configuration", 
            "text": "Open the file  ./example-1/modules/guestbook_service/main.tf  in a text editor and make the change to add a  scale  property with a value of  3  to the  layer0_service  section. For more information about the  scale  property, see  Layer0 Terraform Plugin  documentation. The result should look like the below:  example-1/modules/guestbook_service/main.tf  # Create a service named  guestbook \nresource  layer0_service   guestbook  {\n  name          =  guestbook \n  environment   =  ${ layer0_environment . demo . id } \n  deploy        =  ${ layer0_deploy . guestbook . id } \n  load_balancer =  ${ layer0_load_balancer . guestbook . id } \n  scale         = 3\n}", 
            "title": "Update the Terraform configuration"
        }, 
        {
            "location": "/guides/terraform_beyond_layer0/#plan-and-apply", 
            "text": "Execute the  terraform plan  command to understand the changes that you will be making. Note that if you did not specify  scale , it defaults to '1'.  terraform plan  Outputs:  ...\n\n~ module.guestbook.layer0_service.guestbook\n    scale:  1  =   3   Now run the following command to deploy your changes:  terraform apply  Outputs:  layer0_environment.demo: Refreshing state... (ID: demoenvbb9f6)\ndata.template_file.guestbook: Refreshing state...\nlayer0_deploy.guestbook: Refreshing state... (ID: guestbook.6)\nlayer0_load_balancer.guestbook: Refreshing state... (ID: guestbo43ab0)\nlayer0_service.guestbook: Refreshing state... (ID: guestboebca1)\nlayer0_service.guestbook: Modifying... (ID: guestboebca1)\n  scale:  1  =   3 \nlayer0_service.guestbook: Modifications complete (ID: guestboebca1)\n\nApply complete! Resources: 0 added, 1 changed, 0 destroyed.\n\nThe state of your infrastructure has been saved to the path\nbelow. This state is required to modify and destroy your\ninfrastructure, so keep it safe. To inspect the complete state\nuse the `terraform show` command.\n\nState path: \n\nOutputs:\n\nservices =  guestbook_service_url   To confirm your service has been updated to the desired scale, you can run the following layer0 command. Note that the desired scale for the guestbook service should be eventually be 3/3.  l0 service get guestbook1_guestbook_svc \nOutputs:  SERVICE ID    SERVICE NAME  ENVIRONMENT  LOADBALANCER  DEPLOYMENTS  SCALE\nSERVICE ID    SERVICE NAME              ENVIRONMENT  LOADBALANCER             DEPLOYMENTS                  SCALE\nguestbo4fd3b  guestbook1_guestbook_svc  demo         guestbook1_guestbook_lb  guestbook1_guestbook_dpl:3*  1/3 (2)  As scale is a parameter we are likely to change in the future, rather than hardcoding it to 3 as we have done just now, it would be better to use a variable to store   service_scale . The following Best Practices sections will show how you can achieve this.   Best Practices with Terraform + Layer0  The following sections outline some of the best practices and tips to take into consideration, when using Layer0 with Terraform.", 
            "title": "Plan and Apply"
        }, 
        {
            "location": "/guides/terraform_beyond_layer0/#part-5-terraform-remote-state", 
            "text": "Terraform stores the state of the deployed infrastructure in a local file named  terraform.tfstate  by default. To find out more about why Terraform needs to store state, see  Purpose of Terraform State .   How state is loaded and used for operations such as  terraform apply  is determined by a  Backend . As mentioned, by default the state is stored locally which is enabled by a \"local\" backend.", 
            "title": "Part 5: Terraform Remote State"
        }, 
        {
            "location": "/guides/terraform_beyond_layer0/#remote-state", 
            "text": "By default, Terraform stores state locally but it can also be configured to store state in a remote backend. This can prove useful when you are working as part of a team to provision and manage services deployed by Terraform. All the members of the team will need access to the state file to apply new changes and be able to do so without overwriting each others' changes. See here for more information on the different  backend types  supported by Terraform.  To configure a remote backend, append the  terraform  section below to your terraform file  ./example-1/main.tf . Populate the  bucket  property to an existing s3 bucket.   Tip  If you have been following along with the guide,  ./example-1/main.tf  should already have the below section commented out. You can uncomment the  terraform  section and populate the bucket property with an appropriate value.   terraform {\n  backend  s3  {\n    bucket     =  my-bucket-name \n    key        =  demo-env/remote-backend/terraform.tfstate \n    region     =  us-west-2 \n  }\n}  Once you have modified  main.tf , you will need to initialize the newly configured backend by running the following command.  terraform init  Outputs:  Initializing the backend...\n\nDo you want to copy state from  local  to  consul ?\n  ...\n  Do you want to copy the state from  local  to  consul ? Enter  yes  to copy\n  and  no  to start with the existing state in  consul .\n\n  Enter a value:   Go ahead and enter:  yes .  Successfully configured the backend  consul ! Terraform will automatically\nuse this backend unless the backend configuration changes.\n\nTerraform has been successfully initialized!\n...", 
            "title": "Remote State"
        }, 
        {
            "location": "/guides/terraform_beyond_layer0/#whats-happening_1", 
            "text": "As you are configuring a backend for the first time, Terraform will give you an option to migrate your state to the new backend. From now on, any further changes to your infrastructure made by Terraform will result in the remote state file being updated. For more information see  Terraform backends .  A new team member can use the  main.tf  from their own machine without obtaining a copy of the state file  terraform.tfstate  as the configuration will retrieve the state file from the remote backend.", 
            "title": "What's happening"
        }, 
        {
            "location": "/guides/terraform_beyond_layer0/#locking", 
            "text": "Not all remote backends support locking (locking ensures only one person is able to change the state at a time). The  S3  backend we used earlier in the example supports locking which is disabled by default. The  S3  backend uses a DynamoDB table to acquire a lock before making a change to the state file. To enable locking, you need to specify  locking_table  property with the name of an existing DynamoDB table. The DynamoDB table also needs primary key named  LockID  of type  String .", 
            "title": "Locking"
        }, 
        {
            "location": "/guides/terraform_beyond_layer0/#security", 
            "text": "A Terraform state file is written in plain text. This can lead to a situation where deploying resources that require sensitive data can result in the sensitive data being stored in the state file. To minimize exposure of sensitive data, you can enable  server side encryption  of the state file by adding property  encrypt  set to  true .  This will ensure that the file is encrypted in S3 and by using a remote backend, you will also have the added benefit of the state file not being persisted to disk locally as it will only ever be held in memory by Terraform.  For securing the state file further, you can also enable access logging on the S3 bucket you are using for the remote backend, which can help track down invalid access should it occur.", 
            "title": "Security"
        }, 
        {
            "location": "/guides/terraform_beyond_layer0/#part-6-terraform-configuration-structure", 
            "text": "While there are many different approaches to organizing your Terraform code, we suggest using the following file structure:  example1/  # contains overarching Terraform deployment, pulls in any modules that might exist\n  \u2500 main.tf  \n  \u2500 variables.tf  \n  \u2500 output.tf  \n  + modules/  # if you can break up deployment into smaller modules, keep the modules in here\n      + guestbook_service/  # contains Terraform configuration for a module\n        \u2500 main.tf  \n        \u2500 variables.tf  \n        \u2500 output.tf\n      + service2/  # contains another module\n      + service3/  # contains another module  Here we are making use of Terraform  Modules . Modules in Terraform are self-contained packages of Terraform configurations, that are managed as a group. Modules are used to create reusable components in Terraform as well as for basic code organization. In this example, we are using modules to separate each service and making it consumable as a module.  If you wanted to add a new service, you can create a new service folder inside the ./modules. If you wanted to you could even run multiple copies of the same service. See here for more information about  Creating Modules .  Also see the below repositories for ideas on different ways you can organize your Terraform configuration files for the needs of your specific project:    Terraform Community Modules  Best Pratices Ops", 
            "title": "Part 6: Terraform Configuration Structure"
        }, 
        {
            "location": "/guides/terraform_beyond_layer0/#part-7-state-environments", 
            "text": "Layer0 recommends that you typically make a single environment for each tier of your application, such as  dev ,  staging  and  production . That recommendation still holds when using Terraform with Layer0. Using Layer0 CLI, you can target a specific environment for most CLI commands. This enables you to service each tier relatively easily. In Terraform, there a few approaches you can take to enable a similar workflow.", 
            "title": "Part 7: State Environments"
        }, 
        {
            "location": "/guides/terraform_beyond_layer0/#single-terraform-configuration", 
            "text": "You can use a single Terraform configuration to create and maintain multiple environments by making use of the  Count  parameter, inside a Resource. Count enables you to create multiple copies of a given resource.   For example  variable  environments  {\n  type =  list \n\n  default = [\n     dev ,\n     staging \n     production \n  ]\n}\n\nresource  layer0_environment   demo  {\n  count =  ${ length ( var . environments ) } \n\n  name =  ${ var . environments [ count . index ] } _demo \n}  Let's have a more in-depth look in how this works. You can start by navigating to `./terraform-beyond-layer0/example-2' folder. Start by running the plan command.  terraform plan  Outputs:  + module.environment.aws_dynamodb_table.guestbook.0\n    ...\n    name:                       dev_guestbook \n...\n+ module.environment.aws_dynamodb_table.guestbook.1\n    ..\n    name:                       staging_guestbook \n...  Note that you will see a copy of each resource for each environment specified in your environments file in  ./example-2/variables.tf . Go ahead and run apply.  terraform apply  Outputs:  Apply complete! Resources: 10 added, 0 changed, 0 destroyed.\n\nOutputs:\n\nguestbook_urls =  dev_url  staging_url   You have now created two separate environments using a single terraform configuration: dev   staging. You can navigate to both the urls output and you should note that they are separate instances of the guestbook application backed with their own separate data store.  A common use case for maintaining different environments is to configure each environment slightly differently. For example, you might want to scale your Layer0 service to 3 for staging and leave it as 1 for the dev environment. This can be done easily by using conditional logic to set our  scale  parameter in the layer0 service configuration in  ./example-2/main.tf . Go ahead and open  main.tf  in a text editor. Navigate to the  layer0_service guestbook  section. Uncomment the scale parameter so that your configuration looks like below.  resource  layer0_service   guestbook  {\n  count =  ${ length ( var . environments ) } \n\n  name          =  ${ element ( layer0_environment . demo .*. name ,   count . index ) } _guestbook_svc \n  environment   =  ${ element ( layer0_environment . demo .*. id ,   count . index ) } \n  deploy        =  ${ element ( layer0_deploy . guestbook .*. id ,   count . index ) } \n  load_balancer =  ${ element ( layer0_load_balancer . guestbook .*. id ,   count . index ) } \n  scale         = scale         =  ${ lookup ( var . service_scale ,   var . environments [ count . index ]),   1 ) } \n}  The variable  service_scale  is already defined in  variables.tf . If you now go ahead and run plan, you will see that the  guestbook  service for only the  staging  environment will be scaled up.  terraform plan  Outputs:  ~ layer0_service.guestbook.1\n    scale:  1  =   3   A potential downside of this approach however is that all your environments are using the same state file. Sharing a state file breaks some of the resource encapsulation between environments. Should there ever be a situation where your state file becomes corrupt, it would affect your ability to service all the environments till you resolve the issue by potentially rolling back to a previous copy of the state file.   The next section will show you how you can separate your Terraform environment configuration such that each environment will have its own state file.   Note  As previously mentioned, you will want to avoid hardcoding resource parameter configuration values as much as possible. As an example the scale property of a layer0 service. But this extends to other properties as well like docker image version etc. You should avoid using  latest  and specify a explicit version via configurable variable when possible.", 
            "title": "Single Terraform Configuration"
        }, 
        {
            "location": "/guides/terraform_beyond_layer0/#multiple-terraform-configurations", 
            "text": "The previous example used a single set of Terraform Configuration files to create and maintain multiple environments. This resulted in a single state file which had the state information for all the environments. To avoid all environments sharing a single state file, you can split your Terraform configuration so that you a state file for each environment.  Go ahead and navigate to  ./terraform-beyond-layer0/example-3  folder. Here we are using a folder to separate each environment. So  env-dev  and  env-staging  represent a  dev  and  staging  environment. To work with either of the environments, you will need to navigate into the desired environment's folder and run Terraform commands. This will ensure that each environment will have its own state file.  Open the env-dev folder inside a text editor. Note that  main.tf  doesn't contain any resource definitions. Instead, we only have one module definition which has various variables being passed in, which is also how we are passing in the  environment  variable. To create a  dev  and  staging  environments for our guestbook application, go ahead and run terraform plan and apply commands from  env-dev  and  env-staging  folders.  # assuming you are in the terraform-beyond-layer0/example-3 folder\ncd env-dev\nterraform get\nterraform plan\nterraform apply\n\ncd ../env-staging\nterraform get\nterraform plan\nterraform apply  You should now have two instances of the guestbook application running. Note that our guestbook service in our staging environment has been scaled to 3. We have done this by specifying a map variable  service_scale  in  ./example-3/dev-staging/variables.tf  which can have different scale values for each environment.", 
            "title": "Multiple Terraform Configurations"
        }, 
        {
            "location": "/guides/terraform_beyond_layer0/#part-8-multiple-provider-instances", 
            "text": "You can define multiple instances of the same provider that is uniquely customized. For example, you can have an  aws  provider to support multiple regions, different roles etc or in the case of the  layer0  provider, to support multiple layer0 endpoints.  For example:  # aws provider\nprovider  aws  {\n  alias =  east \n  region =  us-east-1 \n  # ...\n}\n\n# aws provider configured to a west region\nprovider  aws  {\n  alias =  west \n  region =  us-west-1 \n  # ...\n}  This will now allow you to reference aws providers configured to a different region. You can do so by referencing the provider using the naming scheme  TYPE.ALIAS , which in the above example results in  aws.west . See  Provider Configuration  for more information.  resource  aws.east_instance   foo  {\n  # ...\n}\n\nresource  aws.west_instance   bar  {\n  # ...\n}", 
            "title": "Part 8: Multiple Provider Instances"
        }, 
        {
            "location": "/guides/terraform_beyond_layer0/#part-9-cleanup", 
            "text": "When you're finished with the examples in this guide, run the following destroy command in all the following directories to destroy the Layer0 environment, application and the DynamoDB Table.  Directories:     /example-1    /example-2    /example-3/env-dev    /example-3/env-staging     terraform destroy   Remote Backend Resources  If you created additional resources (S3 bucket and a DynamoDB Table) separately when configuring a  Remote Backend , do not forget to delete those if they are no longer needed. You should be able to look at your Terraform configuration file  layer0.tf  to determine the name of the bucket and table.", 
            "title": "Part 9: Cleanup"
        }, 
        {
            "location": "/guides/one_off_task/", 
            "text": "Deployment guide: Guestbook one-off task\n#\n\n\nIn this example, you will learn how to use layer0 to run a one-off task. A task is used to run a single instance of your Task Definition and is typically a short running job that will be stopped once finished.\n\n\n\n\nBefore you start\n#\n\n\nIn order to complete the procedures in this section, you must install and configure Layer0 v0.8.4 or later. If you have not already configured Layer0, see the \ninstallation guide\n. If you are running an older version of Layer0, see the \nupgrade instructions\n.\n\n\nPart 1: Prepare the task definition\n#\n\n\n\n\nDownload the \nGuestbook One-off Task Definition\n and save it to your computer as \nDockerrun.aws.json\n.\n\n\n\n\nPart 2: Create a deploy\n#\n\n\nNext, you will create a new deploy for the task using the \ndeploy create\n command. At the command prompt, run the following command:\n\n\nl0 deploy create Dockerrun.aws.json one-off-task-dpl\n\n\nYou will see the following output:\n\n\nDEPLOY ID           DEPLOY NAME        VERSION\none-off-task-dpl.1  one-off-task-dpl   1\n\n\n\n\n\nPart 3: Create the task\n#\n\n\nAt this point, you can use the \ntask create\n command to run a copy of the task.\n\n\nTo run the task, use the following command:\n\n\nl0 task create demo-env echo-tsk one-off-task-dpl:latest --wait\n\n\nYou will see the following output:\n\n\nTASK ID       TASK NAME         ENVIRONMENT  DEPLOY              SCALE\none-off851c9  echo-tsk          demo-env     one-off-task-dpl:1  0/1 (1)\n\n\n\n\n\nThe \nSCALE\n column shows the running, desired and pending counts. A value of \n0/1 (1)\n indicates that running = 0, desired = 1 and (1) for 1 pending task that is about to transition to running state. After your task has finished running, note that the desired count will remain 1 and pending value will no longer be shown, so the value will be \n0/1\n for a finished task.\n\n\nPart 4: Check the status of the task\n#\n\n\nTo view the logs for this task, and evaluate its progress, you can use the \ntask logs\n command:\n\n\nl0 task logs one-off-task-tsk\n  \n\n\nYou will see the following output:\n\n\nalpine\n\n\n------\n\nTask finished!\n\n\n\n\n\nYou can also use the following command for more information in the task.\n\n\nl0 -o json task get echo-tsk\n\n\nOutputs:\n\n\n[\n    {\n        \ncopies\n: [\n            {\n                \ndetails\n: [],\n                \nreason\n: \nWaiting for cluster capacity to run\n,\n                \ntask_copy_id\n: \n\n            }\n        ],\n        \ndeploy_id\n: \none-off-task-dpl.2\n,\n        \ndeploy_name\n: \none-off-task-dpl\n,\n        \ndeploy_version\n: \n2\n,\n        \ndesired_count\n: 1,\n        \nenvironment_id\n: \ndemoenv669e4\n,\n        \nenvironment_name\n: \ndemo-env\n,\n        \npending_count\n: 1,\n        \nrunning_count\n: 0,\n        \ntask_id\n: \nechotsk1facd\n,\n        \ntask_name\n: \necho-tsk\n\n    }\n]\n\n\n\n\n\nAfter the task has finished, running \nl0 -o json task get echo-tsk\n will show a pending_count of 0.\n\n\nOutputs:\n\n\n...\n\ncopies\n: [\n    {\n        \ndetails\n: [\n            {\n                \ncontainer_name\n: \nalpine\n,\n                \nexit_code\n: 0,\n                \nlast_status\n: \nSTOPPED\n,\n                \nreason\n: \n\n            }\n        ],\n        \nreason\n: \nEssential container in task exited\n,\n        \ntask_copy_id\n: \narn:aws:ecs:us-west-2:856306994068:task/0e723c3e-9cd1-4914-8393-b59abd40eb89\n\n    }\n],\n...\n\npending_count\n: 0,\n\nrunning_count\n: 0,\n...", 
            "title": "One-off Task"
        }, 
        {
            "location": "/guides/one_off_task/#deployment-guide-guestbook-one-off-task", 
            "text": "In this example, you will learn how to use layer0 to run a one-off task. A task is used to run a single instance of your Task Definition and is typically a short running job that will be stopped once finished.", 
            "title": "Deployment guide: Guestbook one-off task"
        }, 
        {
            "location": "/guides/one_off_task/#before-you-start", 
            "text": "In order to complete the procedures in this section, you must install and configure Layer0 v0.8.4 or later. If you have not already configured Layer0, see the  installation guide . If you are running an older version of Layer0, see the  upgrade instructions .", 
            "title": "Before you start"
        }, 
        {
            "location": "/guides/one_off_task/#part-1-prepare-the-task-definition", 
            "text": "Download the  Guestbook One-off Task Definition  and save it to your computer as  Dockerrun.aws.json .", 
            "title": "Part 1: Prepare the task definition"
        }, 
        {
            "location": "/guides/one_off_task/#part-2-create-a-deploy", 
            "text": "Next, you will create a new deploy for the task using the  deploy create  command. At the command prompt, run the following command:  l0 deploy create Dockerrun.aws.json one-off-task-dpl  You will see the following output:  DEPLOY ID           DEPLOY NAME        VERSION\none-off-task-dpl.1  one-off-task-dpl   1", 
            "title": "Part 2: Create a deploy"
        }, 
        {
            "location": "/guides/one_off_task/#part-3-create-the-task", 
            "text": "At this point, you can use the  task create  command to run a copy of the task.  To run the task, use the following command:  l0 task create demo-env echo-tsk one-off-task-dpl:latest --wait  You will see the following output:  TASK ID       TASK NAME         ENVIRONMENT  DEPLOY              SCALE\none-off851c9  echo-tsk          demo-env     one-off-task-dpl:1  0/1 (1)  The  SCALE  column shows the running, desired and pending counts. A value of  0/1 (1)  indicates that running = 0, desired = 1 and (1) for 1 pending task that is about to transition to running state. After your task has finished running, note that the desired count will remain 1 and pending value will no longer be shown, so the value will be  0/1  for a finished task.", 
            "title": "Part 3: Create the task"
        }, 
        {
            "location": "/guides/one_off_task/#part-4-check-the-status-of-the-task", 
            "text": "To view the logs for this task, and evaluate its progress, you can use the  task logs  command:  l0 task logs one-off-task-tsk     You will see the following output:  alpine  ------ \nTask finished!  You can also use the following command for more information in the task.  l0 -o json task get echo-tsk  Outputs:  [\n    {\n         copies : [\n            {\n                 details : [],\n                 reason :  Waiting for cluster capacity to run ,\n                 task_copy_id :  \n            }\n        ],\n         deploy_id :  one-off-task-dpl.2 ,\n         deploy_name :  one-off-task-dpl ,\n         deploy_version :  2 ,\n         desired_count : 1,\n         environment_id :  demoenv669e4 ,\n         environment_name :  demo-env ,\n         pending_count : 1,\n         running_count : 0,\n         task_id :  echotsk1facd ,\n         task_name :  echo-tsk \n    }\n]  After the task has finished, running  l0 -o json task get echo-tsk  will show a pending_count of 0.  Outputs:  ... copies : [\n    {\n         details : [\n            {\n                 container_name :  alpine ,\n                 exit_code : 0,\n                 last_status :  STOPPED ,\n                 reason :  \n            }\n        ],\n         reason :  Essential container in task exited ,\n         task_copy_id :  arn:aws:ecs:us-west-2:856306994068:task/0e723c3e-9cd1-4914-8393-b59abd40eb89 \n    }\n],\n... pending_count : 0, running_count : 0,\n...", 
            "title": "Part 4: Check the status of the task"
        }, 
        {
            "location": "/reference/cli/", 
            "text": "Layer0 CLI Reference\n#\n\n\nGlobal options\n#\n\n\nThe \nl0\n application is designed to be used with one of several commands: \nadmin\n, \ndeploy\n, \nenvironment\n, \njob\n, \nloadbalancer\n, \nservice\n, and \ntask\n. These commands are detailed in the sections below. There are, however, some global parameters that you may specify whenever using \nl0\n.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 [global options] command subcommand [subcommand options] params\n\n  \n\n\n\n\nGlobal options\n#\n\n\n\n  \n\n    \n-o [text|json], --output [text|json]\n\n    \nSpecify the format of Layer0 outputs. By default, Layer0 outputs unformatted text; by issuing the \n--output json\n option, you can force \nl0\n to output JSON-formatted text.\n\n  \n\n  \n\n    \n-t value, --timeout value\n\n    \nSpecify the timeout for running \nl0\n commands. Values can be in h, m, s, or ms.\n\n  \n\n  \n\n    \n-d, --debug\n\n    \nPrint debug statements\n\n  \n\n  \n\n    \n--version\n\n    \nDisplay the version number of the \nl0\n application.\n\n  \n\n\n\n\n\n\nAdmin\n#\n\n\nThe \nadmin\n command is used to manage the Layer0 API server. This command is used with the following subcommands: \ndebug\n, \nsql\n, and \nversion\n.\n\n\nadmin debug\n#\n\n\nUse the \ndebug\n subcommand to view the running version of your Layer0 API server and CLI.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 admin debug\n\n  \n\n\n\n\nadmin sql\n#\n\n\nUse the \nsql\n subcommand to initialize the Layer0 API database.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 admin sql\n\n  \n\n\n\n\nAdditional information\n#\n\n\n\n  \n\n    \nThe \nsql\n subcommand is automatically executed during the Layer0 installation process; we recommend that you do not use this subcommand unless specifically directed to do so.\n\n  \n\n\n\n\nadmin version\n#\n\n\nUse the \nversion\n subcommand to display the current version of the Layer0 API.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 admin version\n\n  \n\n\n\n\n\n\nDeploy\n#\n\n\ndeploy create\n#\n\n\nUse the \ncreate\n subcommand to upload a Docker task definition into Layer0. This command is used with the following subcommands: \ncreate\n, \ndelete\n, \nget\n and \nlist\n.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 deploy create dockerPath deployName\n\n  \n\n\n\n\nRequired parameters\n#\n\n\n\n  \n\n    \ndockerPath\n\n    \nThe path to the Docker task definition that you want to upload.\n\n  \n\n  \n\n    \ndeployName\n\n    \nA name for the deploy.\n\n  \n\n\n\n\nAdditional information\n#\n\n\n\n  \n\n    \nIf \ndeployName\n exactly matches the name of an existing Layer0 deploy, then the version number of that deploy will increase by 1, and the task definition you specified will replace the task definition specified in the previous version.\n\n  \n \n\n  \n\n    \nIf you use Visual Studio to modify or create your Dockerrun file, you may see an \"Invalid Dockerrun.aws.json\" error. This error is caused by the default encoding used by Visual Studio. See the \n\"Common issues\" page\n for steps to resolve this issue.\n\n  \n  \n\n  \n\n    \n\nDeploys created through Layer0 are rendered with a \nlogConfiguration\n section for each container.\nIf a \nlogConfiguration\n section already exists, no changes are made to the section.\nThe additional section enables logs from each container to be sent to the the Layer0 log group.\nThis is where logs are looked up during \nl0 \nentity\n logs\n commands.\nThe added \nlogConfiguration\n section uses the following template:\n\n\nlogConfiguration\n: {\n    \nlogDriver\n: \nawslogs\n,\n        \noptions\n: {\n            \nawslogs-group\n: \nl0-\nprefix\n,\n            \nawslogs-region\n: \nregion\n,\n            \nawslogs-stream-prefix\n: \nl0\n\n        }\n    }\n}\n\n\n\n\n\n\n  \n\n\n\n\ndeploy delete\n#\n\n\nUse the \ndelete\n subcommand to delete a version of a Layer0 deploy.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 deploy delete deployName\n\n  \n\n\n\n\nRequired parameters\n#\n\n\n\n  \n\n    \ndeployName\n\n    \nThe name of the Layer0 deploy you want to delete.\n\n  \n\n\n\n\ndeploy get\n#\n\n\nUse the \nget\n subcommand to view information about an existing Layer0 deploy.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 deploy get deployName\n\n  \n\n\n\n\nRequired parameters\n#\n\n\n\n  \n\n    \ndeployName\n\n    \nThe name of the Layer0 deploy for which you want to view additional information.\n\n  \n\n\n\n\nAdditional information\n#\n\n\n\n  \n\n    \nThe \nget\n subcommand supports wildcard matching: \nl0 deploy get dep*\n would return all deploys beginning with \ndep\n.\n\n  \n\n\n\n\ndeploy list\n#\n\n\nUse the \nlist\n subcommand to view a list of deploys in your instance of Layer0.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 deploy list\n\n  \n\n\n\n\n\n\nEnvironment\n#\n\n\nLayer0 environments allow you to isolate services and load balancers for specific applications.\nThe \nenvironment\n command is used to manage Layer0 environments. This command is used with the following subcommands: \ncreate\n, \ndelete\n, \nget\n, \nlist\n, and \nsetmincount\n.\n\n\nenvironment create\n#\n\n\nUse the \ncreate\n subcommand to create a new Layer0 environment.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 environment create [--size size | --min-count mincount | --user-data path | --os os | --ami amiID] environmentName\n\n  \n\n\n\n\nRequired parameters\n#\n\n\n\n  \n\n    \nenvironmentName\n\n    \nA name for the environment.\n\n  \n\n\n\n\nOptional arguments\n#\n\n\n\n  \n\n    \n--size size\n\n    \nThe instance size of the EC2 instances to create in your environment (default: m3.medium).\n\n  \n\n    \n\n    \n--min-count mincount\n\n    \nThe minimum number of EC2 instances allowed in the environment's autoscaling group (default: 0).\n\n  \n\n  \n\n    \n--user-data path\n\n    \nThe user data template file to use for the environment's autoscaling group.\n\n  \n\n  \n\n    \n--os os\n\n    \nThe operating system used in the environment. Options are \"linux\" or \"windows\" (default: linux).\n        More information on windows environments is documented below\n\n  \n\n  \n\n    \n--ami amiID\n\n    \nA custom EC2 AMI ID to use in the environment. If not specified, Layer0 will use its default AMI ID for the specified operating system.\n\n  \n\n\n\n\nThe user data template can be used to add custom configuration to your Layer0 environment. They are usually scripts that are executed at instance launch time to ensure an EC2 instance is in the correct state after the provisioning process finishes.\nLayer0 uses \nGo Templates\n to render user data.\nCurrently, two variables are passed into the template: \nECSEnvironmentID\n and \nS3Bucket\n.\nPlease review the \nECS Tutorial\n\nto better understand how to write a user data template, and use at your own risk!\n\n\nLinux Environments\n: The default Layer0 user data template is:\n\n\n#!/bin/bash\n\n\necho\n \nECS_CLUSTER\n={{\n .ECSEnvironmentID \n}}\n \n /etc/ecs/ecs.config\n\necho\n \nECS_ENGINE_AUTH_TYPE\n=\ndockercfg \n /etc/ecs/ecs.config\nyum install -y aws-cli awslogs jq\naws s3 cp s3://\n{{\n .S3Bucket \n}}\n/bootstrap/dockercfg dockercfg\n\ncfg\n=\n$(\ncat dockercfg\n)\n\n\necho\n \nECS_ENGINE_AUTH_DATA\n=\n$cfg\n \n /etc/ecs/ecs.config\ndocker pull amazon/amazon-ecs-agent:latest\nstart ecs\n\n\n\n\n\nWindows Environments\n: The default Layer0 user data template is:\n\n\npowershell\n\n\n#\n \nSet\n \nagent\n \nenv\n \nvariables\n \nfor\n \nthe\n \nMachine\n \ncontext\n \n(\ndurable\n)\n\n\n$clusterName\n \n=\n \n{{ .ECSEnvironmentID }}\n\n\nWrite\n-\nHost\n \nCluster\n \nname\n \nset\n \nas\n:\n \n$clusterName\n \n-\nforeground\n \ngreen\n\n\n\n[\nEnvironment\n]\n::\nSetEnvironmentVariable\n(\nECS_CLUSTER\n,\n \n$clusterName\n,\n \nMachine\n)\n\n\n[\nEnvironment\n]\n::\nSetEnvironmentVariable\n(\nECS_ENABLE_TASK_IAM_ROLE\n,\n \nfalse\n,\n \nMachine\n)\n\n\n$agentVersion\n \n=\n \nv1.14.0-1.windows.1\n\n\n$agentZipUri\n \n=\n \nhttps://s3.amazonaws.com/amazon-ecs-agent/ecs-agent-windows-$agentVersion.zip\n\n\n$agentZipMD5Uri\n \n=\n \n$agentZipUri.md5\n\n\n\n#\n \nConfigure\n \ndocker\n \nauth\n\n\nRead\n-\nS3Object\n \n-\nBucketName\n \n{{\n \n.\nS3Bucket\n \n}}\n \n-\nKey\n \nbootstrap\n/\ndockercfg\n \n-\nFile\n \ndockercfg\n.\njson\n\n\n$dockercfgContent\n \n=\n \n[\nIO\n.\nFile\n]\n::\nReadAllText\n(\ndockercfg.json\n)\n\n\n[\nEnvironment\n]\n::\nSetEnvironmentVariable\n(\nECS_ENGINE_AUTH_DATA\n,\n \n$dockercfgContent\n,\n \nMachine\n)\n\n\n[\nEnvironment\n]\n::\nSetEnvironmentVariable\n(\nECS_ENGINE_AUTH_TYPE\n,\n \ndockercfg\n,\n \nMachine\n)\n\n\n\n###\n \n---\n \nNothing\n \nuser\n \nconfigurable\n \nafter\n \nthis\n \npoint\n \n---\n\n\n$ecsExeDir\n \n=\n \n$env:ProgramFiles\\Amazon\\ECS\n\n\n$zipFile\n \n=\n \n$env:TEMP\\ecs-agent.zip\n\n\n$md5File\n \n=\n \n$env:TEMP\\ecs-agent.zip.md5\n\n\n\n###\n \nGet\n \nthe\n \nfiles\n \nfrom\n \nS3\n\n\nInvoke\n-\nRestMethod\n \n-\nOutFile\n \n$zipFile\n \n-\nUri\n \n$agentZipUri\n\n\nInvoke\n-\nRestMethod\n \n-\nOutFile\n \n$md5File\n \n-\nUri\n \n$agentZipMD5Uri\n\n\n\n##\n \nMD5\n \nChecksum\n\n\n$expectedMD5\n \n=\n \n(\nGet\n-\nContent\n \n$md5File\n)\n\n\n$md5\n \n=\n \nNew\n-\nObject\n \n-\nTypeName\n \nSystem\n.\nSecurity\n.\nCryptography\n.\nMD5CryptoServiceProvider\n\n\n$actualMD5\n \n=\n \n[\nSystem\n.\nBitConverter\n]\n::\nToString\n(\n$md5\n.\nComputeHash\n([\nSystem\n.\nIO\n.\nFile\n]\n::\nReadAllBytes\n(\n$zipFile\n))).\nreplace\n(\n-\n,\n \n)\n\n\nif\n(\n$expectedMD5\n \n-\nne\n \n$actualMD5\n)\n \n{\n\n    \necho\n \nDownload doesn\nt match hash.\n\n    \necho\n \nExpected: $expectedMD5 - Got: $actualMD5\n\n    \nexit\n \n1\n\n\n}\n\n\n\n##\n \nPut\n \nthe\n \nexecutables\n \nin\n \nthe\n \nexecutable\n \ndirectory\n.\n\n\nExpand\n-\nArchive\n \n-\nPath\n \n$zipFile\n \n-\nDestinationPath\n \n$ecsExeDir\n \n-\nForce\n\n\n\n##\n \nStart\n \nthe\n \nagent\n \nscript\n \nin\n \nthe\n \nbackground\n.\n\n\n$jobname\n \n=\n \nECS-Agent-Init\n\n\n$script\n \n=\n  \ncd \n$ecsExeDir\n; .\\amazon-ecs-agent.ps1\n\n\n$repeat\n \n=\n \n(\nNew\n-\nTimeSpan\n \n-\nMinutes\n \n1\n)\n\n\n$jobpath\n \n=\n \n$env\n:\nLOCALAPPDATA\n \n+\n \n\\Microsoft\\Windows\\PowerShell\\ScheduledJobs\\$jobname\\ScheduledJobDefinition.xml\n\n\n\nif\n(\n$\n(\nTest\n-\nPath\n \n-\nPath\n \n$jobpath\n))\n \n{\n\n  \necho\n \nJob definition already present\n\n  \nexit\n \n0\n\n\n}\n\n\n\n$scriptblock\n \n=\n \n[\nscriptblock\n]\n::\nCreate\n(\n$script\n)\n\n\n$trigger\n \n=\n \nNew\n-\nJobTrigger\n \n-\nAt\n \n(\nGet\n-\nDate\n).\nDate\n \n-\nRepeatIndefinitely\n \n-\nRepetitionInterval\n \n$repeat\n \n-\nOnce\n\n\n$options\n \n=\n \nNew\n-\nScheduledJobOption\n \n-\nRunElevated\n \n-\nContinueIfGoingOnBattery\n \n-\nStartIfOnBattery\n\n\nRegister\n-\nScheduledJob\n \n-\nName\n \n$jobname\n \n-\nScriptBlock\n \n$scriptblock\n \n-\nTrigger\n \n$trigger\n \n-\nScheduledJobOption\n \n$options\n \n-\nRunNow\n\n\nAdd\n-\nJobTrigger\n \n-\nName\n \n$jobname\n \n-\nTrigger\n \n(\nNew\n-\nJobTrigger\n \n-\nAtStartup\n \n-\nRandomDelay\n \n00\n:\n1\n:\n00\n)\n\n\n/powershell\n\n\npersist\ntrue\n/persist\n\n\n\n\n\n\n\n\nWindows Environments\nWindows containers are still in beta. \n\n\n\n\n\n\nYou can view the documented caveats with ECS \nhere\n.\nWhen creating Windows environments in Layer0, the root volume sizes for instances are 200GiB to accommodate the large size of the containers.\n\nIt can take as long as 45 minutes for a new windows container to come online. \n\n\nenvironment delete\n#\n\n\nUse the \ndelete\n subcommand to delete an existing Layer0 environment.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 environment delete [--wait] environmentName\n\n  \n\n\n\n\nRequired parameters\n#\n\n\n\n  \n\n    \nenvironmentName\n\n    \nThe name of the Layer0 environment that you want to delete.\n\n  \n\n\n\n\nOptional arguments\n#\n\n\n\n  \n\n    \n--wait\n\n    \nWait until the deletion is complete before exiting.\n\n  \n\n\n\n\nAdditional information\n#\n\n\n\n  \n\n    \nThis operation performs several tasks asynchronously. When run without the \n--wait\n option, this operation will most likely exit before all of these tasks are complete; when run with the \n--wait\n option, this operation will only exit once these tasks have completed.\n\n  \n\n\n\n\nenvironment get\n#\n\n\nUse the \nget\n subcommand to display information about an existing Layer0 environment.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 environment get environmentName\n\n  \n\n\n\n\nRequired parameters\n#\n\n\n\n  \n\n    \nenvironmentName\n\n    \nThe name of the Layer0 environment for which you want to view additional information.\n\n  \n\n\n\n\nAdditional information\n#\n\n\n\n  \n\n    \nThe \nget\n subcommand supports wildcard matching: \nl0 environment get test*\n would return all environments beginning with \ntest\n.\n\n  \n\n\n\n\nenvironment list\n#\n\n\nUse the \nlist\n subcommand to display a list of environments in your instance of Layer0.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 environment list\n\n  \n\n\n\n\nenvironment setmincount\n#\n\n\nUse the \nsetmincount\n subcommand to set the minimum number of EC2 instances allowed the environment's autoscaling group.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 enviroment setmincount environmentName count\n\n  \n\n\n\n\nRequired parameters\n#\n\n\n\n  \n\n    \nenvironmentName\n\n    \nThe name of the Layer0 environment that you want to adjust.\n\n  \n\n  \n\n    \ncount\n\n    \nThe minimum number of instances allowed in the environment's autoscaling group.\n\n  \n\n\n\n\nenvironment link\n#\n\n\nUse the \nlink\n subcommand to link two environments together. \nWhen environments are linked, services inside the environments are allowed to communicate with each other as if they were in the same environment. \nThis link is bidirectional. \nThis command is idempotent; it will succeed even if the two specified environments are already linked.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 environment link sourceEnvironmentName destEnvironmentName\n\n  \n\n\n\n\nRequired parameters\n#\n\n\n\n  \n\n    \nsourceEnvironmentName\n\n    \nThe name of the first environment to link.\n\n  \n\n  \n\n    \ndestEnvironmentName\n\n    \nThe name of the second environment to link. \n\n  \n\n\n\n\nenvironment unlink\n#\n\n\nUse the \nunlink\n subcommand to remove the link between two environments.\nThis command is idempotent; it will succeed even if the link does not exist.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 environment unlink sourceEnvironmentName destEnvironmentName\n\n  \n\n\n\n\nRequired parameters\n#\n\n\n\n  \n\n    \nsourceEnvironmentName\n\n    \nThe name of the first environment to unlink.\n\n  \n\n  \n\n    \ndestEnvironmentName\n\n    \nThe name of the second environment to unlink. \n\n  \n\n\n\n\n\n\nJob\n#\n\n\nA Job is a long-running unit of work performed on behalf of the Layer0 API.\nJobs are executed as Layer0 tasks that run in the \napi\n environment.\nThe \njob\n command is used with the following subcommands: \nlogs\n, \ndelete\n, \nget\n, and \nlist\n.\n\n\njob logs\n#\n\n\nUse the \nlogs\n subcommand to display the logs from a Layer0 job that is currently running.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 job logs [--start MM/DD HH:MM | --end MM/DD HH:MM | --tail=N] jobName\n\n  \n\n\n\n\nRequired parameters\n#\n\n\n\n  \n\n    \njobName\n\n    \nThe name of the Layer0 job for which you want to view logs.\n\n  \n\n\n\n\nOptional arguments\n#\n\n\n\n  \n\n    \n--start MM/DD HH:MM\n\n    \nThe start of the time range to fetch logs.\n\n  \n\n  \n\n    \n--end MM/DD HH:MM\n\n    \nThe end of the time range to fetch logs.\n\n  \n\n  \n\n    \n--tail=N\n\n    \nDisplay only the last \nN\n lines of the log.\n\n  \n\n\n\n\njob delete\n#\n\n\nUse the \ndelete\n subcommand to delete an existing job.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 job delete jobName\n\n  \n\n\n\n\nRequired parameters\n#\n\n\n\n  \n\n    \njobName\n\n    \nThe name of the job that you want to delete.\n\n  \n\n\n\n\njob get\n#\n\n\nUse the \nget\n subcommand to display information about an existing Layer0 job.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 job get jobName\n\n  \n\n\n\n\nRequired parameters\n#\n\n\n\n  \n\n    \njobName\n\n    \nThe name of an existing Layer0 job to display.\n\n  \n\n\n\n\nAdditional information\n#\n\n\n\n  \n\n    \nThe \nget\n subcommand supports wildcard matching: \nl0 job get 2a55*\n would return all jobs beginning with \n2a55\n.\n\n  \n\n\n\n\njob list\n#\n\n\nUse the \nlist\n subcommand to display information about all of the existing jobs in an instance of Layer0.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 job list\n\n  \n\n\n\n\n\n\nLoadbalancer\n#\n\n\nA load balancer is a component of a Layer0 environment. Load balancers listen for traffic on certain ports, and then forward that traffic to Layer0 \nservices\n. The \nloadbalancer\n command is used with the following subcommands: \ncreate\n, \ndelete\n, \naddport\n, \ndropport\n, \nget\n, \nlist\n, and \nhealthcheck\n.\n\n\nloadbalancer create\n#\n\n\nUse the \ncreate\n subcommand to create a new load balancer.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 loadbalancer create [--port hostPort:containerPort/protocol ... | --certificate certifiateName | --private | --healthcheck-target target | --healthcheck-interval interval | --healthcheck-timeout timeout | --healthcheck-healthy-threshold healthyThreshold | --healthcheck-unhealthy-threshold unhealthyThreshold] environmentName loadBalancerName\n\n  \n\n\n\n\nRequired parameters\n#\n\n\n\n  \n\n    \nenvironmentName\n\n    \nThe name of the existing Layer0 environment in which you want to create the load balancer.\n\n  \n\n  \n\n    \nloadBalancerName\n\n    \nA name for the load balancer you are creating.\n\n  \n\n\n\n\nOptional arguments\n#\n\n\n\n  \n\n    \n\n      \n--port hostPort:containerPort/protocol ...\n\n    \n\n    \n\n      \nThe port configuration for the load balancer. Multiple ports can be specified using \n--port xxx --port yyy\n. \nhostPort\n is the port on which the load balancer will listen for traffic; \ncontainerPort\n is the port that traffic will be forwarded to. If this option is not specified, Layer0 will use the following configuration: 80:80/tcp\n\n    \n\n  \n\n  \n\n    \n\n      \n--certificate certificateName\n\n    \n\n    \n\n      \nThe name of an existing Layer0 certificate. You must include this option if you are using an HTTPS port configuration.\n\n    \n\n  \n\n  \n\n    \n\n      \n--private\n\n    \n\n    \n\n      \nWhen you use this option, the load balancer will only be accessible from within the Layer0 environment.\n\n    \n\n  \n\n  \n\n    \n\n      \n--healthcheck-target target\n\n    \n\n    \n\n      \nThe target of the check. Valid pattern is \nPROTOCOL:PORT/PATH\n \n(default: \n\"TCP:80\"\n)\n\n      \n\n      If PROTOCOL is \nHTTP\n or \nHTTPS\n, both PORT and PATH are required\n      \n\n      - \nexample: \nHTTP:80/admin/healthcheck\n\n      \n\n      If PROTOCOL is \nTCP\n or \nSSL\n, PORT is required and PATH is not supported\n      \n\n      - \nexample: \nTCP:80\n\n    \n\n  \n\n  \n\n    \n\n      \n--healthcheck-interval interval\n\n    \n\n    \n\n      \nThe interval between checks \n(default: \n30\n)\n.\n\n    \n\n  \n\n  \n\n    \n\n      \n--healthcheck-timeout timeout\n\n    \n\n    \n\n      \nThe length of time before the check times out \n(default: \n5\n)\n.\n\n    \n\n  \n\n  \n\n    \n\n      \n--healthcheck-healthy-threshold healthyThreshold\n\n    \n\n    \n\n      \nThe number of checks before the instance is declared healthy \n(default: \n2\n)\n.\n\n    \n\n  \n\n  \n\n    \n\n      \n--healthcheck-unhealthy-threshold unhealthyThreshold\n\n    \n\n    \n\n      \nThe number of checks before the instance is declared unhealthy \n(default: \n2\n)\n.\n\n    \n\n  \n\n\n\n\n\n\nPorts and Health Checks\n\n\nWhen both the \n--port\n and the \n--healthcheck-target\n options are omitted, Layer0 configures the load balancer with some default values: \n80:80/tcp\n for ports and \ntcp:80\n for healthcheck target.\nThese default values together create a load balancer configured with a simple but functioning health check, opening up a set of ports that allows traffic to the target of the healthcheck.\n(\n--healthcheck-target tcp:80\n tells the load balancer to ping its services at port 80 to determine their status, and \n--port 80:80/tcp\n configures a security group to allow traffic to pass between port 80 of the load balancer and port 80 of its services)\n\n\nWhen creating a load balancer with non-default configurations for either \n--port\n or \n--healthcheck-target\n, make sure that a valid \n--port\n and \n--healthcheck-target\n pairing is also created.\n\n\n\n\nloadbalancer delete\n#\n\n\nUse the \ndelete\n subcommand to delete an existing load balancer.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 loadbalancer delete [--wait] loadBalancerName\n\n  \n\n\n\n\nRequired parameters\n#\n\n\n\n  \n\n    \nloadBalancerName\n\n    \nThe name of the load balancer that you want to delete.\n\n  \n\n\n\n\nOptional arguments\n#\n\n\n\n  \n\n    \n--wait\n\n    \nWait until the deletion is complete before exiting.\n\n  \n\n\n\n\nAdditional information\n#\n\n\n\n  \n\n    \nIn order to delete a load balancer that is already attached to a service, you must first delete the service that uses the load balancer.\n\n  \n\n  \n\n    \nThis operation performs several tasks asynchronously. When run without the \n--wait\n option, this operation will most likely exit before all of these tasks are complete; when run with the \n--wait\n option, this operation will only exit once these tasks have completed.\n\n  \n\n\n\n\nloadbalancer addport\n#\n\n\nUse the \naddport\n subcommand to add a new port configuration to an existing Layer0 load balancer.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 loadbalancer addport [--certificate certificateName] loadBalancerName hostPort:containerPort/protocol\n\n  \n\n\n\n\nRequired parameters\n#\n\n\n\n  \n\n    \nloadBalancerName\n\n    \nThe name of an existing Layer0 load balancer in which you want to add the port configuration.\n\n  \n\n  \n\n    \nhostPort\n\n    \nThe port that the load balancer will listen on.\n\n  \n\n  \n\n    \ncontainerPort\n\n    \nThe port that the load balancer will forward traffic to.\n\n  \n\n  \n\n    \nprotocol\n\n    \nThe protocol to use when forwarding traffic (acceptable values: tcp, ssl, http, and https).\n\n  \n\n\n\n\nOptional arguments\n#\n\n\n\n  \n\n    \n--certificate certificateName\n\n    \nThe name of an existing Layer0 certificate. You must include this option if you are using an HTTPS port configuration.\n\n  \n\n\n\n\nAdditional information\n#\n\n\n\n  \n\n    \nThe port configuration you specify must not already be in use by the load balancer you specify.\n\n  \n\n\n\n\nloadbalancer dropport\n#\n\n\nUse the \ndropport\n subcommand to remove a port configuration from an existing Layer0 load balancer.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 loadbalancer dropport loadBalancerName hostPort\n\n  \n\n\n\n\nRequired parameters\n#\n\n\n\n  \n\n    \nloadBalancerName\n\n    \nThe name of an existing Layer0 load balancer from which you want to remove the port configuration.\n\n  \n\n  \n\n    \nhostPort\n\n    \nThe host port to remove from the load balancer.\n\n  \n\n\n\n\nloadbalancer get\n#\n\n\nUse the \nget\n subcommand to display information about an existing Layer0 load balancer.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 loadbalancer get [environmentName:]loadBalancerName\n\n  \n\n\n\n\nRequired parameters\n#\n\n\n\n  \n\n    \n[environmentName:]loadBalancerName\n\n    \nThe name of an existing Layer0 load balancer. You can optionally provide the Layer0 environment (\nenvironmentName\n) associated with the Load Balancer\n\n  \n\n\n\n\nAdditional information\n#\n\n\n\n  \n\n    \nThe \nget\n subcommand supports wildcard matching: \nl0 loadbalancer get entrypoint*\n would return all jobs beginning with \nentrypoint\n.\n\n  \n\n\n\n\nloadbalancer list\n#\n\n\nUse the \nlist\n subcommand to display information about all of the existing load balancers in an instance of Layer0.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 loadbalancer list\n\n  \n\n\n\n\nloadbalancer healthcheck\n#\n\n\nUse the \nhealthcheck\n subcommand to display information about or update the configuration of a load balancer's health check.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 loadbalancer healthcheck [--set-target target | --set-interval interval | --set-timeout timeout | --set-healthy-threshold healthyThreshold | --set-unhealthy-threshold unhealthyThreshold] loadbalancerName\n\n  \n\n\n\n\nOptional arguments\n#\n\n\n\n  \n\n    \n\n      \n--set-target target\n\n    \n\n    \n\n      \nThe target of the check. Valid pattern is \nPROTOCOL:PORT/PATH\n, where PROTOCOL values are:\n      \n\n      \nHTTP\n or \nHTTPS\n: both PORT and PATH are required\n      \n\n      - \nexample: \nHTTP:80/admin/healthcheck\n\n      \n\n      \nTCP\n or \nSSL\n: PORT is required, PATH is not supported\n      \n\n      - \nexample: \nTCP:80\n\n    \n\n  \n\n  \n\n    \n\n      \n--set-interval interval\n\n    \n\n    \n\n      \nThe interval between checks.\n\n    \n\n  \n\n  \n\n    \n\n      \n--set-timeout timeout\n\n    \n\n    \n\n      \nThe length of time before the check times out.\n\n    \n\n  \n\n  \n\n    \n\n      \n--set-healthy-threshold healthyThreshold\n\n    \n\n    \n\n      \nThe number of checks before the instance is declared healthy.\n\n    \n\n  \n\n  \n\n    \n\n      \n--set-unhealthy-threshold unhealthyThreshold\n\n    \n\n    \n\n      \nThe number of checks before the instance is declared unhealthy.\n\n    \n\n  \n\n\n\n\nAdditional information\n#\n\n\n\n  \n\n    \nCalling the subcommand without flags will display the current configuration of the load balancer's health check. Setting any of the flags will update the corresponding field in the health check, and all omitted flags will leave the corresponding fields unchanged.\n\n  \n\n\n\n\n\n\n\nService\n#\n\n\nA service is a component of a Layer0 environment. The purpose of a service is to execute a Docker image specified in a \ndeploy\n. In order to create a service, you must first create an \nenvironment\n and a \ndeploy\n; in most cases, you should also create a \nload balancer\n before creating the service.\n\n\nThe \nservice\n command is used with the following subcommands: \ncreate\n, \ndelete\n, \nget\n, \nupdate\n, \nlist\n, \nlogs\n, and \nscale\n.\n\n\nservice create\n#\n\n\nUse the \ncreate\n subcommand to create a Layer0 service.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 service create [--loadbalancer [environmentName:]loadBalancerName | --no-logs] environmentName serviceName deployName[:deployVersion]\n\n  \n\n\n\n\nRequired parameters\n#\n\n\n\n  \n\n    \nserviceName\n\n    \nA name for the service that you are creating.\n\n  \n\n  \n\n    \nenvironmentName\n\n    \nThe name of an existing Layer0 environment.\n\n  \n\n  \n\n    \ndeployName[:deployVersion]\n\n    \nThe name of a Layer0 deploy that exists in the environment \nenvironmentName\n. You can optionally specify the version number of the Layer0 deploy that you want to deploy. If you do not specify a version number, the latest version of the deploy will be used.\n\n  \n\n\n\n\nOptional arguments\n#\n\n\n\n  \n\n    \n--loadbalancer [environmentName:]loadBalancerName\n\n    \nPlace the new service behind an existing load balancer named \nloadBalancerName\n. You can optionally specify the Layer0 environment (\nenvironmentName\n) where the Load Balancer exists.\n\n  \n\n  \n\n    \n--no-logs\n\n    \nDisable cloudwatch logging for the service\n\n  \n\n\n\n\nservice update\n#\n\n\nUse the \nupdate\n subcommand to apply an existing Layer0 Deploy to an existing Layer0 service.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 service update [--no-logs] [environmentName:]serviceName deployName[:deployVersion]\n\n  \n\n\n\n\nRequired parameters\n#\n\n\n\n  \n\n    \n[environmentName:]serviceName\n\n    \nThe name of an existing Layer0 service into which you want to apply the deploy. You can optionally specify the Layer0 environment (\nenvironmentName\n) of the service.\n\n  \n\n  \n\n    \ndeployName[:deployVersion]\n\n    \nThe name of the Layer0 deploy that you want to apply to the service. You can optionally specify a specific version of the deploy (\ndeployVersion\n). If you do not specify a version number, the latest version of the deploy will be applied.\n\n  \n\n  \n\n    \n--no-logs\n\n    \nDisable cloudwatch logging for the service\n\n  \n\n\n\n\nAdditional information\n#\n\n\n\n  \n\n    \nIf your service uses a load balancer, when you update the task definition for the service, the container name and container port that were specified when the service was created must remain the same in the task definition. In other words, if your service has a load balancer, you cannot apply any deploy you want to that service. If you are varying the container name or exposed ports, you must create a new service instead.\n\n  \n\n\n\n\n\nservice delete\n#\n\n\nUse the \ndelete\n subcommand to delete an existing Layer0 service.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 service delete [--wait] [environmentName:]serviceName\n\n  \n\n\n\n\nRequired parameters\n#\n\n\n\n  \n\n    \n[environmentName:]serviceName\n\n    \nThe name of the Layer0 service that you want to delete. You can optionally provide the Layer0 environment (\nenvironmentName\n) of the service.\n\n  \n\n\n\n\nOptional arguments\n#\n\n\n\n  \n\n    \n--wait\n\n    \nWait until the deletion is complete before exiting.\n\n  \n\n\n\n\nAdditional information\n#\n\n\n\n  \n\n    \nThis operation performs several tasks asynchronously. When run without the \n--wait\n option, this operation will most likely exit before all of these tasks are complete; when run with the \n--wait\n option, this operation will only exit once these tasks have completed.\n\n  \n\n\n\n\nservice get\n#\n\n\nUse the \nget\n subcommand to display information about an existing Layer0 service.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 service get [environmentName:]serviceName\n\n  \n\n\n\n\nRequired parameters\n#\n\n\n\n  \n\n    \n[environmentName:]serviceName\n\n    \nThe name of an existing Layer0 service. You can optionally provide the Layer0 environment (\nenvironmentName\n) of the service.\n\n  \n\n\n\n\nservice list\n#\n\n\nUse the \nlist\n subcommand to list all of the existing services in your Layer0 instance.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 service list\n\n  \n\n\n\n\nservice logs\n#\n\n\nUse the \nlogs\n subcommand to display the logs from a Layer0 service that is currently running.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 service logs [--start MM/DD HH:MM | --end MM/DD HH:MM | --tail=N] serviceName\n\n  \n\n\n\n\nRequired parameters\n#\n\n\n\n  \n\n    \nserviceName\n\n    \nThe name of the Layer0 service for which you want to view logs.\n\n  \n\n\n\n\nOptional arguments\n#\n\n\n\n  \n\n    \n--start MM/DD HH:MM\n\n    \nThe start of the time range to fetch logs.\n\n  \n\n  \n\n    \n--end MM/DD HH:MM\n\n    \nThe end of the time range to fetch logs.\n\n  \n\n  \n\n    \n--tail=N\n\n    \nDisplay only the last \nN\n lines of the log.\n\n  \n\n\n\n\nservice scale\n#\n\n\nUse the \nscale\n subcommand to specify how many copies of an existing Layer0 service should run.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 service scale [environmentName:]serviceName copies\n\n  \n\n\n\n\nRequired parameters\n#\n\n\n\n  \n\n    \n[environmentName:]serviceName\n\n    \nThe name of the Layer0 service that you want to scale up. You can optionally provide the Layer0 environment (\nenvironmentName\n) of the service.\n\n  \n\n  \n\n    \ncopies\n\n    \nThe number of copies of the specified service that should be run.\n\n  \n\n\n\n\n\n\nTask\n#\n\n\nA Layer0 task is a component of an environment. A task executes the contents of a Docker image, as specified in a deploy. A task differs from a service in that a task does not restart after exiting. Additionally, ports are not exposed when using a task.\n\n\nThe \ntask\n command is used with the following subcommands: \ncreate\n, \ndelete\n, \nget\n, \nlist\n, and \nlogs\n.\n\n\ntask create\n#\n\n\nUse the \ncreate\n subcommand to create a Layer0 task.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 task create [--copies copies | --no-logs] environmentName taskName deployName\n\n  \n\n\n\n\nRequired parameters\n#\n\n\n\n  \n\n    \nenvironmentName\n\n    \nThe name of the existing Layer0 environment in which you want to create the task.\n\n  \n\n  \n\n    \ntaskName\n\n    \nA name for the task.\n\n  \n\n  \n\n    \ndeployName\n\n    \nThe name of an existing Layer0 deploy that the task should use.\n\n  \n\n\n\n\nOptional arguments\n#\n\n\n\n  \n\n    \n--copies copies\n\n    \nThe number of copies of the task to run (default: 1)\n\n  \n\n  \n\n    \n--no-logs\n\n    \nDisable cloudwatch logging for the service\n\n  \n\n\n\n\ntask delete\n#\n\n\nUse the \ndelete\n subcommand to delete an existing Layer0 task.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 task delete [environmentName:]taskName\n\n  \n\n\n\n\nRequired parameters\n#\n\n\n\n  \n\n    \n[environmentName:]taskName\n\n    \nThe name of the Layer0 task that you want to delete. You can optionally specify the name of the Layer0 environment that contains the task. This parameter is only required if mulitiple environments contain tasks with exactly the same name.\n\n  \n\n\n\n\nAdditional information\n#\n\n\n\n  \n\n    \nUntil the record has been purged, the API may indicate that the task is still running. Task records are typically purged within an hour.\n\n  \n\n\n\n\ntask get\n#\n\n\nUse the \nget\n subcommand to display information about an existing Layer0 task (\ntaskName\n).\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 task get [environmentName:]taskName\n\n  \n\n\n\n\nRequired parameters\n#\n\n\n\n  \n\n    \n[environmentName:]taskName\n\n    \nThe name of a Layer0 task for which you want to see information. You can optionally specify the name of the Layer0 Environment that contains the task.\n\n  \n\n\n\n\nAdditional information\n#\n\n\n\n  \n\n    \nThe value of \ntaskName\n does not need to exactly match the name of an existing task. If multiple results are found that match the pattern you specified in \ntaskName\n, then information about all matching tasks will be returned.\n\n  \n\n\n\n\ntask list\n#\n\n\nUse the \ntask\n subcommand to display a list of running tasks in your Layer0.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 task list\n\n  \n\n\n\n\ntask logs\n#\n\n\nUse the \nlogs\n subcommand to display logs for a running Layer0 task.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 task logs [--start MM/DD HH:MM | --end MM/DD HH:MM | --tail=N] taskName\n\n  \n\n\n\n\nRequired parameters\n#\n\n\n\n  \n\n    \ntaskName\n\n    \nThe name of an existing Layer0 task.\n\n  \n\n\n\n\nOptional arguments\n#\n\n\n\n  \n\n    \n--start MM/DD HH:MM\n\n    \nThe start of the time range to fetch logs.\n\n  \n\n  \n\n    \n--end MM/DD HH:MM\n\n    \nThe end of the time range to fetch logs.\n\n  \n\n  \n\n    \n--tail=N\n\n    \nDisplay only the last \nN\n lines of the log.\n\n  \n\n\n\n\nAdditional information\n#\n\n\n\n  \n\n    \nThe value of \ntaskName\n does not need to exactly match the name of an existing task. If multiple results are found that match the pattern you specified in \ntaskName\n, then information about all matching tasks will be returned.\n\n  \n\n\n\n\ntask list\n#\n\n\nUse the \nlist\n subcommand to display a list of running tasks in your Layer0.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 task list", 
            "title": "Layer0 CLI"
        }, 
        {
            "location": "/reference/cli/#layer0-cli-reference", 
            "text": "", 
            "title": "Layer0 CLI Reference"
        }, 
        {
            "location": "/reference/cli/#global-options", 
            "text": "The  l0  application is designed to be used with one of several commands:  admin ,  deploy ,  environment ,  job ,  loadbalancer ,  service , and  task . These commands are detailed in the sections below. There are, however, some global parameters that you may specify whenever using  l0 .", 
            "title": "Global options"
        }, 
        {
            "location": "/reference/cli/#usage", 
            "text": "l0 [global options] command subcommand [subcommand options] params", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#global-options_1", 
            "text": "-o [text|json], --output [text|json] \n     Specify the format of Layer0 outputs. By default, Layer0 outputs unformatted text; by issuing the  --output json  option, you can force  l0  to output JSON-formatted text. \n   \n   \n     -t value, --timeout value \n     Specify the timeout for running  l0  commands. Values can be in h, m, s, or ms. \n   \n   \n     -d, --debug \n     Print debug statements \n   \n   \n     --version \n     Display the version number of the  l0  application.", 
            "title": "Global options"
        }, 
        {
            "location": "/reference/cli/#admin", 
            "text": "The  admin  command is used to manage the Layer0 API server. This command is used with the following subcommands:  debug ,  sql , and  version .", 
            "title": "Admin"
        }, 
        {
            "location": "/reference/cli/#admin-debug", 
            "text": "Use the  debug  subcommand to view the running version of your Layer0 API server and CLI.", 
            "title": "admin debug"
        }, 
        {
            "location": "/reference/cli/#usage_1", 
            "text": "l0 admin debug", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#admin-sql", 
            "text": "Use the  sql  subcommand to initialize the Layer0 API database.", 
            "title": "admin sql"
        }, 
        {
            "location": "/reference/cli/#usage_2", 
            "text": "l0 admin sql", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#additional-information", 
            "text": "The  sql  subcommand is automatically executed during the Layer0 installation process; we recommend that you do not use this subcommand unless specifically directed to do so.", 
            "title": "Additional information"
        }, 
        {
            "location": "/reference/cli/#admin-version", 
            "text": "Use the  version  subcommand to display the current version of the Layer0 API.", 
            "title": "admin version"
        }, 
        {
            "location": "/reference/cli/#usage_3", 
            "text": "l0 admin version", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#deploy", 
            "text": "", 
            "title": "Deploy"
        }, 
        {
            "location": "/reference/cli/#deploy-create", 
            "text": "Use the  create  subcommand to upload a Docker task definition into Layer0. This command is used with the following subcommands:  create ,  delete ,  get  and  list .", 
            "title": "deploy create"
        }, 
        {
            "location": "/reference/cli/#usage_4", 
            "text": "l0 deploy create dockerPath deployName", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#required-parameters", 
            "text": "dockerPath \n     The path to the Docker task definition that you want to upload. \n   \n   \n     deployName \n     A name for the deploy.", 
            "title": "Required parameters"
        }, 
        {
            "location": "/reference/cli/#additional-information_1", 
            "text": "If  deployName  exactly matches the name of an existing Layer0 deploy, then the version number of that deploy will increase by 1, and the task definition you specified will replace the task definition specified in the previous version. \n     \n   \n     If you use Visual Studio to modify or create your Dockerrun file, you may see an \"Invalid Dockerrun.aws.json\" error. This error is caused by the default encoding used by Visual Studio. See the  \"Common issues\" page  for steps to resolve this issue. \n      \n   \n     \nDeploys created through Layer0 are rendered with a  logConfiguration  section for each container.\nIf a  logConfiguration  section already exists, no changes are made to the section.\nThe additional section enables logs from each container to be sent to the the Layer0 log group.\nThis is where logs are looked up during  l0  entity  logs  commands.\nThe added  logConfiguration  section uses the following template:  logConfiguration : {\n     logDriver :  awslogs ,\n         options : {\n             awslogs-group :  l0- prefix ,\n             awslogs-region :  region ,\n             awslogs-stream-prefix :  l0 \n        }\n    }\n}", 
            "title": "Additional information"
        }, 
        {
            "location": "/reference/cli/#deploy-delete", 
            "text": "Use the  delete  subcommand to delete a version of a Layer0 deploy.", 
            "title": "deploy delete"
        }, 
        {
            "location": "/reference/cli/#usage_5", 
            "text": "l0 deploy delete deployName", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#required-parameters_1", 
            "text": "deployName \n     The name of the Layer0 deploy you want to delete.", 
            "title": "Required parameters"
        }, 
        {
            "location": "/reference/cli/#deploy-get", 
            "text": "Use the  get  subcommand to view information about an existing Layer0 deploy.", 
            "title": "deploy get"
        }, 
        {
            "location": "/reference/cli/#usage_6", 
            "text": "l0 deploy get deployName", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#required-parameters_2", 
            "text": "deployName \n     The name of the Layer0 deploy for which you want to view additional information.", 
            "title": "Required parameters"
        }, 
        {
            "location": "/reference/cli/#additional-information_2", 
            "text": "The  get  subcommand supports wildcard matching:  l0 deploy get dep*  would return all deploys beginning with  dep .", 
            "title": "Additional information"
        }, 
        {
            "location": "/reference/cli/#deploy-list", 
            "text": "Use the  list  subcommand to view a list of deploys in your instance of Layer0.", 
            "title": "deploy list"
        }, 
        {
            "location": "/reference/cli/#usage_7", 
            "text": "l0 deploy list", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#environment", 
            "text": "Layer0 environments allow you to isolate services and load balancers for specific applications.\nThe  environment  command is used to manage Layer0 environments. This command is used with the following subcommands:  create ,  delete ,  get ,  list , and  setmincount .", 
            "title": "Environment"
        }, 
        {
            "location": "/reference/cli/#environment-create", 
            "text": "Use the  create  subcommand to create a new Layer0 environment.", 
            "title": "environment create"
        }, 
        {
            "location": "/reference/cli/#usage_8", 
            "text": "l0 environment create [--size size | --min-count mincount | --user-data path | --os os | --ami amiID] environmentName", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#required-parameters_3", 
            "text": "environmentName \n     A name for the environment.", 
            "title": "Required parameters"
        }, 
        {
            "location": "/reference/cli/#optional-arguments", 
            "text": "--size size \n     The instance size of the EC2 instances to create in your environment (default: m3.medium). \n   \n     \n     --min-count mincount \n     The minimum number of EC2 instances allowed in the environment's autoscaling group (default: 0). \n   \n   \n     --user-data path \n     The user data template file to use for the environment's autoscaling group. \n   \n   \n     --os os \n     The operating system used in the environment. Options are \"linux\" or \"windows\" (default: linux).\n        More information on windows environments is documented below \n   \n   \n     --ami amiID \n     A custom EC2 AMI ID to use in the environment. If not specified, Layer0 will use its default AMI ID for the specified operating system. \n     The user data template can be used to add custom configuration to your Layer0 environment. They are usually scripts that are executed at instance launch time to ensure an EC2 instance is in the correct state after the provisioning process finishes.\nLayer0 uses  Go Templates  to render user data.\nCurrently, two variables are passed into the template:  ECSEnvironmentID  and  S3Bucket .\nPlease review the  ECS Tutorial \nto better understand how to write a user data template, and use at your own risk!  Linux Environments : The default Layer0 user data template is:  #!/bin/bash  echo   ECS_CLUSTER ={{  .ECSEnvironmentID  }}    /etc/ecs/ecs.config echo   ECS_ENGINE_AUTH_TYPE = dockercfg   /etc/ecs/ecs.config\nyum install -y aws-cli awslogs jq\naws s3 cp s3:// {{  .S3Bucket  }} /bootstrap/dockercfg dockercfg cfg = $( cat dockercfg )  echo   ECS_ENGINE_AUTH_DATA = $cfg    /etc/ecs/ecs.config\ndocker pull amazon/amazon-ecs-agent:latest\nstart ecs  Windows Environments : The default Layer0 user data template is:  powershell  #   Set   agent   env   variables   for   the   Machine   context   ( durable )  $clusterName   =   {{ .ECSEnvironmentID }}  Write - Host   Cluster   name   set   as :   $clusterName   - foreground   green  [ Environment ] :: SetEnvironmentVariable ( ECS_CLUSTER ,   $clusterName ,   Machine )  [ Environment ] :: SetEnvironmentVariable ( ECS_ENABLE_TASK_IAM_ROLE ,   false ,   Machine )  $agentVersion   =   v1.14.0-1.windows.1  $agentZipUri   =   https://s3.amazonaws.com/amazon-ecs-agent/ecs-agent-windows-$agentVersion.zip  $agentZipMD5Uri   =   $agentZipUri.md5  #   Configure   docker   auth  Read - S3Object   - BucketName   {{   . S3Bucket   }}   - Key   bootstrap / dockercfg   - File   dockercfg . json  $dockercfgContent   =   [ IO . File ] :: ReadAllText ( dockercfg.json )  [ Environment ] :: SetEnvironmentVariable ( ECS_ENGINE_AUTH_DATA ,   $dockercfgContent ,   Machine )  [ Environment ] :: SetEnvironmentVariable ( ECS_ENGINE_AUTH_TYPE ,   dockercfg ,   Machine )  ###   ---   Nothing   user   configurable   after   this   point   ---  $ecsExeDir   =   $env:ProgramFiles\\Amazon\\ECS  $zipFile   =   $env:TEMP\\ecs-agent.zip  $md5File   =   $env:TEMP\\ecs-agent.zip.md5  ###   Get   the   files   from   S3  Invoke - RestMethod   - OutFile   $zipFile   - Uri   $agentZipUri  Invoke - RestMethod   - OutFile   $md5File   - Uri   $agentZipMD5Uri  ##   MD5   Checksum  $expectedMD5   =   ( Get - Content   $md5File )  $md5   =   New - Object   - TypeName   System . Security . Cryptography . MD5CryptoServiceProvider  $actualMD5   =   [ System . BitConverter ] :: ToString ( $md5 . ComputeHash ([ System . IO . File ] :: ReadAllBytes ( $zipFile ))). replace ( - ,   )  if ( $expectedMD5   - ne   $actualMD5 )   { \n     echo   Download doesn t match hash. \n     echo   Expected: $expectedMD5 - Got: $actualMD5 \n     exit   1  }  ##   Put   the   executables   in   the   executable   directory .  Expand - Archive   - Path   $zipFile   - DestinationPath   $ecsExeDir   - Force  ##   Start   the   agent   script   in   the   background .  $jobname   =   ECS-Agent-Init  $script   =    cd  $ecsExeDir ; .\\amazon-ecs-agent.ps1  $repeat   =   ( New - TimeSpan   - Minutes   1 )  $jobpath   =   $env : LOCALAPPDATA   +   \\Microsoft\\Windows\\PowerShell\\ScheduledJobs\\$jobname\\ScheduledJobDefinition.xml  if ( $ ( Test - Path   - Path   $jobpath ))   { \n   echo   Job definition already present \n   exit   0  }  $scriptblock   =   [ scriptblock ] :: Create ( $script )  $trigger   =   New - JobTrigger   - At   ( Get - Date ). Date   - RepeatIndefinitely   - RepetitionInterval   $repeat   - Once  $options   =   New - ScheduledJobOption   - RunElevated   - ContinueIfGoingOnBattery   - StartIfOnBattery  Register - ScheduledJob   - Name   $jobname   - ScriptBlock   $scriptblock   - Trigger   $trigger   - ScheduledJobOption   $options   - RunNow  Add - JobTrigger   - Name   $jobname   - Trigger   ( New - JobTrigger   - AtStartup   - RandomDelay   00 : 1 : 00 )  /powershell  persist true /persist    Windows Environments Windows containers are still in beta.     You can view the documented caveats with ECS  here .\nWhen creating Windows environments in Layer0, the root volume sizes for instances are 200GiB to accommodate the large size of the containers. \nIt can take as long as 45 minutes for a new windows container to come online.", 
            "title": "Optional arguments"
        }, 
        {
            "location": "/reference/cli/#environment-delete", 
            "text": "Use the  delete  subcommand to delete an existing Layer0 environment.", 
            "title": "environment delete"
        }, 
        {
            "location": "/reference/cli/#usage_9", 
            "text": "l0 environment delete [--wait] environmentName", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#required-parameters_4", 
            "text": "environmentName \n     The name of the Layer0 environment that you want to delete.", 
            "title": "Required parameters"
        }, 
        {
            "location": "/reference/cli/#optional-arguments_1", 
            "text": "--wait \n     Wait until the deletion is complete before exiting.", 
            "title": "Optional arguments"
        }, 
        {
            "location": "/reference/cli/#additional-information_3", 
            "text": "This operation performs several tasks asynchronously. When run without the  --wait  option, this operation will most likely exit before all of these tasks are complete; when run with the  --wait  option, this operation will only exit once these tasks have completed.", 
            "title": "Additional information"
        }, 
        {
            "location": "/reference/cli/#environment-get", 
            "text": "Use the  get  subcommand to display information about an existing Layer0 environment.", 
            "title": "environment get"
        }, 
        {
            "location": "/reference/cli/#usage_10", 
            "text": "l0 environment get environmentName", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#required-parameters_5", 
            "text": "environmentName \n     The name of the Layer0 environment for which you want to view additional information.", 
            "title": "Required parameters"
        }, 
        {
            "location": "/reference/cli/#additional-information_4", 
            "text": "The  get  subcommand supports wildcard matching:  l0 environment get test*  would return all environments beginning with  test .", 
            "title": "Additional information"
        }, 
        {
            "location": "/reference/cli/#environment-list", 
            "text": "Use the  list  subcommand to display a list of environments in your instance of Layer0.", 
            "title": "environment list"
        }, 
        {
            "location": "/reference/cli/#usage_11", 
            "text": "l0 environment list", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#environment-setmincount", 
            "text": "Use the  setmincount  subcommand to set the minimum number of EC2 instances allowed the environment's autoscaling group.", 
            "title": "environment setmincount"
        }, 
        {
            "location": "/reference/cli/#usage_12", 
            "text": "l0 enviroment setmincount environmentName count", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#required-parameters_6", 
            "text": "environmentName \n     The name of the Layer0 environment that you want to adjust. \n   \n   \n     count \n     The minimum number of instances allowed in the environment's autoscaling group.", 
            "title": "Required parameters"
        }, 
        {
            "location": "/reference/cli/#environment-link", 
            "text": "Use the  link  subcommand to link two environments together. \nWhen environments are linked, services inside the environments are allowed to communicate with each other as if they were in the same environment. \nThis link is bidirectional. \nThis command is idempotent; it will succeed even if the two specified environments are already linked.", 
            "title": "environment link"
        }, 
        {
            "location": "/reference/cli/#usage_13", 
            "text": "l0 environment link sourceEnvironmentName destEnvironmentName", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#required-parameters_7", 
            "text": "sourceEnvironmentName \n     The name of the first environment to link. \n   \n   \n     destEnvironmentName \n     The name of the second environment to link.", 
            "title": "Required parameters"
        }, 
        {
            "location": "/reference/cli/#environment-unlink", 
            "text": "Use the  unlink  subcommand to remove the link between two environments.\nThis command is idempotent; it will succeed even if the link does not exist.", 
            "title": "environment unlink"
        }, 
        {
            "location": "/reference/cli/#usage_14", 
            "text": "l0 environment unlink sourceEnvironmentName destEnvironmentName", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#required-parameters_8", 
            "text": "sourceEnvironmentName \n     The name of the first environment to unlink. \n   \n   \n     destEnvironmentName \n     The name of the second environment to unlink.", 
            "title": "Required parameters"
        }, 
        {
            "location": "/reference/cli/#job", 
            "text": "A Job is a long-running unit of work performed on behalf of the Layer0 API.\nJobs are executed as Layer0 tasks that run in the  api  environment.\nThe  job  command is used with the following subcommands:  logs ,  delete ,  get , and  list .", 
            "title": "Job"
        }, 
        {
            "location": "/reference/cli/#job-logs", 
            "text": "Use the  logs  subcommand to display the logs from a Layer0 job that is currently running.", 
            "title": "job logs"
        }, 
        {
            "location": "/reference/cli/#usage_15", 
            "text": "l0 job logs [--start MM/DD HH:MM | --end MM/DD HH:MM | --tail=N] jobName", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#required-parameters_9", 
            "text": "jobName \n     The name of the Layer0 job for which you want to view logs.", 
            "title": "Required parameters"
        }, 
        {
            "location": "/reference/cli/#optional-arguments_2", 
            "text": "--start MM/DD HH:MM \n     The start of the time range to fetch logs. \n   \n   \n     --end MM/DD HH:MM \n     The end of the time range to fetch logs. \n   \n   \n     --tail=N \n     Display only the last  N  lines of the log.", 
            "title": "Optional arguments"
        }, 
        {
            "location": "/reference/cli/#job-delete", 
            "text": "Use the  delete  subcommand to delete an existing job.", 
            "title": "job delete"
        }, 
        {
            "location": "/reference/cli/#usage_16", 
            "text": "l0 job delete jobName", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#required-parameters_10", 
            "text": "jobName \n     The name of the job that you want to delete.", 
            "title": "Required parameters"
        }, 
        {
            "location": "/reference/cli/#job-get", 
            "text": "Use the  get  subcommand to display information about an existing Layer0 job.", 
            "title": "job get"
        }, 
        {
            "location": "/reference/cli/#usage_17", 
            "text": "l0 job get jobName", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#required-parameters_11", 
            "text": "jobName \n     The name of an existing Layer0 job to display.", 
            "title": "Required parameters"
        }, 
        {
            "location": "/reference/cli/#additional-information_5", 
            "text": "The  get  subcommand supports wildcard matching:  l0 job get 2a55*  would return all jobs beginning with  2a55 .", 
            "title": "Additional information"
        }, 
        {
            "location": "/reference/cli/#job-list", 
            "text": "Use the  list  subcommand to display information about all of the existing jobs in an instance of Layer0.", 
            "title": "job list"
        }, 
        {
            "location": "/reference/cli/#usage_18", 
            "text": "l0 job list", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#loadbalancer", 
            "text": "A load balancer is a component of a Layer0 environment. Load balancers listen for traffic on certain ports, and then forward that traffic to Layer0  services . The  loadbalancer  command is used with the following subcommands:  create ,  delete ,  addport ,  dropport ,  get ,  list , and  healthcheck .", 
            "title": "Loadbalancer"
        }, 
        {
            "location": "/reference/cli/#loadbalancer-create", 
            "text": "Use the  create  subcommand to create a new load balancer.", 
            "title": "loadbalancer create"
        }, 
        {
            "location": "/reference/cli/#usage_19", 
            "text": "l0 loadbalancer create [--port hostPort:containerPort/protocol ... | --certificate certifiateName | --private | --healthcheck-target target | --healthcheck-interval interval | --healthcheck-timeout timeout | --healthcheck-healthy-threshold healthyThreshold | --healthcheck-unhealthy-threshold unhealthyThreshold] environmentName loadBalancerName", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#required-parameters_12", 
            "text": "environmentName \n     The name of the existing Layer0 environment in which you want to create the load balancer. \n   \n   \n     loadBalancerName \n     A name for the load balancer you are creating.", 
            "title": "Required parameters"
        }, 
        {
            "location": "/reference/cli/#optional-arguments_3", 
            "text": "--port hostPort:containerPort/protocol ... \n     \n     \n       The port configuration for the load balancer. Multiple ports can be specified using  --port xxx --port yyy .  hostPort  is the port on which the load balancer will listen for traffic;  containerPort  is the port that traffic will be forwarded to. If this option is not specified, Layer0 will use the following configuration: 80:80/tcp \n     \n   \n   \n     \n       --certificate certificateName \n     \n     \n       The name of an existing Layer0 certificate. You must include this option if you are using an HTTPS port configuration. \n     \n   \n   \n     \n       --private \n     \n     \n       When you use this option, the load balancer will only be accessible from within the Layer0 environment. \n     \n   \n   \n     \n       --healthcheck-target target \n     \n     \n       The target of the check. Valid pattern is  PROTOCOL:PORT/PATH   (default:  \"TCP:80\" ) \n       \n      If PROTOCOL is  HTTP  or  HTTPS , both PORT and PATH are required\n       \n      -  example:  HTTP:80/admin/healthcheck \n       \n      If PROTOCOL is  TCP  or  SSL , PORT is required and PATH is not supported\n       \n      -  example:  TCP:80 \n     \n   \n   \n     \n       --healthcheck-interval interval \n     \n     \n       The interval between checks  (default:  30 ) . \n     \n   \n   \n     \n       --healthcheck-timeout timeout \n     \n     \n       The length of time before the check times out  (default:  5 ) . \n     \n   \n   \n     \n       --healthcheck-healthy-threshold healthyThreshold \n     \n     \n       The number of checks before the instance is declared healthy  (default:  2 ) . \n     \n   \n   \n     \n       --healthcheck-unhealthy-threshold unhealthyThreshold \n     \n     \n       The number of checks before the instance is declared unhealthy  (default:  2 ) . \n     \n      Ports and Health Checks  When both the  --port  and the  --healthcheck-target  options are omitted, Layer0 configures the load balancer with some default values:  80:80/tcp  for ports and  tcp:80  for healthcheck target.\nThese default values together create a load balancer configured with a simple but functioning health check, opening up a set of ports that allows traffic to the target of the healthcheck.\n( --healthcheck-target tcp:80  tells the load balancer to ping its services at port 80 to determine their status, and  --port 80:80/tcp  configures a security group to allow traffic to pass between port 80 of the load balancer and port 80 of its services)  When creating a load balancer with non-default configurations for either  --port  or  --healthcheck-target , make sure that a valid  --port  and  --healthcheck-target  pairing is also created.", 
            "title": "Optional arguments"
        }, 
        {
            "location": "/reference/cli/#loadbalancer-delete", 
            "text": "Use the  delete  subcommand to delete an existing load balancer.", 
            "title": "loadbalancer delete"
        }, 
        {
            "location": "/reference/cli/#usage_20", 
            "text": "l0 loadbalancer delete [--wait] loadBalancerName", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#required-parameters_13", 
            "text": "loadBalancerName \n     The name of the load balancer that you want to delete.", 
            "title": "Required parameters"
        }, 
        {
            "location": "/reference/cli/#optional-arguments_4", 
            "text": "--wait \n     Wait until the deletion is complete before exiting.", 
            "title": "Optional arguments"
        }, 
        {
            "location": "/reference/cli/#additional-information_6", 
            "text": "In order to delete a load balancer that is already attached to a service, you must first delete the service that uses the load balancer. \n   \n   \n     This operation performs several tasks asynchronously. When run without the  --wait  option, this operation will most likely exit before all of these tasks are complete; when run with the  --wait  option, this operation will only exit once these tasks have completed.", 
            "title": "Additional information"
        }, 
        {
            "location": "/reference/cli/#loadbalancer-addport", 
            "text": "Use the  addport  subcommand to add a new port configuration to an existing Layer0 load balancer.", 
            "title": "loadbalancer addport"
        }, 
        {
            "location": "/reference/cli/#usage_21", 
            "text": "l0 loadbalancer addport [--certificate certificateName] loadBalancerName hostPort:containerPort/protocol", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#required-parameters_14", 
            "text": "loadBalancerName \n     The name of an existing Layer0 load balancer in which you want to add the port configuration. \n   \n   \n     hostPort \n     The port that the load balancer will listen on. \n   \n   \n     containerPort \n     The port that the load balancer will forward traffic to. \n   \n   \n     protocol \n     The protocol to use when forwarding traffic (acceptable values: tcp, ssl, http, and https).", 
            "title": "Required parameters"
        }, 
        {
            "location": "/reference/cli/#optional-arguments_5", 
            "text": "--certificate certificateName \n     The name of an existing Layer0 certificate. You must include this option if you are using an HTTPS port configuration.", 
            "title": "Optional arguments"
        }, 
        {
            "location": "/reference/cli/#additional-information_7", 
            "text": "The port configuration you specify must not already be in use by the load balancer you specify.", 
            "title": "Additional information"
        }, 
        {
            "location": "/reference/cli/#loadbalancer-dropport", 
            "text": "Use the  dropport  subcommand to remove a port configuration from an existing Layer0 load balancer.", 
            "title": "loadbalancer dropport"
        }, 
        {
            "location": "/reference/cli/#usage_22", 
            "text": "l0 loadbalancer dropport loadBalancerName hostPort", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#required-parameters_15", 
            "text": "loadBalancerName \n     The name of an existing Layer0 load balancer from which you want to remove the port configuration. \n   \n   \n     hostPort \n     The host port to remove from the load balancer.", 
            "title": "Required parameters"
        }, 
        {
            "location": "/reference/cli/#loadbalancer-get", 
            "text": "Use the  get  subcommand to display information about an existing Layer0 load balancer.", 
            "title": "loadbalancer get"
        }, 
        {
            "location": "/reference/cli/#usage_23", 
            "text": "l0 loadbalancer get [environmentName:]loadBalancerName", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#required-parameters_16", 
            "text": "[environmentName:]loadBalancerName \n     The name of an existing Layer0 load balancer. You can optionally provide the Layer0 environment ( environmentName ) associated with the Load Balancer", 
            "title": "Required parameters"
        }, 
        {
            "location": "/reference/cli/#additional-information_8", 
            "text": "The  get  subcommand supports wildcard matching:  l0 loadbalancer get entrypoint*  would return all jobs beginning with  entrypoint .", 
            "title": "Additional information"
        }, 
        {
            "location": "/reference/cli/#loadbalancer-list", 
            "text": "Use the  list  subcommand to display information about all of the existing load balancers in an instance of Layer0.", 
            "title": "loadbalancer list"
        }, 
        {
            "location": "/reference/cli/#usage_24", 
            "text": "l0 loadbalancer list", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#loadbalancer-healthcheck", 
            "text": "Use the  healthcheck  subcommand to display information about or update the configuration of a load balancer's health check.", 
            "title": "loadbalancer healthcheck"
        }, 
        {
            "location": "/reference/cli/#usage_25", 
            "text": "l0 loadbalancer healthcheck [--set-target target | --set-interval interval | --set-timeout timeout | --set-healthy-threshold healthyThreshold | --set-unhealthy-threshold unhealthyThreshold] loadbalancerName", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#optional-arguments_6", 
            "text": "--set-target target \n     \n     \n       The target of the check. Valid pattern is  PROTOCOL:PORT/PATH , where PROTOCOL values are:\n       \n       HTTP  or  HTTPS : both PORT and PATH are required\n       \n      -  example:  HTTP:80/admin/healthcheck \n       \n       TCP  or  SSL : PORT is required, PATH is not supported\n       \n      -  example:  TCP:80 \n     \n   \n   \n     \n       --set-interval interval \n     \n     \n       The interval between checks. \n     \n   \n   \n     \n       --set-timeout timeout \n     \n     \n       The length of time before the check times out. \n     \n   \n   \n     \n       --set-healthy-threshold healthyThreshold \n     \n     \n       The number of checks before the instance is declared healthy. \n     \n   \n   \n     \n       --set-unhealthy-threshold unhealthyThreshold \n     \n     \n       The number of checks before the instance is declared unhealthy.", 
            "title": "Optional arguments"
        }, 
        {
            "location": "/reference/cli/#additional-information_9", 
            "text": "Calling the subcommand without flags will display the current configuration of the load balancer's health check. Setting any of the flags will update the corresponding field in the health check, and all omitted flags will leave the corresponding fields unchanged.", 
            "title": "Additional information"
        }, 
        {
            "location": "/reference/cli/#service", 
            "text": "A service is a component of a Layer0 environment. The purpose of a service is to execute a Docker image specified in a  deploy . In order to create a service, you must first create an  environment  and a  deploy ; in most cases, you should also create a  load balancer  before creating the service.  The  service  command is used with the following subcommands:  create ,  delete ,  get ,  update ,  list ,  logs , and  scale .", 
            "title": "Service"
        }, 
        {
            "location": "/reference/cli/#service-create", 
            "text": "Use the  create  subcommand to create a Layer0 service.", 
            "title": "service create"
        }, 
        {
            "location": "/reference/cli/#usage_26", 
            "text": "l0 service create [--loadbalancer [environmentName:]loadBalancerName | --no-logs] environmentName serviceName deployName[:deployVersion]", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#required-parameters_17", 
            "text": "serviceName \n     A name for the service that you are creating. \n   \n   \n     environmentName \n     The name of an existing Layer0 environment. \n   \n   \n     deployName[:deployVersion] \n     The name of a Layer0 deploy that exists in the environment  environmentName . You can optionally specify the version number of the Layer0 deploy that you want to deploy. If you do not specify a version number, the latest version of the deploy will be used.", 
            "title": "Required parameters"
        }, 
        {
            "location": "/reference/cli/#optional-arguments_7", 
            "text": "--loadbalancer [environmentName:]loadBalancerName \n     Place the new service behind an existing load balancer named  loadBalancerName . You can optionally specify the Layer0 environment ( environmentName ) where the Load Balancer exists. \n   \n   \n     --no-logs \n     Disable cloudwatch logging for the service", 
            "title": "Optional arguments"
        }, 
        {
            "location": "/reference/cli/#service-update", 
            "text": "Use the  update  subcommand to apply an existing Layer0 Deploy to an existing Layer0 service.", 
            "title": "service update"
        }, 
        {
            "location": "/reference/cli/#usage_27", 
            "text": "l0 service update [--no-logs] [environmentName:]serviceName deployName[:deployVersion]", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#required-parameters_18", 
            "text": "[environmentName:]serviceName \n     The name of an existing Layer0 service into which you want to apply the deploy. You can optionally specify the Layer0 environment ( environmentName ) of the service. \n   \n   \n     deployName[:deployVersion] \n     The name of the Layer0 deploy that you want to apply to the service. You can optionally specify a specific version of the deploy ( deployVersion ). If you do not specify a version number, the latest version of the deploy will be applied. \n   \n   \n     --no-logs \n     Disable cloudwatch logging for the service", 
            "title": "Required parameters"
        }, 
        {
            "location": "/reference/cli/#additional-information_10", 
            "text": "If your service uses a load balancer, when you update the task definition for the service, the container name and container port that were specified when the service was created must remain the same in the task definition. In other words, if your service has a load balancer, you cannot apply any deploy you want to that service. If you are varying the container name or exposed ports, you must create a new service instead.", 
            "title": "Additional information"
        }, 
        {
            "location": "/reference/cli/#service-delete", 
            "text": "Use the  delete  subcommand to delete an existing Layer0 service.", 
            "title": "service delete"
        }, 
        {
            "location": "/reference/cli/#usage_28", 
            "text": "l0 service delete [--wait] [environmentName:]serviceName", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#required-parameters_19", 
            "text": "[environmentName:]serviceName \n     The name of the Layer0 service that you want to delete. You can optionally provide the Layer0 environment ( environmentName ) of the service.", 
            "title": "Required parameters"
        }, 
        {
            "location": "/reference/cli/#optional-arguments_8", 
            "text": "--wait \n     Wait until the deletion is complete before exiting.", 
            "title": "Optional arguments"
        }, 
        {
            "location": "/reference/cli/#additional-information_11", 
            "text": "This operation performs several tasks asynchronously. When run without the  --wait  option, this operation will most likely exit before all of these tasks are complete; when run with the  --wait  option, this operation will only exit once these tasks have completed.", 
            "title": "Additional information"
        }, 
        {
            "location": "/reference/cli/#service-get", 
            "text": "Use the  get  subcommand to display information about an existing Layer0 service.", 
            "title": "service get"
        }, 
        {
            "location": "/reference/cli/#usage_29", 
            "text": "l0 service get [environmentName:]serviceName", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#required-parameters_20", 
            "text": "[environmentName:]serviceName \n     The name of an existing Layer0 service. You can optionally provide the Layer0 environment ( environmentName ) of the service.", 
            "title": "Required parameters"
        }, 
        {
            "location": "/reference/cli/#service-list", 
            "text": "Use the  list  subcommand to list all of the existing services in your Layer0 instance.", 
            "title": "service list"
        }, 
        {
            "location": "/reference/cli/#usage_30", 
            "text": "l0 service list", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#service-logs", 
            "text": "Use the  logs  subcommand to display the logs from a Layer0 service that is currently running.", 
            "title": "service logs"
        }, 
        {
            "location": "/reference/cli/#usage_31", 
            "text": "l0 service logs [--start MM/DD HH:MM | --end MM/DD HH:MM | --tail=N] serviceName", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#required-parameters_21", 
            "text": "serviceName \n     The name of the Layer0 service for which you want to view logs.", 
            "title": "Required parameters"
        }, 
        {
            "location": "/reference/cli/#optional-arguments_9", 
            "text": "--start MM/DD HH:MM \n     The start of the time range to fetch logs. \n   \n   \n     --end MM/DD HH:MM \n     The end of the time range to fetch logs. \n   \n   \n     --tail=N \n     Display only the last  N  lines of the log.", 
            "title": "Optional arguments"
        }, 
        {
            "location": "/reference/cli/#service-scale", 
            "text": "Use the  scale  subcommand to specify how many copies of an existing Layer0 service should run.", 
            "title": "service scale"
        }, 
        {
            "location": "/reference/cli/#usage_32", 
            "text": "l0 service scale [environmentName:]serviceName copies", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#required-parameters_22", 
            "text": "[environmentName:]serviceName \n     The name of the Layer0 service that you want to scale up. You can optionally provide the Layer0 environment ( environmentName ) of the service. \n   \n   \n     copies \n     The number of copies of the specified service that should be run.", 
            "title": "Required parameters"
        }, 
        {
            "location": "/reference/cli/#task", 
            "text": "A Layer0 task is a component of an environment. A task executes the contents of a Docker image, as specified in a deploy. A task differs from a service in that a task does not restart after exiting. Additionally, ports are not exposed when using a task.  The  task  command is used with the following subcommands:  create ,  delete ,  get ,  list , and  logs .", 
            "title": "Task"
        }, 
        {
            "location": "/reference/cli/#task-create", 
            "text": "Use the  create  subcommand to create a Layer0 task.", 
            "title": "task create"
        }, 
        {
            "location": "/reference/cli/#usage_33", 
            "text": "l0 task create [--copies copies | --no-logs] environmentName taskName deployName", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#required-parameters_23", 
            "text": "environmentName \n     The name of the existing Layer0 environment in which you want to create the task. \n   \n   \n     taskName \n     A name for the task. \n   \n   \n     deployName \n     The name of an existing Layer0 deploy that the task should use.", 
            "title": "Required parameters"
        }, 
        {
            "location": "/reference/cli/#optional-arguments_10", 
            "text": "--copies copies \n     The number of copies of the task to run (default: 1) \n   \n   \n     --no-logs \n     Disable cloudwatch logging for the service", 
            "title": "Optional arguments"
        }, 
        {
            "location": "/reference/cli/#task-delete", 
            "text": "Use the  delete  subcommand to delete an existing Layer0 task.", 
            "title": "task delete"
        }, 
        {
            "location": "/reference/cli/#usage_34", 
            "text": "l0 task delete [environmentName:]taskName", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#required-parameters_24", 
            "text": "[environmentName:]taskName \n     The name of the Layer0 task that you want to delete. You can optionally specify the name of the Layer0 environment that contains the task. This parameter is only required if mulitiple environments contain tasks with exactly the same name.", 
            "title": "Required parameters"
        }, 
        {
            "location": "/reference/cli/#additional-information_12", 
            "text": "Until the record has been purged, the API may indicate that the task is still running. Task records are typically purged within an hour.", 
            "title": "Additional information"
        }, 
        {
            "location": "/reference/cli/#task-get", 
            "text": "Use the  get  subcommand to display information about an existing Layer0 task ( taskName ).", 
            "title": "task get"
        }, 
        {
            "location": "/reference/cli/#usage_35", 
            "text": "l0 task get [environmentName:]taskName", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#required-parameters_25", 
            "text": "[environmentName:]taskName \n     The name of a Layer0 task for which you want to see information. You can optionally specify the name of the Layer0 Environment that contains the task.", 
            "title": "Required parameters"
        }, 
        {
            "location": "/reference/cli/#additional-information_13", 
            "text": "The value of  taskName  does not need to exactly match the name of an existing task. If multiple results are found that match the pattern you specified in  taskName , then information about all matching tasks will be returned.", 
            "title": "Additional information"
        }, 
        {
            "location": "/reference/cli/#task-list", 
            "text": "Use the  task  subcommand to display a list of running tasks in your Layer0.", 
            "title": "task list"
        }, 
        {
            "location": "/reference/cli/#usage_36", 
            "text": "l0 task list", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#task-logs", 
            "text": "Use the  logs  subcommand to display logs for a running Layer0 task.", 
            "title": "task logs"
        }, 
        {
            "location": "/reference/cli/#usage_37", 
            "text": "l0 task logs [--start MM/DD HH:MM | --end MM/DD HH:MM | --tail=N] taskName", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#required-parameters_26", 
            "text": "taskName \n     The name of an existing Layer0 task.", 
            "title": "Required parameters"
        }, 
        {
            "location": "/reference/cli/#optional-arguments_11", 
            "text": "--start MM/DD HH:MM \n     The start of the time range to fetch logs. \n   \n   \n     --end MM/DD HH:MM \n     The end of the time range to fetch logs. \n   \n   \n     --tail=N \n     Display only the last  N  lines of the log.", 
            "title": "Optional arguments"
        }, 
        {
            "location": "/reference/cli/#additional-information_14", 
            "text": "The value of  taskName  does not need to exactly match the name of an existing task. If multiple results are found that match the pattern you specified in  taskName , then information about all matching tasks will be returned.", 
            "title": "Additional information"
        }, 
        {
            "location": "/reference/cli/#task-list_1", 
            "text": "Use the  list  subcommand to display a list of running tasks in your Layer0.", 
            "title": "task list"
        }, 
        {
            "location": "/reference/cli/#usage_38", 
            "text": "l0 task list", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/setup-cli/", 
            "text": "Layer0 Setup Reference\n#\n\n\nThe Layer0 Setup application (commonly called \nl0-setup\n), is used to provision, update, and destroy Layer0 instances.\n\n\n\n\nGeneral Usage\n#\n\n\nYou can use the \n-h, --help\n command to get generate information about the \nl0-setup\n tool:\n\n\n\n\nInit\n#\n\n\nThe \ninit\n command is used to initialize or reconfigure a Layer0 instance. \nThis command will prompt the user for inputs required to create/update a Layer0 instance. \nEach of the inputs can be specified through an optional flag.\n\n\nUsage\n#\n\n\n$ l0-setup init \n[\noptions\n]\n \ninstance_name\n \n\n\n\n\n\nOptions\n#\n\n\n\n\n--docker-path\n - Path to docker config.json file. \nThis is used to include private Docker Registry authentication for this Layer0 instance.\n\n\n--module-source\n - The source input variable is the path to the Terraform Layer0. \nBy default, this points to the Layer0 github repository. \nUsing values other than the default may result in undesired consequences.\n\n\n--version\n - The version input variable specifies the tag to use for the Layer0\nDocker images: \nquintilesims/l0-api\n and \nquintilesims/l0-runner\n.\n\n\n--aws-access-key\n - The access_key input variable is used to provision the AWS resources\nrequired for Layer0. \nThis corresponds to the Access Key ID portion of an AWS Access Key.\nIt is recommended this key has the \nAdministratorAccess\n policy. \n\n\n--aws-secret-key\n The secret_key input variable is used to provision the AWS resources\nrequired for Layer0. \nThis corresponds to the Secret Access Key portion of an AWS Access Key.\nIt is recommended this key has the \nAdministratorAccess\n policy.\n\n\n\n\n--aws-region\n - The region input variable specifies which region to provision the\nAWS resources required for Layer0. The following regions can be used:\n\n\n\n\nus-west-1\n\n\nus-west-2\n\n\nus-east-1\n\n\neu-west-1\n\n\n\n\n\n\n\n\n--aws-ssh-key-pair\n - The ssh_key_pair input variable specifies the name of the\nssh key pair to include in EC2 instances provisioned by Layer0. \nThis key pair must already exist in the AWS account. \nThe names of existing key pairs can be found in the EC2 dashboard.\n\n\n\n\n\n\n\n\nPlan\n#\n\n\nThe \nplan\n command is used to show the planned operation(s) to run during the next \napply\n on a Layer0 instance without actually executing any actions\n\n\nUsage\n#\n\n\n$ l0-setup plan \ninstance_name\n \n\n\n\n\n\nOptions\n#\n\n\nThere are no options for this command\n\n\n\n\nApply\n#\n\n\nThe \napply\n command is used to create and update Layer0 instances. Note that the default behavior of apply is to push the layer0 configuration to an S3 bucket unless the \n--push=false\n flag is set to false. Pushing the configuration to an S3 bucket requires aws credentials which if not set via the optional \n--aws-*\n flags, are read from the environment variables or a credentials file. \n\n\nUsage\n#\n\n\n$ l0-setup apply \n[\noptions\n]\n \ninstance_name\n \n\n\n\n\n\nOptions\n#\n\n\n\n\n--quick\n - Skips verification checks that normally run after \nterraform apply\n has completed\n\n\n--push\n - Skips uploading local Layer0 configuration files to an S3 bucket\n\n\n--aws-access-key\n - The Access Key ID portion of an AWS Access Key that has permissions to push to the Layer0 instances's S3 bucket. If not specified, the application will attempt to use any AWS credentials used by the AWS CLI. \n\n\n--aws-secret-key\n - The Secret Access Key portion of an AWS Access Key that has permissions to push to the Layer0 instances's S3 bucket. If not specified, the application will attempt to use any AWS credentials used by the AWS CLI. \n\n\n--aws-region\n - The region of the Layer0 instance. The default value is \nus-west-2\n. \n\n\n\n\n\n\nList\n#\n\n\nThe \nlist\n command is used to list local and remote Layer0 instances.\n\n\nUsage\n#\n\n\n$ l0-setup list \n[\noptions\n]\n\n\n\n\n\n\nOptions\n#\n\n\n\n\n-l, --local\n - Show local Layer0 instances. This value is true by default.\n\n\n-r, --remote\n - Show remote Layer0 instances. This value is true by default. \n\n\n--aws-access-key\n - The Access Key ID portion of an AWS Access Key that has permissions to list S3 buckets. \nIf not specified, the application will attempt to use any AWS credentials used by the AWS CLI. \n\n\n--aws-secret-key\n - The Secret Access Key portion of an AWS Access Key that has permissions to list S3 buckets. \nIf not specified, the application will attempt to use any AWS credentials used by the AWS CLI. \n\n\n--aws-region\n - The region to list S3 buckets. The default value is \nus-west-2\n. \n\n\n\n\n\n\nPush\n#\n\n\nThe \npush\n command is used to back up your Layer0 configuration files to an S3 bucket.\n\n\nUsage\n#\n\n\n$ l0-setup push \n[\noptions\n]\n \ninstance_name\n \n\n\n\n\n\nOptions\n#\n\n\n\n\n--aws-access-key\n - The Access Key ID portion of an AWS Access Key that has permissions to push to the Layer0 instances's S3 bucket. If not specified, the application will attempt to use any AWS credentials used by the AWS CLI. \n\n\n--aws-secret-key\n - The Secret Access Key portion of an AWS Access Key that has permissions to push to the Layer0 instances's S3 bucket. If not specified, the application will attempt to use any AWS credentials used by the AWS CLI. \n\n\n--aws-region\n - The region of the Layer0 instance. The default value is \nus-west-2\n. \n\n\n\n\n\n\nPull\n#\n\n\nThe \npull\n command is used copy Layer0 configuration files from an S3 bucket.\n\n\nUsage\n#\n\n\n$ l0-setup pull \n[\noptions\n]\n \ninstance_name\n \n\n\n\n\n\nOptions\n#\n\n\n\n\n--aws-access-key\n - The Access Key ID portion of an AWS Access Key that has permissions to pull to the Layer0 instances's S3 bucket. If not specified, the application will attempt to use any AWS credentials used by the AWS CLI. \n\n\n--aws-secret-key\n - The Secret Access Key portion of an AWS Access Key that has permissions to pull to the Layer0 instances's S3 bucket. If not specified, the application will attempt to use any AWS credentials used by the AWS CLI. \n\n\n--aws-region\n - The region of the Layer0 instance. The default value is \nus-west-2\n. \n\n\n\n\n\n\nEndpoint\n#\n\n\nThe \nendpoint\n command is used to show environment variables used to connect to a Layer0 instance\n\n\nUsage\n#\n\n\n$ l0-setup endpoint \n[\noptions\n]\n \ninstance_name\n \n\n\n\n\n\nOptions\n#\n\n\n\n\n-i, --insecure\n - Show environment variables that allow for insecure settings\n\n\n-d, --dev\n - Show environment variables that are required for local development\n\n\n-s --syntax\n - Choose the syntax to display environment variables \n(choices: \nbash\n, \ncmd\n, \npowershell\n) (default: \nbash\n)\n\n\n\n\n\n\nDestroy\n#\n\n\nThe \ndestroy\n command is used to destroy all resources associated with a Layer0 instance.\n\n\n\n\nCaution\n\n\nDestroying a Layer0 instance cannot be undone; if you created backups of your Layer0 configuration using the \npush\n command, those backups will also be deleted when you run the \ndestroy\n command.\n\n\n\n\nUsage\n#\n\n\n$ l0-setup destroy \n[\noptions\n]\n \ninstance_name\n \n\n\n\n\n\nOptions\n#\n\n\n\n\n--force\n - Skips confirmation prompt\n\n\n\n\n\n\nUpgrade\n#\n\n\nThe \nupgrade\n command is used to upgrade a Layer0 instance to a new version.\nYou will need to run an \napply\n after this command has completed. \n\n\nUsage\n#\n\n\n$ l0-setup upgrade \n[\noptions\n]\n \ninstance_name\n \nversion\n\n\n\n\n\n\nOptions\n#\n\n\n\n\n--force\n - Skips confirmation prompt\n\n\n\n\n\n\nSet\n#\n\n\nThe \nset\n command is used set input variable(s) for a Layer0 instance's Terraform module.\nThis command can be used to shorthand the \ninit\n and \nupgrade\n commands, \nand can also be used with custom Layer0 modules. \nYou will need to run an \napply\n after this command has completed. \n\n\nUsage\n#\n\n\n$ l0-setup \nset\n \n[\noptions\n]\n \ninstance_name\n\n\n\n\n\n\nExample Usage\n\n\n$ l0-setup \nset\n --input \nusername\n=\nadmin --input \npassword\n=\npass123 mylayer0\n\n\n\n\n\nOptions\n#\n\n\n\n\n--input\n - Specify an input using \nkey=val\n format", 
            "title": "Layer0 Setup CLI"
        }, 
        {
            "location": "/reference/setup-cli/#layer0-setup-reference", 
            "text": "The Layer0 Setup application (commonly called  l0-setup ), is used to provision, update, and destroy Layer0 instances.", 
            "title": "Layer0 Setup Reference"
        }, 
        {
            "location": "/reference/setup-cli/#general-usage", 
            "text": "You can use the  -h, --help  command to get generate information about the  l0-setup  tool:", 
            "title": "General Usage"
        }, 
        {
            "location": "/reference/setup-cli/#init", 
            "text": "The  init  command is used to initialize or reconfigure a Layer0 instance. \nThis command will prompt the user for inputs required to create/update a Layer0 instance. \nEach of the inputs can be specified through an optional flag.", 
            "title": "Init"
        }, 
        {
            "location": "/reference/setup-cli/#usage", 
            "text": "$ l0-setup init  [ options ]   instance_name", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/setup-cli/#options", 
            "text": "--docker-path  - Path to docker config.json file. \nThis is used to include private Docker Registry authentication for this Layer0 instance.  --module-source  - The source input variable is the path to the Terraform Layer0. \nBy default, this points to the Layer0 github repository. \nUsing values other than the default may result in undesired consequences.  --version  - The version input variable specifies the tag to use for the Layer0\nDocker images:  quintilesims/l0-api  and  quintilesims/l0-runner .  --aws-access-key  - The access_key input variable is used to provision the AWS resources\nrequired for Layer0. \nThis corresponds to the Access Key ID portion of an AWS Access Key.\nIt is recommended this key has the  AdministratorAccess  policy.   --aws-secret-key  The secret_key input variable is used to provision the AWS resources\nrequired for Layer0. \nThis corresponds to the Secret Access Key portion of an AWS Access Key.\nIt is recommended this key has the  AdministratorAccess  policy.   --aws-region  - The region input variable specifies which region to provision the\nAWS resources required for Layer0. The following regions can be used:   us-west-1  us-west-2  us-east-1  eu-west-1     --aws-ssh-key-pair  - The ssh_key_pair input variable specifies the name of the\nssh key pair to include in EC2 instances provisioned by Layer0. \nThis key pair must already exist in the AWS account. \nThe names of existing key pairs can be found in the EC2 dashboard.", 
            "title": "Options"
        }, 
        {
            "location": "/reference/setup-cli/#plan", 
            "text": "The  plan  command is used to show the planned operation(s) to run during the next  apply  on a Layer0 instance without actually executing any actions", 
            "title": "Plan"
        }, 
        {
            "location": "/reference/setup-cli/#usage_1", 
            "text": "$ l0-setup plan  instance_name", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/setup-cli/#options_1", 
            "text": "There are no options for this command", 
            "title": "Options"
        }, 
        {
            "location": "/reference/setup-cli/#apply", 
            "text": "The  apply  command is used to create and update Layer0 instances. Note that the default behavior of apply is to push the layer0 configuration to an S3 bucket unless the  --push=false  flag is set to false. Pushing the configuration to an S3 bucket requires aws credentials which if not set via the optional  --aws-*  flags, are read from the environment variables or a credentials file.", 
            "title": "Apply"
        }, 
        {
            "location": "/reference/setup-cli/#usage_2", 
            "text": "$ l0-setup apply  [ options ]   instance_name", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/setup-cli/#options_2", 
            "text": "--quick  - Skips verification checks that normally run after  terraform apply  has completed  --push  - Skips uploading local Layer0 configuration files to an S3 bucket  --aws-access-key  - The Access Key ID portion of an AWS Access Key that has permissions to push to the Layer0 instances's S3 bucket. If not specified, the application will attempt to use any AWS credentials used by the AWS CLI.   --aws-secret-key  - The Secret Access Key portion of an AWS Access Key that has permissions to push to the Layer0 instances's S3 bucket. If not specified, the application will attempt to use any AWS credentials used by the AWS CLI.   --aws-region  - The region of the Layer0 instance. The default value is  us-west-2 .", 
            "title": "Options"
        }, 
        {
            "location": "/reference/setup-cli/#list", 
            "text": "The  list  command is used to list local and remote Layer0 instances.", 
            "title": "List"
        }, 
        {
            "location": "/reference/setup-cli/#usage_3", 
            "text": "$ l0-setup list  [ options ]", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/setup-cli/#options_3", 
            "text": "-l, --local  - Show local Layer0 instances. This value is true by default.  -r, --remote  - Show remote Layer0 instances. This value is true by default.   --aws-access-key  - The Access Key ID portion of an AWS Access Key that has permissions to list S3 buckets. \nIf not specified, the application will attempt to use any AWS credentials used by the AWS CLI.   --aws-secret-key  - The Secret Access Key portion of an AWS Access Key that has permissions to list S3 buckets. \nIf not specified, the application will attempt to use any AWS credentials used by the AWS CLI.   --aws-region  - The region to list S3 buckets. The default value is  us-west-2 .", 
            "title": "Options"
        }, 
        {
            "location": "/reference/setup-cli/#push", 
            "text": "The  push  command is used to back up your Layer0 configuration files to an S3 bucket.", 
            "title": "Push"
        }, 
        {
            "location": "/reference/setup-cli/#usage_4", 
            "text": "$ l0-setup push  [ options ]   instance_name", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/setup-cli/#options_4", 
            "text": "--aws-access-key  - The Access Key ID portion of an AWS Access Key that has permissions to push to the Layer0 instances's S3 bucket. If not specified, the application will attempt to use any AWS credentials used by the AWS CLI.   --aws-secret-key  - The Secret Access Key portion of an AWS Access Key that has permissions to push to the Layer0 instances's S3 bucket. If not specified, the application will attempt to use any AWS credentials used by the AWS CLI.   --aws-region  - The region of the Layer0 instance. The default value is  us-west-2 .", 
            "title": "Options"
        }, 
        {
            "location": "/reference/setup-cli/#pull", 
            "text": "The  pull  command is used copy Layer0 configuration files from an S3 bucket.", 
            "title": "Pull"
        }, 
        {
            "location": "/reference/setup-cli/#usage_5", 
            "text": "$ l0-setup pull  [ options ]   instance_name", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/setup-cli/#options_5", 
            "text": "--aws-access-key  - The Access Key ID portion of an AWS Access Key that has permissions to pull to the Layer0 instances's S3 bucket. If not specified, the application will attempt to use any AWS credentials used by the AWS CLI.   --aws-secret-key  - The Secret Access Key portion of an AWS Access Key that has permissions to pull to the Layer0 instances's S3 bucket. If not specified, the application will attempt to use any AWS credentials used by the AWS CLI.   --aws-region  - The region of the Layer0 instance. The default value is  us-west-2 .", 
            "title": "Options"
        }, 
        {
            "location": "/reference/setup-cli/#endpoint", 
            "text": "The  endpoint  command is used to show environment variables used to connect to a Layer0 instance", 
            "title": "Endpoint"
        }, 
        {
            "location": "/reference/setup-cli/#usage_6", 
            "text": "$ l0-setup endpoint  [ options ]   instance_name", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/setup-cli/#options_6", 
            "text": "-i, --insecure  - Show environment variables that allow for insecure settings  -d, --dev  - Show environment variables that are required for local development  -s --syntax  - Choose the syntax to display environment variables \n(choices:  bash ,  cmd ,  powershell ) (default:  bash )", 
            "title": "Options"
        }, 
        {
            "location": "/reference/setup-cli/#destroy", 
            "text": "The  destroy  command is used to destroy all resources associated with a Layer0 instance.   Caution  Destroying a Layer0 instance cannot be undone; if you created backups of your Layer0 configuration using the  push  command, those backups will also be deleted when you run the  destroy  command.", 
            "title": "Destroy"
        }, 
        {
            "location": "/reference/setup-cli/#usage_7", 
            "text": "$ l0-setup destroy  [ options ]   instance_name", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/setup-cli/#options_7", 
            "text": "--force  - Skips confirmation prompt", 
            "title": "Options"
        }, 
        {
            "location": "/reference/setup-cli/#upgrade", 
            "text": "The  upgrade  command is used to upgrade a Layer0 instance to a new version.\nYou will need to run an  apply  after this command has completed.", 
            "title": "Upgrade"
        }, 
        {
            "location": "/reference/setup-cli/#usage_8", 
            "text": "$ l0-setup upgrade  [ options ]   instance_name   version", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/setup-cli/#options_8", 
            "text": "--force  - Skips confirmation prompt", 
            "title": "Options"
        }, 
        {
            "location": "/reference/setup-cli/#set", 
            "text": "The  set  command is used set input variable(s) for a Layer0 instance's Terraform module.\nThis command can be used to shorthand the  init  and  upgrade  commands, \nand can also be used with custom Layer0 modules. \nYou will need to run an  apply  after this command has completed.", 
            "title": "Set"
        }, 
        {
            "location": "/reference/setup-cli/#usage_9", 
            "text": "$ l0-setup  set   [ options ]   instance_name   Example Usage  $ l0-setup  set  --input  username = admin --input  password = pass123 mylayer0", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/setup-cli/#options_9", 
            "text": "--input  - Specify an input using  key=val  format", 
            "title": "Options"
        }, 
        {
            "location": "/reference/terraform_introduction/", 
            "text": "Introduction to Terraform\n#\n\n\nWhat does Terraform do?\n#\n\n\nTerraform is a powerful orchestration tool for creating, updating, deleting, and otherwise managing infrastructure in an easy-to-understand, declarative manner.\nTerraform's \ndocumentation\n is very good, but at a glance:\n\n\nBe Declarative -\n\nSpecify desired infrastructure results in Terraform (\n*.tf\n) files, and let Terraform do the heavy work of figuring out how to make that specification a reality.\n\n\nScry the Future -\n\nUse \nterraform plan\n to see a list of everything that Terraform \nwould\n do without actually making those changes.\n\n\nVersion Infrastructure -\n\nCheck Terraform files into a VCS to track changes to and manage versions of your infrastructure.\n\n\nWhy Terraform?\n#\n\n\nWhy did we latch onto Terraform instead of something like CloudFormation?\n\n\nCloud-Agnostic -\n\nUnlike CloudFormation, Terraform is able to incorporate different \nresource providers\n to manage infrastructure across multiple cloud services (not just AWS).\n\n\nCustom Providers -\n\nTerraform can be extended to manage tools that don't come natively through use of custom providers.\nWe wrote a \nLayer0 provider\n so that Terraform can manage Layer0 resources in addition to tools and resources and infrastructure beyond Layer0's scope.\n\n\nTerraform has some \nthings to say\n on the matter as well.\n\n\nAdvantages Versus Layer0 CLI?\n#\n\n\nWhy should you move from using (or scripting) the Layer0 CLI directly?\n\n\nReduce Fat-Fingering Mistakes -\n\nCreating Terraform files (and using \nterraform plan\n) allows you to review your deployment and catch errors.\nExecuting Layer0 CLI commands one-by-one is tiresome, non-transportable, and a process ripe for typos.\n\n\nGo Beyond Layer0 -\n\nRetain the benefits of leveraging Layer0's concepts and resources using our \nprovider\n, but also gain the ability to orchestrate resources and tools beyond the CLI's scope.\n\n\nHow do I get Terraform?\n#\n\n\nCheck out Terraform's \ndocumentation\n on the subject.", 
            "title": "Terraform"
        }, 
        {
            "location": "/reference/terraform_introduction/#introduction-to-terraform", 
            "text": "", 
            "title": "Introduction to Terraform"
        }, 
        {
            "location": "/reference/terraform_introduction/#what-does-terraform-do", 
            "text": "Terraform is a powerful orchestration tool for creating, updating, deleting, and otherwise managing infrastructure in an easy-to-understand, declarative manner.\nTerraform's  documentation  is very good, but at a glance:  Be Declarative - \nSpecify desired infrastructure results in Terraform ( *.tf ) files, and let Terraform do the heavy work of figuring out how to make that specification a reality.  Scry the Future - \nUse  terraform plan  to see a list of everything that Terraform  would  do without actually making those changes.  Version Infrastructure - \nCheck Terraform files into a VCS to track changes to and manage versions of your infrastructure.", 
            "title": "What does Terraform do?"
        }, 
        {
            "location": "/reference/terraform_introduction/#why-terraform", 
            "text": "Why did we latch onto Terraform instead of something like CloudFormation?  Cloud-Agnostic - \nUnlike CloudFormation, Terraform is able to incorporate different  resource providers  to manage infrastructure across multiple cloud services (not just AWS).  Custom Providers - \nTerraform can be extended to manage tools that don't come natively through use of custom providers.\nWe wrote a  Layer0 provider  so that Terraform can manage Layer0 resources in addition to tools and resources and infrastructure beyond Layer0's scope.  Terraform has some  things to say  on the matter as well.", 
            "title": "Why Terraform?"
        }, 
        {
            "location": "/reference/terraform_introduction/#advantages-versus-layer0-cli", 
            "text": "Why should you move from using (or scripting) the Layer0 CLI directly?  Reduce Fat-Fingering Mistakes - \nCreating Terraform files (and using  terraform plan ) allows you to review your deployment and catch errors.\nExecuting Layer0 CLI commands one-by-one is tiresome, non-transportable, and a process ripe for typos.  Go Beyond Layer0 - \nRetain the benefits of leveraging Layer0's concepts and resources using our  provider , but also gain the ability to orchestrate resources and tools beyond the CLI's scope.", 
            "title": "Advantages Versus Layer0 CLI?"
        }, 
        {
            "location": "/reference/terraform_introduction/#how-do-i-get-terraform", 
            "text": "Check out Terraform's  documentation  on the subject.", 
            "title": "How do I get Terraform?"
        }, 
        {
            "location": "/reference/terraform-plugin/", 
            "text": "Layer0 Terraform Provider Reference\n#\n\n\nTerraform is an open-source tool for provisioning and managing infrastructure.\nIf you are new to Terraform, we recommend checking out their \ndocumentation\n.\n\n\nLayer0 has built a custom \nprovider\n for Layer0.\nThis provider allows users to create, manage, and update Layer0 entities using Terraform.\n\n\nPrerequisites\n#\n\n\n\n\nTerraform v0.9.4+\n (\ndownload\n), accessible in your system path.\n\n\n\n\nInstall\n#\n\n\nDownload a Layer0 v0.8.4+ \nrelease\n.\nThe Terraform plugin binary is located in the release zip file as \nterraform-provider-layer0\n.\nCopy this \nterraform-provider-layer0\n binary into the same directory as your Terraform binary - and you're done!\n\n\nFor further information, see Terraform's documentation on installing a Terraform plugin \nhere\n.\n\n\nGetting Started\n#\n\n\n\n\nCheckout the \nTerraform\n section of the Guestbook walkthrough \nhere\n.\n\n\nWe've added some tips and links to helpful resources in the \nBest Practices\n section below.\n\n\n\n\n\n\nProvider\n#\n\n\nThe Layer0 provider is used to interact with a Layer0 API.\nThe provider needs to be configured with the proper credentials before it can be used.\n\n\nExample Usage\n#\n\n\n# Add \nendpoint\n and \ntoken\n variables\nvariable \nendpoint\n {}\n\nvariable \ntoken\n {}\n\n# Configure the layer0 provider\nprovider \nlayer0\n {\n  endpoint        = \n${\nvar\n.\nendpoint\n}\n\n  token           = \n${\nvar\n.\ntoken\n}\n\n  skip_ssl_verify = true\n}\n\n\n\n\n\nArgument Reference\n#\n\n\nThe following arguments are supported:\n\n\n\n\nConfiguration\n\n\nThe \nendpoint\n and \ntoken\n variables for your layer0 api can be found using the \nl0-setup endpoint\n command\n\n\n\n\n\n\nendpoint\n - (Required) The endpoint of the layer0 api\n\n\ntoken\n - (Required) The authentication token for the layer0 api\n\n\nskip_ssl_verify\n - (Optional) If true, ssl certificate mismatch warnings will be ignored\n\n\n\n\n\n\nAPI Data Source\n#\n\n\nThe API data source is used to extract useful read-only variables from the Layer0 API.\n\n\nExample Usage\n#\n\n\n# Configure the api data source\ndata \nlayer0_api\n \nconfig\n {}\n\n# Output the layer0 vpc id\noutput \nvpc id\n {\n  val = \n${\ndata\n.\nlayer0_api\n.\nconfig\n.\nvpc_id\n}\n\n}\n\n\n\n\n\nAttribute Reference\n#\n\n\nThe following attributes are exported:\n\n\n\n\nprefix\n - The prefix of the layer0 instance\n\n\nvpc_id\n - The vpc id of the layer0 instance\n\n\npublic_subnets\n - A list containing the 2 public subnet ids in the layer0 vpc\n\n\nprivate_subnets\n - A list containing the 2 private subnet ids in the layer0 vpc\n\n\n\n\n\n\nDeploy Data Source\n#\n\n\nThe Deploy data source is used to extract Layer0 Deploy attributes.\n\n\nExample Usage\n#\n\n\n# Configure the deploy data source\ndata \nlayer0_deploy\n \ndpl\n {\n  name    = \nmy-deploy\n\n  version = \n1\n\n}\n\n# Output the layer0 deploy id\noutput \ndeploy_id\n {\n  val = \n${\ndata\n.\nlayer0_deploy\n.\ndpl\n.\nid\n}\n\n}\n\n\n\n\n\nArgument Reference\n#\n\n\nThe following arguments are supported:\n\n\n\n\nname\n - (Required) The name of the deploy\n\n\nversion\n - (Required) The version of the deploy\n\n\n\n\nAttribute Reference\n#\n\n\nThe following attributes are exported:\n\n\n\n\nname\n - The name of the deploy\n\n\nversion\n - The version of the deploy\n\n\nid\n - The id of the deploy\n\n\n\n\n\n\nEnvironment Data Source\n#\n\n\nThe Environment data source is used to extract Layer0 Environment attributes.\n\n\nExample Usage\n#\n\n\n# Configure the environment data source\ndata \nlayer0_environment\n \nenv\n {\n  name = \nmy-environment\n\n}\n\n# Output the layer0 environment id\noutput \nenvironment_id\n {\n  val = \n${\ndata\n.\nlayer0_environment\n.\nenv\n.\nid\n}\n\n}\n\n\n\n\n\nArgument Reference\n#\n\n\nThe following arguments are supported:\n\n\n\n\nname\n - (Required) The name of the environment\n\n\n\n\nAttribute Reference\n#\n\n\nThe following attributes are exported:\n\n\n\n\nid\n - The id of the environment\n\n\nname\n - The name of the environment\n\n\nsize\n - The size of the instances in the environment\n\n\nmin_count\n - The current number instances in the environment\n\n\nos\n - The operating system used for the environment\n\n\nami\n - The AMI ID used for the environment\n\n\n\n\n\n\nLoad Balancer Data Source\n#\n\n\nThe Load Balancer data source is used to extract Layer0 Load Balancer attributes.\n\n\nExample Usage\n#\n\n\n# Configure the load balancer source\ndata \nlayer0_load_balancer\n \nlb\n {\n  name           = \nmy-loadbalancer\n\n  environment_id = \n${\ndata\n.\nlayer0_environment\n.\nenv\n.\nenvironment_id\n}\n\n}\n\n# Output the layer0 load balancer id\noutput \nload_balancer_id\n {\n  val = \n${\ndata\n.\nlayer0_load_balancer\n.\nlb\n.\nid\n}\n\n}\n\n\n\n\n\nArgument Reference\n#\n\n\nThe following arguments are supported:\n\n\n\n\nname\n - (required) The name of the load balancer\n\n\nenvironment_id\n - (required) The id of the environment the load balancer exists in\n\n\n\n\nAttribute Reference\n#\n\n\nThe following attributes are exported:\n\n\n\n\nid\n - The id of the load balancer\n\n\nname\n - The name of the load balancer\n\n\nenvironment_id\n - The id of the environment the load balancer exists in\n\n\nenvironment_name\n - The name of the environment the load balancer exists in\n\n\nprivate\n - Whether or not the load balancer is private\n\n\nurl\n - The URL of the load balancer\n\n\n\n\n\n\nService Data Source\n#\n\n\nThe Service data source is used to extract Layer0 Service attributes.\n\n\nExample Usage\n#\n\n\n# Configure the service data source\ndata \nlayer0_service\n \nsvc\n {\n  name           = \nmy-service\n\n  environment_id = \n${\ndata\n.\nlayer0_environment\n.\nenv\n.\nenvironment_id\n}\n\n}\n\n# Output the layer0 service id\noutput \nservice_id\n {\n  val = \n${\ndata\n.\nlayer0_service\n.\nsvc\n.\nid\n}\n\n}\n\n\n\n\n\nArgument Reference\n#\n\n\nThe following arguments are supported:\n\n\n\n\nname\n - (required) The name of the service\n\n\nenvironment_id\n - (required) The id of the environment the service exists in\n\n\n\n\nAttribute Reference\n#\n\n\nThe following attributes are exported:\n\n\n\n\nid\n - The id of the service\n\n\nname\n - The name of the service\n\n\nenvironment_id\n - The id of the environment the service exists in\n\n\nenvironment_name\n - The name of the environment the service exists in\n\n\nscale\n - The current desired scale of the service\n\n\n\n\n\n\nDeploy Resource\n#\n\n\nProvides a Layer0 Deploy.\n\n\nPerforming variable substitution inside of your deploy's json file (typically named \nDockerrun.aws.json\n) can be done through Terraform's \ntemplate_file\n.\nFor a working example, please see the sample \nGuestbook\n application\n\n\nExample Usage\n#\n\n\n# Configure the deploy template\ndata \ntemplate_file\n \nguestbook\n {\n  template = \n${\nfile\n(\nDockerrun.aws.json\n)\n}\n\n  vars {\n    docker_image_tag = \nlatest\n\n  }\n}\n\n# Create a deploy using the rendered template\nresource \nlayer0_deploy\n \nguestbook\n {\n  name    = \nguestbook\n\n  content = \n${\ndata\n.\ntemplate_file\n.\nguestbook\n.\nrendered\n}\n\n}\n\n\n\n\n\nArgument Reference\n#\n\n\nThe following arguments are supported:\n\n\n\n\nname\n - (Required) The name of the deploy\n\n\ncontent\n - (Required) The content of the deploy\n\n\n\n\nAttribute Reference\n#\n\n\nThe following attributes are exported:\n\n\n\n\nid\n - The id of the deploy\n\n\nname\n - The name of the deploy\n\n\nversion\n - The version number of the deploy\n\n\n\n\n\n\nEnvironment Resource\n#\n\n\nProvides a Layer0 Environment\n\n\nExample Usage\n#\n\n\n# Create a new environment\nresource \nlayer0_environment\n \ndemo\n {\n  name      = \ndemo\n\n  size      = \nm3.medium\n\n  min_count = 0\n  user_data = \necho hello, world\n\n  os        = \nlinux\n\n  ami       = \nami123\n\n}\n\n\n\n\n\nArgument Reference\n#\n\n\nThe following arguments are supported:\n\n\n\n\nname\n - (Required) The name of the environment\n\n\nsize\n - (Optional, Default: \"m3.medium\") The size of the instances in the environment.\nAvailable instance sizes can be found \nhere\n\n\nmin_count\n - (Optional, Default: 0) The minimum number of instances allowed in the environment\n\n\nuser-data\n - (Optional) The user data template to use for the environment's autoscaling group.\nSee the \ncli reference\n for the default template.\n\n\nos\n - (Optional, Default: \"linux\") Specifies the type of operating system used in the environment.\nOptions are \"linux\" or \"windows\".\n\n\nami\n - (Optional) A custom AMI ID to use in the environment. \nIf not specified, Layer0 will use its default AMI ID for the specified operating system.\n\n\n\n\nAttribute Reference\n#\n\n\nThe following attributes are exported:\n\n\n\n\nid\n - The id of the environment\n\n\nname\n - The name of the environment\n\n\nsize\n - The size of the instances in the environment\n\n\ncluster_count\n - The current number instances in the environment\n\n\nsecurity_group_id\n - The ID of the environment's security group\n\n\nos\n - The operating system used for the environment\n\n\nami\n - The AMI ID used for the environment\n\n\n\n\n\n\nLoad Balancer Resource\n#\n\n\nProvides a Layer0 Load Balancer\n\n\nExample Usage\n#\n\n\n# Create a new load balancer\nresource \nlayer0_load_balancer\n \nguestbook\n {\n  name        = \nguestbook\n\n  environment = \ndemo123\n\n  private     = false\n\n  port {\n    host_port      = 80\n    container_port = 80\n    protocol       = \nhttp\n\n  }\n\n  port {\n    host_port      = 443\n    container_port = 443\n    protocol       = \nhttps\n\n    certificate    = \ncert\n\n  }\n\n  health_check {\n    target              = \ntcp:80\n\n    interval            = 30\n    timeout             = 5\n    healthy_threshold   = 2\n    unhealthy_threshold = 2\n  }\n}\n\n\n\n\n\nArgument Reference\n#\n\n\nThe following arguments are supported:\n\n\n\n\nname\n - (Required) The name of the load balancer\n\n\nenvironment\n - (Required) The id of the environment to place the load balancer inside of\n\n\nprivate\n - (Optional) If true, the load balancer will not be exposed to the public internet\n\n\nport\n - (Optional, Default: 80:80/tcp) A list of port blocks. Ports documented below\n\n\nhealth_check\n - (Optional, Default: {\"TCP:80\" 30 5 2 2}) A health_check block. Health check documented below\n\n\n\n\nPorts (\nport\n) support the following:\n\n\n\n\nhost_port\n - (Required) The port on the load balancer to listen on\n\n\ncontainer_port\n - (Required) The port on the docker container to route to\n\n\nprotocol\n - (Required) The protocol to listen on. Valid values are \nHTTP, HTTPS, TCP, or SSL\n\n\ncertificate\n - (Optional) The name of an SSL certificate. Only required if the \nHTTP\n or \nSSL\n protocol is used.\n\n\n\n\nHealthcheck (\nhealth_check\n) supports the following:\n\n\n\n\ntarget\n - (Required) The target of the check. Valid pattern is \"${PROTOCOL}:${PORT}${PATH}\", where PROTOCOL values are:\n\n\nHTTP\n, \nHTTPS\n - PORT and PATH are required\n\n\nTCP\n, \nSSL\n - PORT is required, PATH is not supported\n\n\n\n\n\n\ninterval\n - (Required) The interval between checks.\n\n\ntimeout\n - (Required) The length of time before the check times out.\n\n\nhealthy_threshold\n - (Required) The number of checks before the instance is declared healthy.\n\n\nunhealthy_threshold\n - (Required) The number of checks before the instance is declared unhealthy.\n\n\n\n\nAttribute Reference\n#\n\n\nThe following attributes are exported:\n\n\n\n\nid\n - The id of the load balancer\n\n\nname\n - The name of the load balancer\n\n\nenvironment\n - The id of the environment the load balancer exists in\n\n\nprivate\n - Whether or not the load balancer is private\n\n\nurl\n - The URL of the load balancer\n\n\n\n\n\n\nService Resource\n#\n\n\nProvides a Layer0 Service\n\n\nExample Usage\n#\n\n\n# Create a new service\nresource \nlayer0_service\n \nguestbook\n {\n  name          = \nguestbook\n\n  environment   = \nenvironment123\n\n  deploy        = \ndeploy123\n\n  load_balancer = \nloadbalancer123\n\n  scale         = 3\n}\n\n\n\n\n\nArgument Reference\n#\n\n\nThe following arguments are supported:\n\n\n\n\nname\n - (Required) The name of the service\n\n\nenvironment\n - (Required) The id of the environment to place the service inside of\n\n\ndeploy\n - (Required) The id of the deploy for the service to run\n\n\nload_balancer\n (Optional) The id of the load balancer to place the service behind\n\n\nscale\n (Optional, Default: 1) The number of copies of the service to run\n\n\n\n\nAttribute Reference\n#\n\n\nThe following attributes are exported:\n\n\n\n\nid\n - The id of the service\n\n\nname\n - The name of the service\n\n\nenvironment\n - The id of the environment the service exists in\n\n\ndeploy\n - The id of the deploy the service is running\n\n\nload_balancer\n - The id of the load balancer the service is behind (if \nload_balancer\n was set)\n\n\nscale\n - The current desired scale of the service\n\n\n\n\n\n\nBest Practices\n#\n\n\n\n\nAlways run \nTerraform plan\n before \nterraform apply\n.\nThis will show you what action(s) Terraform plans to make before actually executing them.\n\n\nUse \nvariables\n to reference secrets.\nSecrets can be placed in a file named \nTerraform.tfvars\n, or by setting \nTF_VAR_*\n environment variables.\nMore information can be found \nhere\n.\n\n\nUse Terraform's \nremote\n command to backup and sync your \nterraform.tfstate\n file across different members in your organization.\nTerraform has documentation for using S3 as a backend \nhere\n.\n\n\nTerraform \nmodules\n allow you to define and consume reusable components.\n\n\nExample configurations can be found \nhere", 
            "title": "Layer0 Terraform Plugin"
        }, 
        {
            "location": "/reference/terraform-plugin/#layer0-terraform-provider-reference", 
            "text": "Terraform is an open-source tool for provisioning and managing infrastructure.\nIf you are new to Terraform, we recommend checking out their  documentation .  Layer0 has built a custom  provider  for Layer0.\nThis provider allows users to create, manage, and update Layer0 entities using Terraform.", 
            "title": "Layer0 Terraform Provider Reference"
        }, 
        {
            "location": "/reference/terraform-plugin/#prerequisites", 
            "text": "Terraform v0.9.4+  ( download ), accessible in your system path.", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/reference/terraform-plugin/#install", 
            "text": "Download a Layer0 v0.8.4+  release .\nThe Terraform plugin binary is located in the release zip file as  terraform-provider-layer0 .\nCopy this  terraform-provider-layer0  binary into the same directory as your Terraform binary - and you're done!  For further information, see Terraform's documentation on installing a Terraform plugin  here .", 
            "title": "Install"
        }, 
        {
            "location": "/reference/terraform-plugin/#getting-started", 
            "text": "Checkout the  Terraform  section of the Guestbook walkthrough  here .  We've added some tips and links to helpful resources in the  Best Practices  section below.", 
            "title": "Getting Started"
        }, 
        {
            "location": "/reference/terraform-plugin/#provider", 
            "text": "The Layer0 provider is used to interact with a Layer0 API.\nThe provider needs to be configured with the proper credentials before it can be used.", 
            "title": "Provider"
        }, 
        {
            "location": "/reference/terraform-plugin/#example-usage", 
            "text": "# Add  endpoint  and  token  variables\nvariable  endpoint  {}\n\nvariable  token  {}\n\n# Configure the layer0 provider\nprovider  layer0  {\n  endpoint        =  ${ var . endpoint } \n  token           =  ${ var . token } \n  skip_ssl_verify = true\n}", 
            "title": "Example Usage"
        }, 
        {
            "location": "/reference/terraform-plugin/#argument-reference", 
            "text": "The following arguments are supported:   Configuration  The  endpoint  and  token  variables for your layer0 api can be found using the  l0-setup endpoint  command    endpoint  - (Required) The endpoint of the layer0 api  token  - (Required) The authentication token for the layer0 api  skip_ssl_verify  - (Optional) If true, ssl certificate mismatch warnings will be ignored", 
            "title": "Argument Reference"
        }, 
        {
            "location": "/reference/terraform-plugin/#api-data-source", 
            "text": "The API data source is used to extract useful read-only variables from the Layer0 API.", 
            "title": "API Data Source"
        }, 
        {
            "location": "/reference/terraform-plugin/#example-usage_1", 
            "text": "# Configure the api data source\ndata  layer0_api   config  {}\n\n# Output the layer0 vpc id\noutput  vpc id  {\n  val =  ${ data . layer0_api . config . vpc_id } \n}", 
            "title": "Example Usage"
        }, 
        {
            "location": "/reference/terraform-plugin/#attribute-reference", 
            "text": "The following attributes are exported:   prefix  - The prefix of the layer0 instance  vpc_id  - The vpc id of the layer0 instance  public_subnets  - A list containing the 2 public subnet ids in the layer0 vpc  private_subnets  - A list containing the 2 private subnet ids in the layer0 vpc", 
            "title": "Attribute Reference"
        }, 
        {
            "location": "/reference/terraform-plugin/#deploy-data-source", 
            "text": "The Deploy data source is used to extract Layer0 Deploy attributes.", 
            "title": "Deploy Data Source"
        }, 
        {
            "location": "/reference/terraform-plugin/#example-usage_2", 
            "text": "# Configure the deploy data source\ndata  layer0_deploy   dpl  {\n  name    =  my-deploy \n  version =  1 \n}\n\n# Output the layer0 deploy id\noutput  deploy_id  {\n  val =  ${ data . layer0_deploy . dpl . id } \n}", 
            "title": "Example Usage"
        }, 
        {
            "location": "/reference/terraform-plugin/#argument-reference_1", 
            "text": "The following arguments are supported:   name  - (Required) The name of the deploy  version  - (Required) The version of the deploy", 
            "title": "Argument Reference"
        }, 
        {
            "location": "/reference/terraform-plugin/#attribute-reference_1", 
            "text": "The following attributes are exported:   name  - The name of the deploy  version  - The version of the deploy  id  - The id of the deploy", 
            "title": "Attribute Reference"
        }, 
        {
            "location": "/reference/terraform-plugin/#environment-data-source", 
            "text": "The Environment data source is used to extract Layer0 Environment attributes.", 
            "title": "Environment Data Source"
        }, 
        {
            "location": "/reference/terraform-plugin/#example-usage_3", 
            "text": "# Configure the environment data source\ndata  layer0_environment   env  {\n  name =  my-environment \n}\n\n# Output the layer0 environment id\noutput  environment_id  {\n  val =  ${ data . layer0_environment . env . id } \n}", 
            "title": "Example Usage"
        }, 
        {
            "location": "/reference/terraform-plugin/#argument-reference_2", 
            "text": "The following arguments are supported:   name  - (Required) The name of the environment", 
            "title": "Argument Reference"
        }, 
        {
            "location": "/reference/terraform-plugin/#attribute-reference_2", 
            "text": "The following attributes are exported:   id  - The id of the environment  name  - The name of the environment  size  - The size of the instances in the environment  min_count  - The current number instances in the environment  os  - The operating system used for the environment  ami  - The AMI ID used for the environment", 
            "title": "Attribute Reference"
        }, 
        {
            "location": "/reference/terraform-plugin/#load-balancer-data-source", 
            "text": "The Load Balancer data source is used to extract Layer0 Load Balancer attributes.", 
            "title": "Load Balancer Data Source"
        }, 
        {
            "location": "/reference/terraform-plugin/#example-usage_4", 
            "text": "# Configure the load balancer source\ndata  layer0_load_balancer   lb  {\n  name           =  my-loadbalancer \n  environment_id =  ${ data . layer0_environment . env . environment_id } \n}\n\n# Output the layer0 load balancer id\noutput  load_balancer_id  {\n  val =  ${ data . layer0_load_balancer . lb . id } \n}", 
            "title": "Example Usage"
        }, 
        {
            "location": "/reference/terraform-plugin/#argument-reference_3", 
            "text": "The following arguments are supported:   name  - (required) The name of the load balancer  environment_id  - (required) The id of the environment the load balancer exists in", 
            "title": "Argument Reference"
        }, 
        {
            "location": "/reference/terraform-plugin/#attribute-reference_3", 
            "text": "The following attributes are exported:   id  - The id of the load balancer  name  - The name of the load balancer  environment_id  - The id of the environment the load balancer exists in  environment_name  - The name of the environment the load balancer exists in  private  - Whether or not the load balancer is private  url  - The URL of the load balancer", 
            "title": "Attribute Reference"
        }, 
        {
            "location": "/reference/terraform-plugin/#service-data-source", 
            "text": "The Service data source is used to extract Layer0 Service attributes.", 
            "title": "Service Data Source"
        }, 
        {
            "location": "/reference/terraform-plugin/#example-usage_5", 
            "text": "# Configure the service data source\ndata  layer0_service   svc  {\n  name           =  my-service \n  environment_id =  ${ data . layer0_environment . env . environment_id } \n}\n\n# Output the layer0 service id\noutput  service_id  {\n  val =  ${ data . layer0_service . svc . id } \n}", 
            "title": "Example Usage"
        }, 
        {
            "location": "/reference/terraform-plugin/#argument-reference_4", 
            "text": "The following arguments are supported:   name  - (required) The name of the service  environment_id  - (required) The id of the environment the service exists in", 
            "title": "Argument Reference"
        }, 
        {
            "location": "/reference/terraform-plugin/#attribute-reference_4", 
            "text": "The following attributes are exported:   id  - The id of the service  name  - The name of the service  environment_id  - The id of the environment the service exists in  environment_name  - The name of the environment the service exists in  scale  - The current desired scale of the service", 
            "title": "Attribute Reference"
        }, 
        {
            "location": "/reference/terraform-plugin/#deploy-resource", 
            "text": "Provides a Layer0 Deploy.  Performing variable substitution inside of your deploy's json file (typically named  Dockerrun.aws.json ) can be done through Terraform's  template_file .\nFor a working example, please see the sample  Guestbook  application", 
            "title": "Deploy Resource"
        }, 
        {
            "location": "/reference/terraform-plugin/#example-usage_6", 
            "text": "# Configure the deploy template\ndata  template_file   guestbook  {\n  template =  ${ file ( Dockerrun.aws.json ) } \n  vars {\n    docker_image_tag =  latest \n  }\n}\n\n# Create a deploy using the rendered template\nresource  layer0_deploy   guestbook  {\n  name    =  guestbook \n  content =  ${ data . template_file . guestbook . rendered } \n}", 
            "title": "Example Usage"
        }, 
        {
            "location": "/reference/terraform-plugin/#argument-reference_5", 
            "text": "The following arguments are supported:   name  - (Required) The name of the deploy  content  - (Required) The content of the deploy", 
            "title": "Argument Reference"
        }, 
        {
            "location": "/reference/terraform-plugin/#attribute-reference_5", 
            "text": "The following attributes are exported:   id  - The id of the deploy  name  - The name of the deploy  version  - The version number of the deploy", 
            "title": "Attribute Reference"
        }, 
        {
            "location": "/reference/terraform-plugin/#environment-resource", 
            "text": "Provides a Layer0 Environment", 
            "title": "Environment Resource"
        }, 
        {
            "location": "/reference/terraform-plugin/#example-usage_7", 
            "text": "# Create a new environment\nresource  layer0_environment   demo  {\n  name      =  demo \n  size      =  m3.medium \n  min_count = 0\n  user_data =  echo hello, world \n  os        =  linux \n  ami       =  ami123 \n}", 
            "title": "Example Usage"
        }, 
        {
            "location": "/reference/terraform-plugin/#argument-reference_6", 
            "text": "The following arguments are supported:   name  - (Required) The name of the environment  size  - (Optional, Default: \"m3.medium\") The size of the instances in the environment.\nAvailable instance sizes can be found  here  min_count  - (Optional, Default: 0) The minimum number of instances allowed in the environment  user-data  - (Optional) The user data template to use for the environment's autoscaling group.\nSee the  cli reference  for the default template.  os  - (Optional, Default: \"linux\") Specifies the type of operating system used in the environment.\nOptions are \"linux\" or \"windows\".  ami  - (Optional) A custom AMI ID to use in the environment. \nIf not specified, Layer0 will use its default AMI ID for the specified operating system.", 
            "title": "Argument Reference"
        }, 
        {
            "location": "/reference/terraform-plugin/#attribute-reference_6", 
            "text": "The following attributes are exported:   id  - The id of the environment  name  - The name of the environment  size  - The size of the instances in the environment  cluster_count  - The current number instances in the environment  security_group_id  - The ID of the environment's security group  os  - The operating system used for the environment  ami  - The AMI ID used for the environment", 
            "title": "Attribute Reference"
        }, 
        {
            "location": "/reference/terraform-plugin/#load-balancer-resource", 
            "text": "Provides a Layer0 Load Balancer", 
            "title": "Load Balancer Resource"
        }, 
        {
            "location": "/reference/terraform-plugin/#example-usage_8", 
            "text": "# Create a new load balancer\nresource  layer0_load_balancer   guestbook  {\n  name        =  guestbook \n  environment =  demo123 \n  private     = false\n\n  port {\n    host_port      = 80\n    container_port = 80\n    protocol       =  http \n  }\n\n  port {\n    host_port      = 443\n    container_port = 443\n    protocol       =  https \n    certificate    =  cert \n  }\n\n  health_check {\n    target              =  tcp:80 \n    interval            = 30\n    timeout             = 5\n    healthy_threshold   = 2\n    unhealthy_threshold = 2\n  }\n}", 
            "title": "Example Usage"
        }, 
        {
            "location": "/reference/terraform-plugin/#argument-reference_7", 
            "text": "The following arguments are supported:   name  - (Required) The name of the load balancer  environment  - (Required) The id of the environment to place the load balancer inside of  private  - (Optional) If true, the load balancer will not be exposed to the public internet  port  - (Optional, Default: 80:80/tcp) A list of port blocks. Ports documented below  health_check  - (Optional, Default: {\"TCP:80\" 30 5 2 2}) A health_check block. Health check documented below   Ports ( port ) support the following:   host_port  - (Required) The port on the load balancer to listen on  container_port  - (Required) The port on the docker container to route to  protocol  - (Required) The protocol to listen on. Valid values are  HTTP, HTTPS, TCP, or SSL  certificate  - (Optional) The name of an SSL certificate. Only required if the  HTTP  or  SSL  protocol is used.   Healthcheck ( health_check ) supports the following:   target  - (Required) The target of the check. Valid pattern is \"${PROTOCOL}:${PORT}${PATH}\", where PROTOCOL values are:  HTTP ,  HTTPS  - PORT and PATH are required  TCP ,  SSL  - PORT is required, PATH is not supported    interval  - (Required) The interval between checks.  timeout  - (Required) The length of time before the check times out.  healthy_threshold  - (Required) The number of checks before the instance is declared healthy.  unhealthy_threshold  - (Required) The number of checks before the instance is declared unhealthy.", 
            "title": "Argument Reference"
        }, 
        {
            "location": "/reference/terraform-plugin/#attribute-reference_7", 
            "text": "The following attributes are exported:   id  - The id of the load balancer  name  - The name of the load balancer  environment  - The id of the environment the load balancer exists in  private  - Whether or not the load balancer is private  url  - The URL of the load balancer", 
            "title": "Attribute Reference"
        }, 
        {
            "location": "/reference/terraform-plugin/#service-resource", 
            "text": "Provides a Layer0 Service", 
            "title": "Service Resource"
        }, 
        {
            "location": "/reference/terraform-plugin/#example-usage_9", 
            "text": "# Create a new service\nresource  layer0_service   guestbook  {\n  name          =  guestbook \n  environment   =  environment123 \n  deploy        =  deploy123 \n  load_balancer =  loadbalancer123 \n  scale         = 3\n}", 
            "title": "Example Usage"
        }, 
        {
            "location": "/reference/terraform-plugin/#argument-reference_8", 
            "text": "The following arguments are supported:   name  - (Required) The name of the service  environment  - (Required) The id of the environment to place the service inside of  deploy  - (Required) The id of the deploy for the service to run  load_balancer  (Optional) The id of the load balancer to place the service behind  scale  (Optional, Default: 1) The number of copies of the service to run", 
            "title": "Argument Reference"
        }, 
        {
            "location": "/reference/terraform-plugin/#attribute-reference_8", 
            "text": "The following attributes are exported:   id  - The id of the service  name  - The name of the service  environment  - The id of the environment the service exists in  deploy  - The id of the deploy the service is running  load_balancer  - The id of the load balancer the service is behind (if  load_balancer  was set)  scale  - The current desired scale of the service", 
            "title": "Attribute Reference"
        }, 
        {
            "location": "/reference/terraform-plugin/#best-practices", 
            "text": "Always run  Terraform plan  before  terraform apply .\nThis will show you what action(s) Terraform plans to make before actually executing them.  Use  variables  to reference secrets.\nSecrets can be placed in a file named  Terraform.tfvars , or by setting  TF_VAR_*  environment variables.\nMore information can be found  here .  Use Terraform's  remote  command to backup and sync your  terraform.tfstate  file across different members in your organization.\nTerraform has documentation for using S3 as a backend  here .  Terraform  modules  allow you to define and consume reusable components.  Example configurations can be found  here", 
            "title": "Best Practices"
        }, 
        {
            "location": "/reference/updateservice/", 
            "text": "Updating a Layer0 service\n#\n\n\nThere are three methods of updating an existing Layer0 service. The first method is to update the existing Deploy to refer to a new Docker task definition. The second method is to create a new Service that uses the same Loadbalancer. The third method is to create both a new Loadbalancer and a new Service.\n\n\nThere are advantages and disadvantages to each of these methods. The following sections discuss the advantages and disadvantages of using each method, and include procedures for implementing each method.\n\n\nMethod 1: Refer to a new task definition\n#\n\n\nThis method of updating a Layer0 application is the easiest to implement, because you do not need to rescale the Service or modify the Loadbalancer. This method is completely transparent to all other components of the application, and using this method does not involve any downtime.\n\n\nThe disadvantage of using this method is that you cannot perform A/B testing of the old and new services, and you cannot control which traffic goes to the old service and which goes to the new one.\n\n\nTo replace a Deploy to refer to a new task definition:\n\n\n\n\nAt the command line, type the following to create a new Deploy: \nl0 deploy create [pathToTaskDefinition] [deployName]\nNote that if \n[deployName]\n already exists, this step will create a new version of that Deploy.\n\n\nType the following to update the existing Service: \nl0 service update [existingServiceName] [deployName]\nBy default, the Service you specify in this command will refer to the latest version of \n[deployName]\n, if multiple versions of the Deploy exist.\nNote\nIf you want to refer to a specific version of the Deploy, type the following command instead of the one shown above: \nl0 service update [serviceName] [deployName]:[deployVersion]\n\n\n\n\nMethod 2: Create a new Deploy and Service using the same Loadbalancer\n#\n\n\nThis method of updating a Layer0 application is also rather easy to implement. Like the method described in the previous section, this method is completely transparent to all other services and components of the application. This method also you allows you to re-scale the service if necessary, using the \nl0 service scale\n command. Finally, this method allows for indirect A/B testing of the application; you can change the scale of the application, and observe the success and failure rates.\n\n\nThe disadvantage of using this method is that you cannot control the routing of traffic between the old and new versions of the application.\n\n\nTo create a new Deploy and Service:\n\n\n\n\nAt the command line, type the following to create a new Deploy (or a new version of the Deploy, if \n[deployName]\n already exists):\n \nl0 deploy create [pathToTaskDefinition] [deployName]\n\n\nType the following command to create a new Service that refers to \n[deployName]\n behind an existing Loadbalancer named \n[loadbalancerName]\n:\n \nl0 service create --loadbalancer [loadbalancerName] [environmentName] [deployName]\n\n\nCheck to make sure that the new Service is working as expected. If it is, and you do not want to keep the old Service, type the following command to delete the old Service: \nl0 service delete [oldServiceName]\n\n\n\n\nMethod 3: Create a new Deploy, Loadbalancer and Service\n#\n\n\nThe final method of updating a Layer0 service is to create an entirely new Deploy, Loadbalancer and Service. This method gives you complete control over both the new and the old Service, and allows you to perform true A/B testing by routing traffic to individual Services.\n\n\nThe disadvantage of using this method is that you need to implement a method of routing traffic between the new and the old Loadbalancer.\n\n\nTo create a new Deploy, Loadbalancer and Service:\n\n\n\n\nAt the command line, type the following command to create a new Deploy:\nl0 deploy create [pathToTaskDefinition] [deployName]\n\n\nType the following command to create a new Loadbalancer:\n \nl0 loadbalancer create --port [portNumber] [environmentName] [loadbalancerName] [deployName]\nNote\nThe value of \n[loadbalancerName]\n in the above command must be unique.\n\n\nType the following command to create a new Service: \nl0 service create --loadbalancer [loadBalancerName] [environmentName] [serviceName] [deployName]\nNote\nThe value of \n[serviceName]\n in the above command  must be unique.\n\n\nImplement a method of routing traffic between the old and new Services, such as \nHAProxy\n or \nConsul\n.", 
            "title": "Updating a Service"
        }, 
        {
            "location": "/reference/updateservice/#updating-a-layer0-service", 
            "text": "There are three methods of updating an existing Layer0 service. The first method is to update the existing Deploy to refer to a new Docker task definition. The second method is to create a new Service that uses the same Loadbalancer. The third method is to create both a new Loadbalancer and a new Service.  There are advantages and disadvantages to each of these methods. The following sections discuss the advantages and disadvantages of using each method, and include procedures for implementing each method.", 
            "title": "Updating a Layer0 service"
        }, 
        {
            "location": "/reference/updateservice/#method-1-refer-to-a-new-task-definition", 
            "text": "This method of updating a Layer0 application is the easiest to implement, because you do not need to rescale the Service or modify the Loadbalancer. This method is completely transparent to all other components of the application, and using this method does not involve any downtime.  The disadvantage of using this method is that you cannot perform A/B testing of the old and new services, and you cannot control which traffic goes to the old service and which goes to the new one.  To replace a Deploy to refer to a new task definition:   At the command line, type the following to create a new Deploy:  l0 deploy create [pathToTaskDefinition] [deployName] Note that if  [deployName]  already exists, this step will create a new version of that Deploy.  Type the following to update the existing Service:  l0 service update [existingServiceName] [deployName] By default, the Service you specify in this command will refer to the latest version of  [deployName] , if multiple versions of the Deploy exist. Note If you want to refer to a specific version of the Deploy, type the following command instead of the one shown above:  l0 service update [serviceName] [deployName]:[deployVersion]", 
            "title": "Method 1: Refer to a new task definition"
        }, 
        {
            "location": "/reference/updateservice/#method-2-create-a-new-deploy-and-service-using-the-same-loadbalancer", 
            "text": "This method of updating a Layer0 application is also rather easy to implement. Like the method described in the previous section, this method is completely transparent to all other services and components of the application. This method also you allows you to re-scale the service if necessary, using the  l0 service scale  command. Finally, this method allows for indirect A/B testing of the application; you can change the scale of the application, and observe the success and failure rates.  The disadvantage of using this method is that you cannot control the routing of traffic between the old and new versions of the application.  To create a new Deploy and Service:   At the command line, type the following to create a new Deploy (or a new version of the Deploy, if  [deployName]  already exists):   l0 deploy create [pathToTaskDefinition] [deployName]  Type the following command to create a new Service that refers to  [deployName]  behind an existing Loadbalancer named  [loadbalancerName] :   l0 service create --loadbalancer [loadbalancerName] [environmentName] [deployName]  Check to make sure that the new Service is working as expected. If it is, and you do not want to keep the old Service, type the following command to delete the old Service:  l0 service delete [oldServiceName]", 
            "title": "Method 2: Create a new Deploy and Service using the same Loadbalancer"
        }, 
        {
            "location": "/reference/updateservice/#method-3-create-a-new-deploy-loadbalancer-and-service", 
            "text": "The final method of updating a Layer0 service is to create an entirely new Deploy, Loadbalancer and Service. This method gives you complete control over both the new and the old Service, and allows you to perform true A/B testing by routing traffic to individual Services.  The disadvantage of using this method is that you need to implement a method of routing traffic between the new and the old Loadbalancer.  To create a new Deploy, Loadbalancer and Service:   At the command line, type the following command to create a new Deploy: l0 deploy create [pathToTaskDefinition] [deployName]  Type the following command to create a new Loadbalancer:   l0 loadbalancer create --port [portNumber] [environmentName] [loadbalancerName] [deployName] Note The value of  [loadbalancerName]  in the above command must be unique.  Type the following command to create a new Service:  l0 service create --loadbalancer [loadBalancerName] [environmentName] [serviceName] [deployName] Note The value of  [serviceName]  in the above command  must be unique.  Implement a method of routing traffic between the old and new Services, such as  HAProxy  or  Consul .", 
            "title": "Method 3: Create a new Deploy, Loadbalancer and Service"
        }, 
        {
            "location": "/reference/consul/", 
            "text": "Consul reference\n#\n\n\nConsul\n is an open-source tool for discovering and configuring services in your network architecture. Specifically, Consul provides the following features:\n\n\n\n\nDiscovery of services\n\n\nMonitoring of the health of services\n\n\nKey/value storage with a simple HTTP API\n\n\n\n\nConsul Agent\n#\n\n\nThe \nConsul Agent\n exposes a DNS API for easy consumption of data generated by \nRegistrator\n. The Consul Agent can run either in server or client mode.\n\n\nWhen run as a Layer0 Service, the Consul Agent runs in server mode. To ensure the integrity of your data, the service in which you run consul should be scaled to size 3 or greater. A group of several consul deployments is known as a \"\ncluster\n.\"\n\n\nOther Layer0 Services that use Consul will run the Consul Agent in client mode, alongside their application containers.\nThe client is a very lightweight process that registers services, runs health checks, and forwards queries to servers.\n\n\nRegistrator\n#\n\n\nRegistrator\n is a tool that automatically registers and deregisters services into a Consul Cluster by inspecting Docker containers as they come online.\nContainer registration is based off of environment variables on the container.\nLayer0 Services that use Consul will run Registrator alongside their application containers.\n\n\nService Configuration\n#\n\n\nLayer0 Services that use Consul will need to add the \nRegistrator\n and \nConsul Agent\n definitions to the\n\ncontainerDefinitions\n section of your Deploys. You must also add the \nDocker Socket\n definition to the \nvolumes\n section of your Deploys.\n\n\nFor an example of a Deploy that uses Consul, see the \nGuestbook with Consul\n guide.\n\n\n\n\nRegistrator Container Definition\n#\n\n\n{\n    \nname\n: \nregistrator\n,\n    \nimage\n: \ngliderlabs/registrator:master\n,\n    \nessential\n: true,\n    \nlinks\n: [\nconsul-agent\n],\n    \nentrypoint\n: [\n/bin/sh\n, \n-c\n],\n    \ncommand\n: [\n/bin/registrator -retry-attempts=-1 -retry-interval=30000 -ip $(wget http://169.254.169.254/latest/meta-data/local-ipv4 -q -O -) consul://consul-agent:8500\n],\n    \nmemory\n: 128,\n    \nmountPoints\n: [\n        {\n            \nsourceVolume\n: \ndockersocket\n,\n            \ncontainerPath\n: \n/tmp/docker.sock\n\n        }\n    ]\n},\n\n\n\n\n\n\n\nConsul Agent Container Definition\n#\n\n\n\n\nWarning\n\n\n\n\nYou must replace \nurl\n with your Layer0 Consul Load Balancer's\n\n\n{\n    \nname\n: \nconsul-agent\n,\n    \nimage\n: \nprogrium/consul\n,\n    \nessential\n: true,\n    \nentrypoint\n: [\n/bin/bash\n, \n-c\n],\n    \ncommand\n: [\n/bin/start -advertise $(wget http://169.254.169.254/latest/meta-data/local-ipv4 -q -O -) -retry-join $EXTERNAL_URL -recursor $UPSTREAM_DNS -retry-interval 30s\n],\n    \nmemory\n: 128,\n    \nportMappings\n: [\n        {\n            \nhostPort\n: 8500,\n            \ncontainerPort\n: 8500\n        },\n        {\n            \nhostPort\n: 53,\n            \ncontainerPort\n: 53,\n            \nprotocol\n: \nudp\n\n        }\n    ],\n    \nenvironment\n: [\n        {\n            \nname\n: \nEXTERNAL_URL\n,\n            \nvalue\n: \nurl\n\n    },\n    {\n            \nname\n: \nUPSTREAM_DNS\n,\n            \nvalue\n: \n10.100.0.2\n\n        }\n    ]\n},\n\n\n\n\n\nEnvironment Variables\n#\n\n\n\n\nEXTERNAL_URL\n - URL of the consul cluster\n\n\nUPSTREAM_DNS\n - The DNS server consul-agent queries for DNS entries that it cannot resolve internally (e.g. google.com)\n\n\nThe default value for \nUPSTREAM_DNS\n assumes you're using the default Layer0 configuration, making your internal DNS endpoint 10.100.0.2.  If you are a using a non standard configuration (e.g. installing Layer0 in an existing VPC with a CIDR other than \n10.100.0.0/16\n) please modify this variable accordingly.\n\n\n\n\n\n\n\n\n\n\nDocker Socket Volume Definition\n#\n\n\nvolumes\n: [\n    {\n        \nname\n: \ndockersocket\n,\n        \nhost\n: {\n                \nsourcePath\n: \n/var/run/docker.sock\n\n        }\n    }\n],", 
            "title": "Consul"
        }, 
        {
            "location": "/reference/consul/#consul-reference", 
            "text": "Consul  is an open-source tool for discovering and configuring services in your network architecture. Specifically, Consul provides the following features:   Discovery of services  Monitoring of the health of services  Key/value storage with a simple HTTP API", 
            "title": "Consul reference"
        }, 
        {
            "location": "/reference/consul/#consul-agent", 
            "text": "The  Consul Agent  exposes a DNS API for easy consumption of data generated by  Registrator . The Consul Agent can run either in server or client mode.  When run as a Layer0 Service, the Consul Agent runs in server mode. To ensure the integrity of your data, the service in which you run consul should be scaled to size 3 or greater. A group of several consul deployments is known as a \" cluster .\"  Other Layer0 Services that use Consul will run the Consul Agent in client mode, alongside their application containers.\nThe client is a very lightweight process that registers services, runs health checks, and forwards queries to servers.", 
            "title": "Consul Agent"
        }, 
        {
            "location": "/reference/consul/#registrator", 
            "text": "Registrator  is a tool that automatically registers and deregisters services into a Consul Cluster by inspecting Docker containers as they come online.\nContainer registration is based off of environment variables on the container.\nLayer0 Services that use Consul will run Registrator alongside their application containers.", 
            "title": "Registrator"
        }, 
        {
            "location": "/reference/consul/#service-configuration", 
            "text": "Layer0 Services that use Consul will need to add the  Registrator  and  Consul Agent  definitions to the containerDefinitions  section of your Deploys. You must also add the  Docker Socket  definition to the  volumes  section of your Deploys.  For an example of a Deploy that uses Consul, see the  Guestbook with Consul  guide.", 
            "title": "Service Configuration"
        }, 
        {
            "location": "/reference/consul/#registrator-container-definition", 
            "text": "{\n     name :  registrator ,\n     image :  gliderlabs/registrator:master ,\n     essential : true,\n     links : [ consul-agent ],\n     entrypoint : [ /bin/sh ,  -c ],\n     command : [ /bin/registrator -retry-attempts=-1 -retry-interval=30000 -ip $(wget http://169.254.169.254/latest/meta-data/local-ipv4 -q -O -) consul://consul-agent:8500 ],\n     memory : 128,\n     mountPoints : [\n        {\n             sourceVolume :  dockersocket ,\n             containerPath :  /tmp/docker.sock \n        }\n    ]\n},", 
            "title": "Registrator Container Definition"
        }, 
        {
            "location": "/reference/consul/#consul-agent-container-definition", 
            "text": "Warning   You must replace  url  with your Layer0 Consul Load Balancer's  {\n     name :  consul-agent ,\n     image :  progrium/consul ,\n     essential : true,\n     entrypoint : [ /bin/bash ,  -c ],\n     command : [ /bin/start -advertise $(wget http://169.254.169.254/latest/meta-data/local-ipv4 -q -O -) -retry-join $EXTERNAL_URL -recursor $UPSTREAM_DNS -retry-interval 30s ],\n     memory : 128,\n     portMappings : [\n        {\n             hostPort : 8500,\n             containerPort : 8500\n        },\n        {\n             hostPort : 53,\n             containerPort : 53,\n             protocol :  udp \n        }\n    ],\n     environment : [\n        {\n             name :  EXTERNAL_URL ,\n             value :  url \n    },\n    {\n             name :  UPSTREAM_DNS ,\n             value :  10.100.0.2 \n        }\n    ]\n},", 
            "title": "Consul Agent Container Definition"
        }, 
        {
            "location": "/reference/consul/#environment-variables", 
            "text": "EXTERNAL_URL  - URL of the consul cluster  UPSTREAM_DNS  - The DNS server consul-agent queries for DNS entries that it cannot resolve internally (e.g. google.com)  The default value for  UPSTREAM_DNS  assumes you're using the default Layer0 configuration, making your internal DNS endpoint 10.100.0.2.  If you are a using a non standard configuration (e.g. installing Layer0 in an existing VPC with a CIDR other than  10.100.0.0/16 ) please modify this variable accordingly.", 
            "title": "Environment Variables"
        }, 
        {
            "location": "/reference/consul/#docker-socket-volume-definition", 
            "text": "volumes : [\n    {\n         name :  dockersocket ,\n         host : {\n                 sourcePath :  /var/run/docker.sock \n        }\n    }\n],", 
            "title": "Docker Socket Volume Definition"
        }, 
        {
            "location": "/reference/task_definition/", 
            "text": "Task Definitions\n#\n\n\nThis guide gives some overview into the composition of a task definition.\nFor more comprehensive documentation, we recommend taking a look at the official AWS docs:\n\n\n\n\nCreating a Task Definition\n\n\nTask Definition Parameters\n\n\n\n\nSample\n#\n\n\nThe following snippet contains the task definition for the \nGuestbook\n application\n\n\n{\n    \nAWSEBDockerrunVersion\n: 2,\n    \ncontainerDefinitions\n: [\n        {\n            \nname\n: \nguestbook\n,\n            \nimage\n: \nquintilesims/guestbook\n,\n            \nessential\n: true,\n            \nmemory\n: 128,\n            \nportMappings\n: [\n                {\n                    \nhostPort\n: 80,\n                    \ncontainerPort\n: 80\n                }\n            ],\n        }\n    ]\n}\n\n\n\n\n\n\n\nName\n The name of the container\n\n\n\n\n\n\nWarning\n\n\n\n\nIf you wish to update your task definition, the container names \nmust\n remain the same.\nIf any container names are changed or removed in an updated task definition,\nECS will not know how the existing container(s) should be mapped over and you will not be able to deploy the updated task definition.\nIf you encounter a scenario where you must change or remove a container's name in a task definition, we recommend re-creating the Layer0 Deploy and Service.\n\n\n\n\nImage\n The Docker image used to build the container. The image format is \nurl/image:tag\n\n\nThe \nurl\n specifies which Docker Repo to pull the image from\n    If a non-Docker-Hub \nurl\n is not specified, \nDocker Hub\n is used (as is the case here)\n\n\nThe \nimage\n specifies the name of the image to grab (in this case, the \nguestbook\n image from the \nquintilesims\n Docker Hub group)\n\n\nThe \ntag\n specifies which version of image to grab\nIf \ntag\n is not specified, \n:latest\n is used\n\n\n\n\n\n\nEssential\n If set to \ntrue\n, all other containers in the task definition will be stopped if that container fails or stops for any reason.\nOtherwise, the container's failure will not affect the rest of the containers in the task definition.\n\n\nMemory\n The number of MiB of memory to reserve for the container.\nIf your container attempts to exceed the memory allocated here, the container is killed\n\n\nPortMappings\n A list of hostPort, containerPort mappings for the container\n\n\nHostPort\n The port number on the host instance reserved for your container.\nIf your Layer0 Service is behind a Layer0 Load Balancer, this should map to an \ninstancePort\n on the Layer0 Load Balancer.\n\n\nContainerPort\n The port number the container should receive traffic on.\nAny traffic received from the instance's \nhostPort\n will be forwarded to the container on this port", 
            "title": "Task Definitions"
        }, 
        {
            "location": "/reference/task_definition/#task-definitions", 
            "text": "This guide gives some overview into the composition of a task definition.\nFor more comprehensive documentation, we recommend taking a look at the official AWS docs:   Creating a Task Definition  Task Definition Parameters", 
            "title": "Task Definitions"
        }, 
        {
            "location": "/reference/task_definition/#sample", 
            "text": "The following snippet contains the task definition for the  Guestbook  application  {\n     AWSEBDockerrunVersion : 2,\n     containerDefinitions : [\n        {\n             name :  guestbook ,\n             image :  quintilesims/guestbook ,\n             essential : true,\n             memory : 128,\n             portMappings : [\n                {\n                     hostPort : 80,\n                     containerPort : 80\n                }\n            ],\n        }\n    ]\n}   Name  The name of the container    Warning   If you wish to update your task definition, the container names  must  remain the same.\nIf any container names are changed or removed in an updated task definition,\nECS will not know how the existing container(s) should be mapped over and you will not be able to deploy the updated task definition.\nIf you encounter a scenario where you must change or remove a container's name in a task definition, we recommend re-creating the Layer0 Deploy and Service.   Image  The Docker image used to build the container. The image format is  url/image:tag  The  url  specifies which Docker Repo to pull the image from\n    If a non-Docker-Hub  url  is not specified,  Docker Hub  is used (as is the case here)  The  image  specifies the name of the image to grab (in this case, the  guestbook  image from the  quintilesims  Docker Hub group)  The  tag  specifies which version of image to grab\nIf  tag  is not specified,  :latest  is used    Essential  If set to  true , all other containers in the task definition will be stopped if that container fails or stops for any reason.\nOtherwise, the container's failure will not affect the rest of the containers in the task definition.  Memory  The number of MiB of memory to reserve for the container.\nIf your container attempts to exceed the memory allocated here, the container is killed  PortMappings  A list of hostPort, containerPort mappings for the container  HostPort  The port number on the host instance reserved for your container.\nIf your Layer0 Service is behind a Layer0 Load Balancer, this should map to an  instancePort  on the Layer0 Load Balancer.  ContainerPort  The port number the container should receive traffic on.\nAny traffic received from the instance's  hostPort  will be forwarded to the container on this port", 
            "title": "Sample"
        }, 
        {
            "location": "/reference/architecture/", 
            "text": "Layer0 Architecture\n#\n\n\nLayer0 is built on top of the following primary technologies:\n\n\n\n\nApplication Container: \nDocker\n\n\nCloud Provider: \nAmazon Web Services\n\n\nContainer Management: \nAmazon EC2 Container Service (ECS)\n\n\nLoad Balancing: \nAmazon Elastic Load Balancing\n\n\nInfrastructure Configuration: Hashicorp \nTerraform\n\n\nIdentity Management: \nAuth0", 
            "title": "Architecture"
        }, 
        {
            "location": "/reference/architecture/#layer0-architecture", 
            "text": "Layer0 is built on top of the following primary technologies:   Application Container:  Docker  Cloud Provider:  Amazon Web Services  Container Management:  Amazon EC2 Container Service (ECS)  Load Balancing:  Amazon Elastic Load Balancing  Infrastructure Configuration: Hashicorp  Terraform  Identity Management:  Auth0", 
            "title": "Layer0 Architecture"
        }, 
        {
            "location": "/reference/ecr/", 
            "text": "EC2 Container Registry\n#\n\n\nECR is an Amazon implementation of a docker registry.  It acts as a private registry in your AWS account, which can be accessed from any docker client, and Layer0.  Consider using ECR if you have stability issues with hosted docker registries, and do not wish to share your images publicly on \ndockerhub\n.\n\n\nSetup\n#\n\n\nWhen interacting with ECR, you will first need to create a repository and a login to interact from your development machine.\n\n\nRepository\n#\n\n\nEach repository needs to be created by an AWS api call.\n\n\n  \n aws ecr create-repository --repository-name myteam/myproject\n\n\n\n\n\nLogin\n#\n\n\nTo authenticate with the ECR service, Amazon provides the \nget-login\n command, which generates an authentication token, and returns a docker command to set it up\n\n\n  \n aws ecr get-login\n  # this command will return the following: (password is typically hundreds of characters)\n  docker login -u AWS -p password -e none https://aws_account_id.dkr.ecr.us-east-1.amazonaws.com\n\n\n\n\n\nExecute the provided docker command to store the login credentials\n\n\nAfterward creating the repository and local login credentials you may interact with images (and tags) under this path from a local docker client.\n\n\n  docker pull \n${\necr\n-\nurl\n}\n/myteam/myproject\n  docker push \n${\necr\n-\nurl\n}\n/myteam/myproject:custom-tag-1\n\n\n\n\n\nDeploy Example\n#\n\n\nHere we'll walk through using ECR when deploying to Layer0,  Using a very basic wait container.\n\n\nMake docker image\n#\n\n\nYour docker image can be built locally or pulled from dockerhub.  For this example, we made a service that waits and then exits (useful for triggering regular restarts).\n\n\nFROM busybox\n\nENV SLEEP_TIME=60\n\nCMD sleep $SLEEP_TIME\n\n\n\n\n\nThen build the file, with the tag \nxfra/wait\n\n\n \n docker build -f Dockerfile.wait -t xfra/wait .\n\n\n\n\n\nUpload to ECR\n#\n\n\nAfter preparing a login and registry, tag the image with the remote url, and use \ndocker push\n\n\n  docker tag xfra/wait 111222333444.dkr.ecr.us-east-1.amazonaws.com/xfra-wait\n  docker push 111222333444.dkr.ecr.us-east-1.amazonaws.com/xfra-wait\n\n\n\n\n\n\n\nNote: your account id in this url will be different.\n\n\n\n\nCreate a deploy\n#\n\n\nTo run this image in Layer0, we create a dockerrun file, describing the instance and any additional variables\n\n\n{\n  \ncontainerDefinitions\n: [\n    {\n      \nname\n: \ntimeout\n,\n      \nimage\n: \n111222333444.dkr.ecr.us-east-1.amazonaws.com/xfra-wait:latest\n,\n      \nessential\n: true,\n      \nmemory\n: 10,\n      \nenvironment\n: [\n        { \nname\n: \nSLEEP_TIME\n, \nvalue\n: \n43200\n }\n      ]\n    }\n  ]\n}\n\n\n\n\n\nAnd create that in Layer0\n\n\n  l0 deploy create timeout.dockerrun.aws.json timeout\n\n\n\n\n\nDeploy\n#\n\n\nFinally, run that deploy as a service or a task. (the service will restart every 12 hours)\n\n\n  l0 service create demo timeoutsvc timeout:latest\n\n\n\n\n\nReferences\n#\n\n\n\n\nECR User Guide\n\n\ncreate-repository\n\n\nget-login", 
            "title": "ECR"
        }, 
        {
            "location": "/reference/ecr/#ec2-container-registry", 
            "text": "ECR is an Amazon implementation of a docker registry.  It acts as a private registry in your AWS account, which can be accessed from any docker client, and Layer0.  Consider using ECR if you have stability issues with hosted docker registries, and do not wish to share your images publicly on  dockerhub .", 
            "title": "EC2 Container Registry"
        }, 
        {
            "location": "/reference/ecr/#setup", 
            "text": "When interacting with ECR, you will first need to create a repository and a login to interact from your development machine.", 
            "title": "Setup"
        }, 
        {
            "location": "/reference/ecr/#repository", 
            "text": "Each repository needs to be created by an AWS api call.      aws ecr create-repository --repository-name myteam/myproject", 
            "title": "Repository"
        }, 
        {
            "location": "/reference/ecr/#login", 
            "text": "To authenticate with the ECR service, Amazon provides the  get-login  command, which generates an authentication token, and returns a docker command to set it up      aws ecr get-login\n  # this command will return the following: (password is typically hundreds of characters)\n  docker login -u AWS -p password -e none https://aws_account_id.dkr.ecr.us-east-1.amazonaws.com  Execute the provided docker command to store the login credentials  Afterward creating the repository and local login credentials you may interact with images (and tags) under this path from a local docker client.    docker pull  ${ ecr - url } /myteam/myproject\n  docker push  ${ ecr - url } /myteam/myproject:custom-tag-1", 
            "title": "Login"
        }, 
        {
            "location": "/reference/ecr/#deploy-example", 
            "text": "Here we'll walk through using ECR when deploying to Layer0,  Using a very basic wait container.", 
            "title": "Deploy Example"
        }, 
        {
            "location": "/reference/ecr/#make-docker-image", 
            "text": "Your docker image can be built locally or pulled from dockerhub.  For this example, we made a service that waits and then exits (useful for triggering regular restarts).  FROM busybox\n\nENV SLEEP_TIME=60\n\nCMD sleep $SLEEP_TIME  Then build the file, with the tag  xfra/wait     docker build -f Dockerfile.wait -t xfra/wait .", 
            "title": "Make docker image"
        }, 
        {
            "location": "/reference/ecr/#upload-to-ecr", 
            "text": "After preparing a login and registry, tag the image with the remote url, and use  docker push    docker tag xfra/wait 111222333444.dkr.ecr.us-east-1.amazonaws.com/xfra-wait\n  docker push 111222333444.dkr.ecr.us-east-1.amazonaws.com/xfra-wait   Note: your account id in this url will be different.", 
            "title": "Upload to ECR"
        }, 
        {
            "location": "/reference/ecr/#create-a-deploy", 
            "text": "To run this image in Layer0, we create a dockerrun file, describing the instance and any additional variables  {\n   containerDefinitions : [\n    {\n       name :  timeout ,\n       image :  111222333444.dkr.ecr.us-east-1.amazonaws.com/xfra-wait:latest ,\n       essential : true,\n       memory : 10,\n       environment : [\n        {  name :  SLEEP_TIME ,  value :  43200  }\n      ]\n    }\n  ]\n}  And create that in Layer0    l0 deploy create timeout.dockerrun.aws.json timeout", 
            "title": "Create a deploy"
        }, 
        {
            "location": "/reference/ecr/#deploy", 
            "text": "Finally, run that deploy as a service or a task. (the service will restart every 12 hours)    l0 service create demo timeoutsvc timeout:latest", 
            "title": "Deploy"
        }, 
        {
            "location": "/reference/ecr/#references", 
            "text": "ECR User Guide  create-repository  get-login", 
            "title": "References"
        }, 
        {
            "location": "/troubleshooting/commonissues/", 
            "text": "Common issues and their solutions\n#\n\n\n\"Connection refused\" error when executing Layer0 commands\n#\n\n\nWhen executing commands using the Layer0 CLI, you may see the following error message: \"Get http://localhost:9090/\ncommand\n/: dial tcp 127.0.0.1:9090: connection refused\", where \ncommand\n is the Layer0 command you are trying to execute.\n\n\nThis error indicates that your Layer0 environment variables have not been set for the current session. See the \n\"Configure environment variables\" section\n of the Layer0 installation guide for instructions for setting up your environment variables.\n\n\n\n\n\"Invalid Dockerrun.aws.json\" error when creating a deploy\n#\n\n\nByte Order Marks (BOM) in Dockerrun file\n#\n\n\nIf your Dockerrun.aws.json file contains a Byte Order Marker, you may receive an \"Invalid Dockerrun.aws.json\" error when creating a deploy. If you create or edit the Dockerrun file using Visual Studio, and you have not modified the file encoding settings in Visual Studio, you are likely to encounter this error.\n\n\nTo remove the BOM:\n\n\n\n\n\n\nAt the command line, type the following to remove the BOM:\n\n\n\n\n(Linux/OS X) \ntail -c +4\n \nDockerrunFile\n \n \nDockerrunFileNew\n\n\nReplace \nDockerrunFile\n with the path to your Dockerrun file, and \nDockerrunFileNew\n with a new name for the Dockerrun file without the BOM.\n\n\n\n\n\n\n\n\nAlternatively, you can use the \ndos2unix file converter\n to remove the BOM from your Dockerrun files. Dos2unix is available for Windows, Linux and Mac OS.\n\n\nTo remove the BOM using dos2unix:\n\n\n\n\n\n\nAt the command line, type the following:\n\n\n\n\ndos2unix --remove-bom -n\n \nDockerrunFile\n \nDockerrunFileNew\n\n\nReplace \nDockerrunFile\n with the path to your Dockerrun file, and \nDockerrunFileNew\n with a new name for the Dockerrun file without the BOM.\n\n\n\n\n\n\n\n\n\n\n\"AWS Error: the key pair '\n' does not exist (code 'ValidationError')\" with l0-setup\n#\n\n\nThis occurs when you pass a non-existent EC2 keypair to l0-setup. To fix this, follow the instructions for \ncreating an EC2 Key Pair\n.\n\n\n\n\nAfter you've created a new EC2 Key Pair, run the following command:\n\n\n  \nl0-setup plan\n \nprefix\n \n-var key_pair\n=\nkeypair", 
            "title": "Common Issues"
        }, 
        {
            "location": "/troubleshooting/commonissues/#common-issues-and-their-solutions", 
            "text": "", 
            "title": "Common issues and their solutions"
        }, 
        {
            "location": "/troubleshooting/commonissues/#connection-refused-error-when-executing-layer0-commands", 
            "text": "When executing commands using the Layer0 CLI, you may see the following error message: \"Get http://localhost:9090/ command /: dial tcp 127.0.0.1:9090: connection refused\", where  command  is the Layer0 command you are trying to execute.  This error indicates that your Layer0 environment variables have not been set for the current session. See the  \"Configure environment variables\" section  of the Layer0 installation guide for instructions for setting up your environment variables.", 
            "title": "\"Connection refused\" error when executing Layer0 commands"
        }, 
        {
            "location": "/troubleshooting/commonissues/#invalid-dockerrunawsjson-error-when-creating-a-deploy", 
            "text": "", 
            "title": "\"Invalid Dockerrun.aws.json\" error when creating a deploy"
        }, 
        {
            "location": "/troubleshooting/commonissues/#byte-order-marks-bom-in-dockerrun-file", 
            "text": "If your Dockerrun.aws.json file contains a Byte Order Marker, you may receive an \"Invalid Dockerrun.aws.json\" error when creating a deploy. If you create or edit the Dockerrun file using Visual Studio, and you have not modified the file encoding settings in Visual Studio, you are likely to encounter this error.  To remove the BOM:    At the command line, type the following to remove the BOM:   (Linux/OS X)  tail -c +4   DockerrunFile     DockerrunFileNew  Replace  DockerrunFile  with the path to your Dockerrun file, and  DockerrunFileNew  with a new name for the Dockerrun file without the BOM.     Alternatively, you can use the  dos2unix file converter  to remove the BOM from your Dockerrun files. Dos2unix is available for Windows, Linux and Mac OS.  To remove the BOM using dos2unix:    At the command line, type the following:   dos2unix --remove-bom -n   DockerrunFile   DockerrunFileNew  Replace  DockerrunFile  with the path to your Dockerrun file, and  DockerrunFileNew  with a new name for the Dockerrun file without the BOM.", 
            "title": "Byte Order Marks (BOM) in Dockerrun file"
        }, 
        {
            "location": "/troubleshooting/commonissues/#aws-error-the-key-pair-does-not-exist-code-validationerror-with-l0-setup", 
            "text": "This occurs when you pass a non-existent EC2 keypair to l0-setup. To fix this, follow the instructions for  creating an EC2 Key Pair .   After you've created a new EC2 Key Pair, run the following command: \n   l0-setup plan   prefix   -var key_pair = keypair", 
            "title": "\"AWS Error: the key pair '' does not exist (code 'ValidationError')\" with l0-setup"
        }, 
        {
            "location": "/troubleshooting/ssh/", 
            "text": "Secure Shell (SSH)\n#\n\n\nYou can use Secure Shell (SSH) to access your Layer0 environment(s).\n\n\nBy default, Layer0 Setup asks for an EC2 key pair when creating a new Layer0. This key pair is associated with all machines that host your Layer0 Services. This means you can use SSH to log into the underlying Docker host to perform tasks such as troubleshooting failing containers or viewing logs. For information about creating an EC2 key pair, see \nInstall and Configure Layer0\n.\n\n\n\n\nWarning\n\n\nThis section is recommended for development debugging only.\nIt is \nnot\n recommended for production environments.\n\n\n\n\nTo SSH into a Service\n#\n\n\n\n\nIn a console window, add port 2222:22/tcp to your Service's load balancer:\n\n\n\n\nl0 loadbalancer addport \nname\n 2222:22/tcp\n\n\n\n\n\n\n  \nSSH into your Service by supplying the load balancer url and key pair file name.\n\n\n\n\n\nssh -i \nkey pair path and file name\n ec2-user@\nload balancer url\n -p 2222\n\n\n\n\n\n\n  \nIf required, Use Docker to access a specific container with Bash.\n\n\n\n\n\ndocker exec -it \ncontainer id\n /bin/bash\n\n\n\n\n\nRemarks\n#\n\n\nYou can get the load balancer url from the Load Balancers section of your Layer0 AWS console.\n\n\nUse the \nloadbalancer dropport\n subcommand to remove a port configuration from an existing Layer0 load balancer.\n\n\nYou \ncannot\n change the key pair after a Layer0 has been created. If you lose your key pair or need to generate a new one, you will need to create a new Layer0.\n\n\nIf your Service is behind a private load balancer, or none at all, you can either re-create your Service behind a public load balancer, use an existing public load balancer as a \"jump\" point, or create a new Layer0 Service behind a public load balancer to serve as a \"jump\" point.", 
            "title": "Secure Shell (SSH)"
        }, 
        {
            "location": "/troubleshooting/ssh/#secure-shell-ssh", 
            "text": "You can use Secure Shell (SSH) to access your Layer0 environment(s).  By default, Layer0 Setup asks for an EC2 key pair when creating a new Layer0. This key pair is associated with all machines that host your Layer0 Services. This means you can use SSH to log into the underlying Docker host to perform tasks such as troubleshooting failing containers or viewing logs. For information about creating an EC2 key pair, see  Install and Configure Layer0 .   Warning  This section is recommended for development debugging only.\nIt is  not  recommended for production environments.", 
            "title": "Secure Shell (SSH)"
        }, 
        {
            "location": "/troubleshooting/ssh/#to-ssh-into-a-service", 
            "text": "In a console window, add port 2222:22/tcp to your Service's load balancer:   l0 loadbalancer addport  name  2222:22/tcp  \n   SSH into your Service by supplying the load balancer url and key pair file name.   ssh -i  key pair path and file name  ec2-user@ load balancer url  -p 2222  \n   If required, Use Docker to access a specific container with Bash.   docker exec -it  container id  /bin/bash", 
            "title": "To SSH into a Service"
        }, 
        {
            "location": "/troubleshooting/ssh/#remarks", 
            "text": "You can get the load balancer url from the Load Balancers section of your Layer0 AWS console.  Use the  loadbalancer dropport  subcommand to remove a port configuration from an existing Layer0 load balancer.  You  cannot  change the key pair after a Layer0 has been created. If you lose your key pair or need to generate a new one, you will need to create a new Layer0.  If your Service is behind a private load balancer, or none at all, you can either re-create your Service behind a public load balancer, use an existing public load balancer as a \"jump\" point, or create a new Layer0 Service behind a public load balancer to serve as a \"jump\" point.", 
            "title": "Remarks"
        }
    ]
}