{
    "docs": [
        {
            "location": "/",
            "text": "Build, Manage, and Deploy Your Application\n#\n\n\n\n\nMeet Layer0\n#\n\n\nLayer0 is a framework that helps you deploy web applications to the cloud with minimal fuss. Using a simple command line interface (CLI), you can manage the entire life cycle of your application without having to focus on infrastructure.\n\n\nReady to learn more about Layer0? See our \nintroduction page\n to learn about some important concepts. When you're ready to get started, take a look at the \ninstallation page\n for information about setting up Layer0.\n\n\nDownload\n#\n\n\n\n\n\n\n\n\nDownload \nv0.10.4\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmacOS\n\n\nLinux\n\n\nWindows\n\n\n\n\n\n\n\n\nContact Us\n#\n\n\nIf you have questions about Layer0, email the development team at \ncarbon@us.imshealth.com\n.",
            "title": "Home"
        },
        {
            "location": "/#build-manage-and-deploy-your-application",
            "text": "",
            "title": "Build, Manage, and Deploy Your Application"
        },
        {
            "location": "/#meet-layer0",
            "text": "Layer0 is a framework that helps you deploy web applications to the cloud with minimal fuss. Using a simple command line interface (CLI), you can manage the entire life cycle of your application without having to focus on infrastructure.  Ready to learn more about Layer0? See our  introduction page  to learn about some important concepts. When you're ready to get started, take a look at the  installation page  for information about setting up Layer0.",
            "title": "Meet Layer0"
        },
        {
            "location": "/#download",
            "text": "Download  v0.10.4             macOS  Linux  Windows",
            "title": "Download"
        },
        {
            "location": "/#contact-us",
            "text": "If you have questions about Layer0, email the development team at  carbon@us.imshealth.com .",
            "title": "Contact Us"
        },
        {
            "location": "/releases/",
            "text": "Version\n\n\nmacOS\n\n\nLinux\n\n\nWindows\n\n\n\n\n\n\n\n\n\n\nv0.10.4\n\n\nmacOS\n\n\nLinux\n\n\nWindows\n\n\n\n\n\n\nv0.10.3\n\n\nmacOS\n\n\nLinux\n\n\nWindows\n\n\n\n\n\n\nv0.10.2\n\n\nmacOS\n\n\nLinux\n\n\nWindows\n\n\n\n\n\n\nv0.10.1\n\n\nmacOS\n\n\nLinux\n\n\nWindows\n\n\n\n\n\n\nv0.10.0\n\n\nmacOS\n\n\nLinux\n\n\nWindows\n\n\n\n\n\n\nv0.9.0\n\n\nmacOS\n\n\nLinux\n\n\nWindows\n\n\n\n\n\n\nv0.8.4\n\n\nmacOS\n\n\nLinux\n\n\nWindows",
            "title": "Releases"
        },
        {
            "location": "/intro/",
            "text": "Layer0 Introduction\n#\n\n\nIn recent years, the process of deploying applications has seen incredible innovation. However, this innovation has taken a somewhat simple task and made it into something quite \ncomplicated\n. Cloud providers, load balancing, virtual servers, IP subnets, and a continuing list of technological considerations are not only required to be understood, but their creation and management must be automated for a modern application to be successful at scale.\n\n\nThe burden of understanding a complicated and ever-growing infrastructure is a large aspect of what Layer0 is trying to fix. We've already done the leg work for huge swathes of your backend infrastructure, and we've made it easy to tear down and start over again, too. Meanwhile, you can develop locally using \nDocker\n and be assured that your application will properly translate to the cloud when you're ready to deploy.\n\n\nLayer0 requires a solid understanding of Docker to get the most out of it. We highly recommend starting with \nDocker's Understanding the Architecture\n to learn more about using Docker locally and in the cloud. We also recommend the \nTwelve-Factor App\n primer, which is a critical resource for understanding how to build a microservice.\n\n\n\n\nLayer0 Concepts\n#\n\n\nThe following concepts are core Layer0 abstractions for the technologies and features we use \nbehind the scenes\n. These terms will be used throughout our guides, so having a general understanding of them is helpful.\n\n\nCertificates\n#\n\n\nSSL certificates obtained from a valid \nCertificate Authority (CA)\n. You can use these certificates to secure your HTTPS services by applying them to your Layer0 load balancers.\n\n\nDeploys\n#\n\n\nECS Task Definitions\n. These configuration files detail how to deploy your application. We have several \nsample applications\n available that show what these files look like --- they're called \nDockerrun.aws.json\n within each sample app.\n\n\nTasks\n#\n\n\nManual one-off commands that don't necessarily make sense to keep running, or to restart when they finish. These run using Amazon's \nRunTask\n action (more info \nhere\n), and are \"ideally suited for processes such as batch jobs that perform work and then stop.\"\n\n\nLoad Balancers\n#\n\n\nPowerful tools that give you the basic building blocks for high-availability, scaling, and HTTPS. We currently use Amazon's \nElastic Load Balancing\n, and it pays to understand the basics of this service when working with Layer0.\n\n\nServices\n#\n\n\nYour running Layer0 applications. We also use the term \nservice\n for tools such as Consul, for which we provide a pre-built \nsample implementation\n using Layer0.\n\n\nEnvironments\n#\n\n\nLogical groupings of services. Typically, you would make a single environment for each tier of your application, such as \ndev\n, \nstaging\n, and \nprod\n.",
            "title": "Introduction"
        },
        {
            "location": "/intro/#layer0-introduction",
            "text": "In recent years, the process of deploying applications has seen incredible innovation. However, this innovation has taken a somewhat simple task and made it into something quite  complicated . Cloud providers, load balancing, virtual servers, IP subnets, and a continuing list of technological considerations are not only required to be understood, but their creation and management must be automated for a modern application to be successful at scale.  The burden of understanding a complicated and ever-growing infrastructure is a large aspect of what Layer0 is trying to fix. We've already done the leg work for huge swathes of your backend infrastructure, and we've made it easy to tear down and start over again, too. Meanwhile, you can develop locally using  Docker  and be assured that your application will properly translate to the cloud when you're ready to deploy.  Layer0 requires a solid understanding of Docker to get the most out of it. We highly recommend starting with  Docker's Understanding the Architecture  to learn more about using Docker locally and in the cloud. We also recommend the  Twelve-Factor App  primer, which is a critical resource for understanding how to build a microservice.",
            "title": "Layer0 Introduction"
        },
        {
            "location": "/intro/#layer0-concepts",
            "text": "The following concepts are core Layer0 abstractions for the technologies and features we use  behind the scenes . These terms will be used throughout our guides, so having a general understanding of them is helpful.",
            "title": "Layer0 Concepts"
        },
        {
            "location": "/intro/#certificates",
            "text": "SSL certificates obtained from a valid  Certificate Authority (CA) . You can use these certificates to secure your HTTPS services by applying them to your Layer0 load balancers.",
            "title": "Certificates"
        },
        {
            "location": "/intro/#deploys",
            "text": "ECS Task Definitions . These configuration files detail how to deploy your application. We have several  sample applications  available that show what these files look like --- they're called  Dockerrun.aws.json  within each sample app.",
            "title": "Deploys"
        },
        {
            "location": "/intro/#tasks",
            "text": "Manual one-off commands that don't necessarily make sense to keep running, or to restart when they finish. These run using Amazon's  RunTask  action (more info  here ), and are \"ideally suited for processes such as batch jobs that perform work and then stop.\"",
            "title": "Tasks"
        },
        {
            "location": "/intro/#load-balancers",
            "text": "Powerful tools that give you the basic building blocks for high-availability, scaling, and HTTPS. We currently use Amazon's  Elastic Load Balancing , and it pays to understand the basics of this service when working with Layer0.",
            "title": "Load Balancers"
        },
        {
            "location": "/intro/#services",
            "text": "Your running Layer0 applications. We also use the term  service  for tools such as Consul, for which we provide a pre-built  sample implementation  using Layer0.",
            "title": "Services"
        },
        {
            "location": "/intro/#environments",
            "text": "Logical groupings of services. Typically, you would make a single environment for each tier of your application, such as  dev ,  staging , and  prod .",
            "title": "Environments"
        },
        {
            "location": "/setup/install/",
            "text": "Create a new Layer0 Instance\n#\n\n\nPrerequisites\n#\n\n\nBefore you can install and configure Layer0, you must obtain the following:\n\n\n\n\n\n\nAccess to an AWS account\n\n\n\n\n\n\nAn EC2 Key Pair\n\nThis key pair allows you to access the EC2 instances running your Services using SSH.\nIf you have already created a key pair, you can use it for this process.\nOtherwise, \nfollow the AWS documentation\n to create a new key pair.\nMake a note of the name that you selected when creating the key pair.\n\n\n\n\n\n\nTerraform v0.11+\n\nWe use Terraform to create the resources that Layer0 needs.\nIf you're unfamiliar with Terraform, you may want to check out our \nintroduction\n.\nIf you're ready to install Terraform, there are instructions in the \nTerraform documentation\n.\n\n\n\n\n\n\nPart 1: Download and extract Layer0\n#\n\n\n\n\nIn the \nDownloads section of the home page\n, select the appropriate installation file for your operating system. Extract the zip file to a directory on your computer.\n\n\n(Optional) Place the \nl0\n and \nl0-setup\n binaries into your system path. \nFor more information about adding directories to your system path, see the following resources:\n\n\n(Windows): \nHow to Edit Your System PATH for Easy Command Line Access in Windows\n\n\n(Linux/macOS): \nAdding a Directory to the Path\n\n\n\n\n\n\n\n\nPart 2: Create an Access Key\n#\n\n\nThis step will create an Identity & Access Management (IAM) access key for your AWS account. \nYou will use the credentials created in this section when creating, updating, or removing Layer0 instances.\n\n\nTo create an Access Key:\n\n\n\n\n\n\nIn a web browser, login to the \nAWS Console\n.\n\n\n\n\n\n\nClick the \nServices\n dropdown menu in the upper left portion of the console page, then type \nIAM\n in the text box that appears at the top of the page after you click \nServices\n. As you type IAM, a search result will appear below the text box. Click on the IAM service result that appears below the text box.\n\n\n\n\n\n\nIn the left panel, click \nGroups\n, and then confirm that you have a group called \nAdministrators\n.\n\n\n\n\n\n\n\n\nIs the Administrators group missing in your AWS account?\n\n\nIf the \nAdministrators\n group does not already exist, complete the following steps:\n\n\n\n\n\n\nClick \nCreate New Group\n. Name the new group \nAdministrators\n, and then click \nNext Step\n.\n\n\n\n\n\n\nCheck the \nAdministratorAccess\n policy to attach the Administrator policy to your new group.\n\n\n\n\n\n\nClick \nNext Step\n, and then click \nCreate Group\n.\n\n\n\n\n\n\n\n\n\n\n\n\nIn the left panel, click \nUsers\n.\n\n\n\n\n\n\nClick the \nNew User\n button and enter a unique user name you will use for Layer0. This user name can be used for multiple Layer0 installations. Check the box next to \nProgrammatic access\n, and then click the \nNext: Permissions\n button.\n\n\n\n\n\n\nMake sure the \nAdd user to group\n button is highlighted. Find and check the box next to the group \nAdministrators\n. Click \nNext: Review\n button to continue. This will make your newly created user an administrator for your AWS account, so be sure to keep your security credentials safe!\n\n\n\n\n\n\nReview your choices and then click the \nCreate user\n button.\n\n\n\n\n\n\nOnce your user account has been created, click the \nDownload .csv\n button to save your access and secret key to a CSV file.\n\n\n\n\n\n\nPart 3: Create a new Layer0 Instance\n#\n\n\nNow that you have downloaded Layer0 and configured your AWS account, you can create your Layer0 instance.\nFrom a command prompt, run the following (replacing \n<instance_name>\n with a name for your Layer0 instance):\n\n\nl0-setup init <instance_name>\n\n\n\n\n\nThis command will prompt you for many different inputs. \nEnter the required values for \nAWS Access Key\n, \nAWS Secret Key\n, and \nAWS SSH Key\n as they come up.\nAll remaining inputs are optional and can be set to their default by pressing enter.\n\n\n...\nAWS Access Key: The access_key input variable is used to provision the AWS resources\nrequired for Layer0. This corresponds to the Access Key ID portion of an AWS Access Key.\nIt is recommended this key has the 'AdministratorAccess' policy. Note that Layer0 will\nonly use this key for 'l0-setup' commands associated with this Layer0 instance; the\nLayer0 API will use its own key with limited permissions to provision AWS resources.\n\n[current: <none>]\nPlease enter a value and press 'enter'.\n        Input: ABC123xzy\n\nAWS Secret Key: The secret_key input variable is used to provision the AWS resources\nrequired for Layer0. This corresponds to the Secret Access Key portion of an AWS Access Key.\nIt is recommended this key has the 'AdministratorAccess' policy. Note that Layer0 will\nonly use this key for 'l0-setup' commands associated with this Layer0 instance; the\nLayer0 API will use its own key with limited permissions to provision AWS resources.\n\n[current: <none>]\nPlease enter a value and press 'enter'.\n        Input: ZXY987cba\n\nAWS SSH Key Pair: The ssh_key_pair input variable specifies the name of the\nssh key pair to include in EC2 instances provisioned by Layer0. This key pair must\nalready exist in the AWS account. The names of existing key pairs can be found\nin the EC2 dashboard. Note that changing this value will not effect instances\nthat have already been provisioned.\n\n[current: <none>]\nPlease enter a value and press 'enter'.\n        Input: mySSHKey\n...\n\n\n\n\n\nOnce the \ninit\n command has successfully completed, you're ready to actually create the resources needed to use Layer0.\nRun the following command (again, replace \n<instance_name>\n with the name you've chosen for your Layer0 instance):\n\n\nl0-setup apply <instance_name>\n\n\n\n\n\nThe first time you run the \napply\n command, it may take around 5 minutes to complete. \nThis command is idempotent; it is safe to run multiple times if it fails the first.\n\n\nAt the end of the \napply\n command, your Layer0 instance's configuration and state will be automatically backed up to an S3 bucket. You can manually back up your configuration at any time using the \npush\n command. It's a good idea to run this command regularly (\nl0-setup push <instance_name>\n) to ensure that your configuration is backed up.\nThese files can be downloaded at any time using the \npull\n command (\nl0-setup pull <instance_name>\n).\n\n\n\n\nUsing a Private Docker Registry\n\n\nThe procedures in this section are optional, but are highly recommended for production use.\n\n\n\n\nIf you require authentication to a private Docker registry, you will need a Docker configuration file present on your machine with access to private repositories (typically located at \n~/.docker/config.json\n). \n\n\nIf you don't have a config file yet, you can generate one by running \ndocker login [registry-address]\n. \nA configuration file will be generated at \n~/.docker/config.json\n.\n\n\nTo add this authentication to your Layer0 instance, run:\n\n\nl0-setup init --docker-path=<path/to/config.json> <instance_name>\n\n\n\n\n\nThis will reconfigure your Layer0 configuration and add a rendered file into your Layer0 instance's directory at \n~/.layer0/<instance_name>/dockercfg.json\n.\n\n\nYou can modify a Layer0 instance's \ndockercfg.json\n file and re-run the \napply\n command (\nl0-setup apply <instance_name>\n) to make changes to your authentication. \n\nNote:\n Any EC2 instances created prior to changing your \ndockercfg.json\n file will need to be manually terminated since they only grab the authentication file during instance creation. \nTerminated EC2 instances will be automatically re-created by autoscaling.\n\n\n\n\nUsing an Existing VPC\n\n\nThe procedures in this section must be followed precisely to properly install Layer0 into an existing VPC\n\n\n\n\nBy default, \nl0-setup\n creates a new VPC to place resources. \nHowever, \nl0-setup\n can place resources in an existing VPC if the VPC meets all of the following conditions:\n\n\n\n\nHas access to the public internet (through a NAT instance or gateway)\n\n\nHas at least 1 public and 1 private subnet\n\n\nThe public and private subnets have the tag \nTier: Public\n or \nTier: Private\n, respectively.\nFor information on how to tag AWS resources, please visit the \nAWS documentation\n. \n\n\n\n\nOnce you are sure the existing VPC satisfies these requirements, run the \ninit\n command, \nplacing the VPC ID when prompted:\n\n\nl0-setup init <instance_name>\n...\nVPC ID (optional): The vpc_id input variable specifies an existing AWS VPC to provision\nthe AWS resources required for Layer0. If no input is specified, a new VPC will be\ncreated for you. Existing VPCs must satisfy the following constraints:\n\n    - Have access to the public internet (through a NAT instance or gateway)\n    - Have at least 1 public and 1 private subnet\n    - Each subnet must be tagged with [\"Tier\": \"Private\"] or [\"Tier\": \"Public\"]\n\nNote that changing this value will destroy and recreate any existing resources.\n\n[current: ]\nPlease enter a new value, or press 'enter' to keep the current value.\n        Input: vpc123\n\n\n\n\n\nOnce the command has completed, it is safe to run \napply\n to provision the resources. \n\n\nPart 4: Connect to a Layer0 Instance\n#\n\n\nOnce the \napply\n command has run successfully, you can configure the environment variables needed to connect to the Layer0 API using the \nendpoint\n command.\n\n\nl0-setup endpoint --insecure <instance_name>\nexport LAYER0_API_ENDPOINT=\"https://l0-instance_name-api-123456.us-west-2.elb.amazonaws.com\"\nexport LAYER0_AUTH_TOKEN=\"abcDEFG123\"\nexport LAYER0_SKIP_SSL_VERIFY=\"1\"\nexport LAYER0_SKIP_VERSION_VERIFY=\"1\"\n\n\n\n\n\n\n\nDanger\n\n\nThe \n--insecure\n flag shows configurations that bypass SSL and version verifications. \nThis is required as the Layer0 API created uses a self-signed SSL certificate by default.\nThese settings are \nnot\n recommended for production use!\n\n\n\n\nThe \nendpoint\n command supports a \n--syntax\n option, which can be used to turn configuration into a single line:\n\n\n\n\nBash (default) - \neval \"$(l0-setup endpoint --insecure <instance_name>)\"\n\n\nPowershell - \nl0-setup endpoint --insecure --syntax=powershell <instance_name> | Out-String | Invoke-Expression",
            "title": "Install"
        },
        {
            "location": "/setup/install/#create-a-new-layer0-instance",
            "text": "",
            "title": "Create a new Layer0 Instance"
        },
        {
            "location": "/setup/install/#prerequisites",
            "text": "Before you can install and configure Layer0, you must obtain the following:    Access to an AWS account    An EC2 Key Pair \nThis key pair allows you to access the EC2 instances running your Services using SSH.\nIf you have already created a key pair, you can use it for this process.\nOtherwise,  follow the AWS documentation  to create a new key pair.\nMake a note of the name that you selected when creating the key pair.    Terraform v0.11+ \nWe use Terraform to create the resources that Layer0 needs.\nIf you're unfamiliar with Terraform, you may want to check out our  introduction .\nIf you're ready to install Terraform, there are instructions in the  Terraform documentation .",
            "title": "Prerequisites"
        },
        {
            "location": "/setup/install/#part-1-download-and-extract-layer0",
            "text": "In the  Downloads section of the home page , select the appropriate installation file for your operating system. Extract the zip file to a directory on your computer.  (Optional) Place the  l0  and  l0-setup  binaries into your system path. \nFor more information about adding directories to your system path, see the following resources:  (Windows):  How to Edit Your System PATH for Easy Command Line Access in Windows  (Linux/macOS):  Adding a Directory to the Path",
            "title": "Part 1: Download and extract Layer0"
        },
        {
            "location": "/setup/install/#part-2-create-an-access-key",
            "text": "This step will create an Identity & Access Management (IAM) access key for your AWS account. \nYou will use the credentials created in this section when creating, updating, or removing Layer0 instances.  To create an Access Key:    In a web browser, login to the  AWS Console .    Click the  Services  dropdown menu in the upper left portion of the console page, then type  IAM  in the text box that appears at the top of the page after you click  Services . As you type IAM, a search result will appear below the text box. Click on the IAM service result that appears below the text box.    In the left panel, click  Groups , and then confirm that you have a group called  Administrators .     Is the Administrators group missing in your AWS account?  If the  Administrators  group does not already exist, complete the following steps:    Click  Create New Group . Name the new group  Administrators , and then click  Next Step .    Check the  AdministratorAccess  policy to attach the Administrator policy to your new group.    Click  Next Step , and then click  Create Group .       In the left panel, click  Users .    Click the  New User  button and enter a unique user name you will use for Layer0. This user name can be used for multiple Layer0 installations. Check the box next to  Programmatic access , and then click the  Next: Permissions  button.    Make sure the  Add user to group  button is highlighted. Find and check the box next to the group  Administrators . Click  Next: Review  button to continue. This will make your newly created user an administrator for your AWS account, so be sure to keep your security credentials safe!    Review your choices and then click the  Create user  button.    Once your user account has been created, click the  Download .csv  button to save your access and secret key to a CSV file.",
            "title": "Part 2: Create an Access Key"
        },
        {
            "location": "/setup/install/#part-3-create-a-new-layer0-instance",
            "text": "Now that you have downloaded Layer0 and configured your AWS account, you can create your Layer0 instance.\nFrom a command prompt, run the following (replacing  <instance_name>  with a name for your Layer0 instance):  l0-setup init <instance_name>  This command will prompt you for many different inputs. \nEnter the required values for  AWS Access Key ,  AWS Secret Key , and  AWS SSH Key  as they come up.\nAll remaining inputs are optional and can be set to their default by pressing enter.  ...\nAWS Access Key: The access_key input variable is used to provision the AWS resources\nrequired for Layer0. This corresponds to the Access Key ID portion of an AWS Access Key.\nIt is recommended this key has the 'AdministratorAccess' policy. Note that Layer0 will\nonly use this key for 'l0-setup' commands associated with this Layer0 instance; the\nLayer0 API will use its own key with limited permissions to provision AWS resources.\n\n[current: <none>]\nPlease enter a value and press 'enter'.\n        Input: ABC123xzy\n\nAWS Secret Key: The secret_key input variable is used to provision the AWS resources\nrequired for Layer0. This corresponds to the Secret Access Key portion of an AWS Access Key.\nIt is recommended this key has the 'AdministratorAccess' policy. Note that Layer0 will\nonly use this key for 'l0-setup' commands associated with this Layer0 instance; the\nLayer0 API will use its own key with limited permissions to provision AWS resources.\n\n[current: <none>]\nPlease enter a value and press 'enter'.\n        Input: ZXY987cba\n\nAWS SSH Key Pair: The ssh_key_pair input variable specifies the name of the\nssh key pair to include in EC2 instances provisioned by Layer0. This key pair must\nalready exist in the AWS account. The names of existing key pairs can be found\nin the EC2 dashboard. Note that changing this value will not effect instances\nthat have already been provisioned.\n\n[current: <none>]\nPlease enter a value and press 'enter'.\n        Input: mySSHKey\n...  Once the  init  command has successfully completed, you're ready to actually create the resources needed to use Layer0.\nRun the following command (again, replace  <instance_name>  with the name you've chosen for your Layer0 instance):  l0-setup apply <instance_name>  The first time you run the  apply  command, it may take around 5 minutes to complete. \nThis command is idempotent; it is safe to run multiple times if it fails the first.  At the end of the  apply  command, your Layer0 instance's configuration and state will be automatically backed up to an S3 bucket. You can manually back up your configuration at any time using the  push  command. It's a good idea to run this command regularly ( l0-setup push <instance_name> ) to ensure that your configuration is backed up.\nThese files can be downloaded at any time using the  pull  command ( l0-setup pull <instance_name> ).   Using a Private Docker Registry  The procedures in this section are optional, but are highly recommended for production use.   If you require authentication to a private Docker registry, you will need a Docker configuration file present on your machine with access to private repositories (typically located at  ~/.docker/config.json ).   If you don't have a config file yet, you can generate one by running  docker login [registry-address] . \nA configuration file will be generated at  ~/.docker/config.json .  To add this authentication to your Layer0 instance, run:  l0-setup init --docker-path=<path/to/config.json> <instance_name>  This will reconfigure your Layer0 configuration and add a rendered file into your Layer0 instance's directory at  ~/.layer0/<instance_name>/dockercfg.json .  You can modify a Layer0 instance's  dockercfg.json  file and re-run the  apply  command ( l0-setup apply <instance_name> ) to make changes to your authentication.  Note:  Any EC2 instances created prior to changing your  dockercfg.json  file will need to be manually terminated since they only grab the authentication file during instance creation. \nTerminated EC2 instances will be automatically re-created by autoscaling.   Using an Existing VPC  The procedures in this section must be followed precisely to properly install Layer0 into an existing VPC   By default,  l0-setup  creates a new VPC to place resources. \nHowever,  l0-setup  can place resources in an existing VPC if the VPC meets all of the following conditions:   Has access to the public internet (through a NAT instance or gateway)  Has at least 1 public and 1 private subnet  The public and private subnets have the tag  Tier: Public  or  Tier: Private , respectively.\nFor information on how to tag AWS resources, please visit the  AWS documentation .    Once you are sure the existing VPC satisfies these requirements, run the  init  command, \nplacing the VPC ID when prompted:  l0-setup init <instance_name>\n...\nVPC ID (optional): The vpc_id input variable specifies an existing AWS VPC to provision\nthe AWS resources required for Layer0. If no input is specified, a new VPC will be\ncreated for you. Existing VPCs must satisfy the following constraints:\n\n    - Have access to the public internet (through a NAT instance or gateway)\n    - Have at least 1 public and 1 private subnet\n    - Each subnet must be tagged with [\"Tier\": \"Private\"] or [\"Tier\": \"Public\"]\n\nNote that changing this value will destroy and recreate any existing resources.\n\n[current: ]\nPlease enter a new value, or press 'enter' to keep the current value.\n        Input: vpc123  Once the command has completed, it is safe to run  apply  to provision the resources.",
            "title": "Part 3: Create a new Layer0 Instance"
        },
        {
            "location": "/setup/install/#part-4-connect-to-a-layer0-instance",
            "text": "Once the  apply  command has run successfully, you can configure the environment variables needed to connect to the Layer0 API using the  endpoint  command.  l0-setup endpoint --insecure <instance_name>\nexport LAYER0_API_ENDPOINT=\"https://l0-instance_name-api-123456.us-west-2.elb.amazonaws.com\"\nexport LAYER0_AUTH_TOKEN=\"abcDEFG123\"\nexport LAYER0_SKIP_SSL_VERIFY=\"1\"\nexport LAYER0_SKIP_VERSION_VERIFY=\"1\"   Danger  The  --insecure  flag shows configurations that bypass SSL and version verifications. \nThis is required as the Layer0 API created uses a self-signed SSL certificate by default.\nThese settings are  not  recommended for production use!   The  endpoint  command supports a  --syntax  option, which can be used to turn configuration into a single line:   Bash (default) -  eval \"$(l0-setup endpoint --insecure <instance_name>)\"  Powershell -  l0-setup endpoint --insecure --syntax=powershell <instance_name> | Out-String | Invoke-Expression",
            "title": "Part 4: Connect to a Layer0 Instance"
        },
        {
            "location": "/setup/upgrade/",
            "text": "Upgrade a Layer0 Instance\n#\n\n\nThis section provides procedures for upgrading your Layer0 installation to the latest version.\nThis assumes you are using Layer0 version \nv0.10.0\n or later. \n\n\n\n\nWarning\n\n\nLayer0 does not support updating MAJOR or MINOR versions in place unless explicitly stated otherwise.\nUsers will either need to create a new Layer0 instance and migrate to it or destroy and re-create their Layer0 instance in these circumstances.\n\n\n\n\nRun the \nupgrade\n command, replacing \n<instance_name>\n and \n<version>\n with the name of the Layer0 instance and new version, respectively:\n\n\nl0-setup upgrade <instance_name> <version>\n\n\n\n\n\nThis will prompt you about the updated \nsource\n and \nversion\n inputs changing. \nIf you are not satisfied with the changes, exit the application during the prompts. \nFor full control on changing inputs, use the \nset\n command. \n\n\nExample Usage\n\n\nl0-setup upgrade mylayer0 v0.10.1\n\nThis will update the 'version' input\n        From: [v0.10.0]\n        To:   [v0.10.1]\n\n        Press 'enter' to accept this change:\nThis will update the 'source' input\n        From: [github.com/quintilesims/layer0//setup/module?ref=v0.10.0]\n        To:   [github.com/quintilesims/layer0//setup/module?ref=v0.10.1]\n\n        Press 'enter' to accept this change:\n        ...\n\nEverything looks good! You are now ready to run 'l0-setup apply mylayer0'\n\n\n\n\n\nAs stated by the command output, run the \napply\n command to apply the changes to the Layer0 instance.\nIf any errors occur, please contact the Layer0 team.",
            "title": "Upgrade"
        },
        {
            "location": "/setup/upgrade/#upgrade-a-layer0-instance",
            "text": "This section provides procedures for upgrading your Layer0 installation to the latest version.\nThis assumes you are using Layer0 version  v0.10.0  or later.    Warning  Layer0 does not support updating MAJOR or MINOR versions in place unless explicitly stated otherwise.\nUsers will either need to create a new Layer0 instance and migrate to it or destroy and re-create their Layer0 instance in these circumstances.   Run the  upgrade  command, replacing  <instance_name>  and  <version>  with the name of the Layer0 instance and new version, respectively:  l0-setup upgrade <instance_name> <version>  This will prompt you about the updated  source  and  version  inputs changing. \nIf you are not satisfied with the changes, exit the application during the prompts. \nFor full control on changing inputs, use the  set  command.   Example Usage  l0-setup upgrade mylayer0 v0.10.1\n\nThis will update the 'version' input\n        From: [v0.10.0]\n        To:   [v0.10.1]\n\n        Press 'enter' to accept this change:\nThis will update the 'source' input\n        From: [github.com/quintilesims/layer0//setup/module?ref=v0.10.0]\n        To:   [github.com/quintilesims/layer0//setup/module?ref=v0.10.1]\n\n        Press 'enter' to accept this change:\n        ...\n\nEverything looks good! You are now ready to run 'l0-setup apply mylayer0'  As stated by the command output, run the  apply  command to apply the changes to the Layer0 instance.\nIf any errors occur, please contact the Layer0 team.",
            "title": "Upgrade a Layer0 Instance"
        },
        {
            "location": "/setup/destroy/",
            "text": "Destroying a Layer0 Instance\n#\n\n\nThis section provides procedures for destroying (deleting) a Layer0 instance.\n\n\nPart 1: Clean Up Your Layer0 Environments\n#\n\n\nIn order to destroy a Layer0 instance, you must first delete all environments in the instance.\nList all environments with:\n\n\nl0 environment list\n\n\n\n\n\nFor each environment listed in the previous step, with the exception of the environment named \napi\n, \nissue the following command (replacing \n<environment_name>\n with the name of the environment to delete):\n\n\nl0 environment delete --wait <environment_name>\n\n\n\n\n\nPart 2: Destroy the Layer0 Instance\n#\n\n\nOnce all environments have been deleted, the Layer0 instance can be deleted using the \nl0-setup\n tool. \nRun the following command (replacing \n<instance_name>\n with the name of the Layer0 instance):\n\n\nl0-setup destroy <instance_name>\n\n\n\n\n\nThe \ndestroy\n command is idempotent; if it fails, it is safe to re-attempt multiple times. \n\n\n\n\nNote\n\n\nIf the operation continues to fail, it is likely there are resources that were created outside of Layer0 that have dependencies on the resources \nl0-setup\n is attempting to destroy. You will need to manually remove these dependencies in order to get the \ndestroy\n command to complete successfully.",
            "title": "Destroy"
        },
        {
            "location": "/setup/destroy/#destroying-a-layer0-instance",
            "text": "This section provides procedures for destroying (deleting) a Layer0 instance.",
            "title": "Destroying a Layer0 Instance"
        },
        {
            "location": "/setup/destroy/#part-1-clean-up-your-layer0-environments",
            "text": "In order to destroy a Layer0 instance, you must first delete all environments in the instance.\nList all environments with:  l0 environment list  For each environment listed in the previous step, with the exception of the environment named  api , \nissue the following command (replacing  <environment_name>  with the name of the environment to delete):  l0 environment delete --wait <environment_name>",
            "title": "Part 1: Clean Up Your Layer0 Environments"
        },
        {
            "location": "/setup/destroy/#part-2-destroy-the-layer0-instance",
            "text": "Once all environments have been deleted, the Layer0 instance can be deleted using the  l0-setup  tool. \nRun the following command (replacing  <instance_name>  with the name of the Layer0 instance):  l0-setup destroy <instance_name>  The  destroy  command is idempotent; if it fails, it is safe to re-attempt multiple times.    Note  If the operation continues to fail, it is likely there are resources that were created outside of Layer0 that have dependencies on the resources  l0-setup  is attempting to destroy. You will need to manually remove these dependencies in order to get the  destroy  command to complete successfully.",
            "title": "Part 2: Destroy the Layer0 Instance"
        },
        {
            "location": "/guides/walkthrough/intro/",
            "text": "An Iterative Walkthrough\n#\n\n\nThis guide aims to take you through two increasingly-complex deployment examples using Layer0.\nSuccessive sections build upon the previous ones, and each deployment can be completed either through the Layer0 CLI directly, or through Terraform using our custom \nLayer0 Terraform Provider\n.\n\n\nWe assume that you're using Layer0 v0.9.0 or later.\nIf you have not already installed and configured Layer0, see the \ninstallation guide\n.\nIf you are running an older version of Layer0, you may need to \nupgrade\n.\n\n\nIf you intend to deploy services using the Layer0 Terraform Provider, you'll want to make sure that you've \ninstalled\n the provider correctly.\n\n\nRegardless of the deployment method you choose, we maintain a \nguides repository\n that you should clone/download.\nIt contains all the files you will need to progress through this walkthrough.\nAs you do so, we will assume that your working directory matches the part of the guide that you're following (for example, Deployment 1 of this guide will assume that your working directory is \n.../walkthrough/deployment-1/\n).\n\n\nTable of Contents\n:\n\n\n\n\nDeployment 1\n: Deploying a web service (Guestbook)\n\n\nDeployment 2\n: Deploying Guestbook and a data store service (Redis)",
            "title": "Walkthrough: Introduction"
        },
        {
            "location": "/guides/walkthrough/intro/#an-iterative-walkthrough",
            "text": "This guide aims to take you through two increasingly-complex deployment examples using Layer0.\nSuccessive sections build upon the previous ones, and each deployment can be completed either through the Layer0 CLI directly, or through Terraform using our custom  Layer0 Terraform Provider .  We assume that you're using Layer0 v0.9.0 or later.\nIf you have not already installed and configured Layer0, see the  installation guide .\nIf you are running an older version of Layer0, you may need to  upgrade .  If you intend to deploy services using the Layer0 Terraform Provider, you'll want to make sure that you've  installed  the provider correctly.  Regardless of the deployment method you choose, we maintain a  guides repository  that you should clone/download.\nIt contains all the files you will need to progress through this walkthrough.\nAs you do so, we will assume that your working directory matches the part of the guide that you're following (for example, Deployment 1 of this guide will assume that your working directory is  .../walkthrough/deployment-1/ ).  Table of Contents :   Deployment 1 : Deploying a web service (Guestbook)  Deployment 2 : Deploying Guestbook and a data store service (Redis)",
            "title": "An Iterative Walkthrough"
        },
        {
            "location": "/guides/walkthrough/deployment-1/",
            "text": "Deployment 1: A Simple Guestbook App\n#\n\n\nIn this section you'll learn how different Layer0 commands work together to deploy applications to the cloud.\nThe example application in this section is a guestbook -- a web application that acts as a simple message board.\nYou can choose to complete this section using either \nthe Layer0 CLI\n or \nTerraform\n.\n\n\n\n\nDeploy with Layer0 CLI\n#\n\n\nIf you're following along, you'll want to be working in the \nwalkthrough/deployment-1/\n directory of your clone of the \nguides\n repo.\n\n\nFiles used in this deployment:\n\n\n\n\n\n\n\n\nFilename\n\n\nPurpose\n\n\n\n\n\n\n\n\n\n\nGuestbook.Dockerrun.aws.json\n\n\nTemplate for running the Guestbook application\n\n\n\n\n\n\n\n\n\n\nPart 1: Create the Environment\n#\n\n\nThe first step in deploying an application with Layer0 is to create an environment.\nAn environment is a dedicated space in which one or more services can reside.\nHere, we'll create a new environment named \ndemo-env\n.\nAt the command prompt, execute the following:\n\n\nl0 environment create demo-env\n\n\nWe should see output like the following:\n\n\nENVIRONMENT ID  ENVIRONMENT NAME  CLUSTER COUNT  INSTANCE SIZE  LINKS\ndemo00e6aa9     demo-env          0              m3.medium\n\n\n\n\n\nWe can inspect our environments in a couple of different ways:\n\n\n\n\nl0 environment list\n will give us a brief summary of all environments:\n\n\n\n\nENVIRONMENT ID  ENVIRONMENT NAME\ndemo00e6aa9     demo-env\napi             api\n\n\n\n\n\n\n\nl0 environment get demo-env\n will show us more information about the \ndemo-env\n environment we just created:\n\n\n\n\nENVIRONMENT ID  ENVIRONMENT NAME  CLUSTER COUNT  INSTANCE SIZE  LINKS\ndemo00e6aa9     demo-env          0              m3.medium\n\n\n\n\n\n\n\nl0 environment get \\*\n illustrates wildcard matching (you could also have used \ndemo*\n in the above command), and it will return detailed information for \neach\n environment, not just one - it's like a detailed \nlist\n:\n\n\n\n\nENVIRONMENT ID  ENVIRONMENT NAME  CLUSTER COUNT  INSTANCE SIZE  LINKS\ndemo00e6aa9     demo-env          0              m3.medium\napi             api               2              m3.medium\n\n\n\n\n\n\n\nPart 2: Create the Load Balancer\n#\n\n\nIn order to expose a web application to the public internet, we need to create a load balancer.\nA load balancer listens for web traffic at a specific address and directs that traffic to a Layer0 service.\n\n\nA load balancer also has a notion of a health check - a way to assess whether or not the service is healthy and running properly.\nBy default, Layer0 configures the health check of a load balancer based upon a simple TCP ping to port 80 every thirty seconds.\nAlso by default, this ping will timeout after five seconds of no response from the service, and two consecutive successes or failures are required for the service to be considered healthy or unhealthy.\n\n\nHere, we'll create a new load balancer named \nguestbook-lb\n inside of our environment named \ndemo-env\n.\nThe load balancer will listen on port 80, and forward that traffic along to port 80 in the Docker container using the HTTP protocol.\nSince the port configuration is already aligned with the default health check, we don't need to specify any health check configuration when we create this load balancer.\nAt the command prompt, execute the following:\n\n\nl0 loadbalancer create --port 80:80/http demo-env guestbook-lb\n\n\nWe should see output like the following:\n\n\nLOADBALANCER ID  LOADBALANCER NAME  ENVIRONMENT  SERVICE  PORTS       PUBLIC  URL\nguestbodb65a     guestbook-lb       demo-env              80:80/HTTP  true\n\n\n\n\n\nThe following is a summary of the arguments passed in the above command:\n\n\n\n\nloadbalancer create\n: creates a new load balancer\n\n\n--port 80:80/HTTP\n: instructs the load balancer to forward requests from port 80 on the load balancer to port 80 in the EC2 instance using the HTTP protocol\n\n\ndemo-env\n: the name of the environment in which you are creating the load balancer\n\n\nguestbook-lb\n: a name for the load balancer itself\n\n\n\n\nYou can inspect load balancers in the same way that you inspected environments in Part 1.\nTry running the following commands to get an idea of the information available to you:\n\n\n\n\nl0 loadbalancer list\n\n\nl0 loadbalancer get guestbook-lb\n\n\nl0 loadbalancer get gues*\n\n\nl0 loadbalancer get \\*\n\n\n\n\n\n\nNote\n\n\nNotice that the load balancer \nlist\n and \nget\n outputs list an \nENVIRONMENT\n field - if you ever have load balancers (or other Layer0 entities) with the same name but in different environments, you can target a specific load balancer by qualifying it with its environment name:\n\n\n`l0 loadbalancer get demo-env:guestbook-lb`\n\n\n\n\n\n\nPart 3: Deploy the ECS Task Definition\n#\n\n\nThe \ndeploy\n command is used to specify the ECS task definition that outlines a web application.\nA deploy, once created, can be applied to multiple services - even across different environments!\n\n\nHere, we'll create a new deploy called \nguestbook-dpl\n that refers to the \nGuestbook.Dockerrun.aws.json\n file found in the guides reposiory.\nAt the command prompt, execute the following:\n\n\nl0 deploy create Guestbook.Dockerrun.aws.json guestbook-dpl\n\n\nWe should see output like the following:\n\n\nDEPLOY ID        DEPLOY NAME    VERSION\nguestbook-dpl.1  guestbook-dpl  1\n\n\n\n\n\nThe following is a summary of the arguments passed in the above command:\n\n\n\n\ndeploy create\n: creates a new deployment and allows you to specify an ECS task definition\n\n\nGuestbook.Dockerrun.aws.json\n: the file name of the ECS task definition (use the full path of the file if it is not in your current working directory)\n\n\nguestbook-dpl\n: a name for the deploy, which you will use later when you create the service\n\n\n\n\n\n\nDeploy Versioning\n\n\nThe \nDEPLOY NAME\n and \nVERSION\n are combined to create a unique identifier for a deploy.\nIf you create additional deploys named \nguestbook-dpl\n, they will be assigned different version numbers.\n\n\nYou can always specify the latest version when targeting a deploy by using \n<deploy name>:latest\n -- for example, \nguestbook-dpl:latest\n.\n\n\n\n\nDeploys support the same methods of inspection as environments and load balancers:\n\n\n\n\nl0 deploy list\n\n\nl0 deploy get guestbook*\n\n\nl0 deploy get guestbook:1\n\n\nl0 deploy get guestbook:latest\n\n\nl0 deploy get \\*\n\n\n\n\n\n\nPart 4: Create the Service\n#\n\n\nThe final stage of the deployment process involves using the \nservice\n command to create a new service and associate it with the environment, load balancer, and deploy that we created in the previous sections.\nThe service will execute the Docker containers which have been described in the deploy.\n\n\nHere, we'll create a new service called \nguestbook-svc\n. At the command prompt, execute the following:\n\n\nl0 service create --loadbalancer demo-env:guestbook-lb demo-env guestbook-svc guestbook-dpl:latest\n\n\nWe should see output like the following:\n\n\nSERVICE ID    SERVICE NAME   ENVIRONMENT  LOADBALANCER  DEPLOYMENTS       SCALE\nguestbo9364b  guestbook-svc  demo-env     guestbook-lb  guestbook-dpl:1*  0/1\n\n\n\n\n\nThe following is a summary of the arguments passed in the above command:\n\n\n\n\nservice create\n: creates a new service\n\n\n--loadbalancer demo-env:guestbook-lb\n: the fully-qualified name of the load balancer; in this case, the load balancer named \nguestbook-lb\n in the environment named \ndemo-env\n. \n\n\n(It is not strictly necessary to use the fully qualified name of the load balancer, unless another load balancer with exactly the same name exists in a different environment.)\n\n\n\n\n\n\ndemo-env\n: the name of the environment you created in Part 1\n\n\nguestbook-svc\n: a name for the service you are creating\n\n\nguestbook-dpl\n: the name of the deploy that you created in Part 3\n\n\n\n\nLayer0 services can be queried using the same \nget\n and \nlist\n commands that we've come to expect by now.\n\n\n\n\nCheck the Status of the Service\n#\n\n\nAfter a service has been created, it may take several minutes for that service to completely finish deploying.\nA service's status may be checked by using the \nservice get\n command.\n\n\nLet's take a peek at our \nguestbook-svc\n service.\nAt the command prompt, execute the following:\n\n\nl0 service get demo-env:guestbook-svc\n\n\nIf we're quick enough, we'll be able to see the first stage of the process (this is what was output after running the \nservice create\n command up in Part 4).\nWe should see an asterisk (*) next to the name of the \nguestbook-dpl:1\n deploy, which indicates that the service is in a transitional state:\n\n\nSERVICE ID    SERVICE NAME   ENVIRONMENT  LOADBALANCER  DEPLOYMENTS       SCALE\nguestbo9364b  guestbook-svc  demo-env     guestbook-lb  guestbook-dpl:1*  0/1\n\n\n\n\n\nIn the next phase of deployment, if we execute the \nservice get\n command again, we will see \n(1)\n in the \nScale\n column; this indicates that 1 copy of the service is transitioning to an active state:\n\n\nSERVICE ID    SERVICE NAME   ENVIRONMENT  LOADBALANCER  DEPLOYMENTS       SCALE\nguestbo9364b  guestbook-svc  demo-env     guestbook-lb  guestbook-dpl:1*  0/1 (1)\n\n\n\n\n\nIn the final phase of deployment, we will see \n1/1\n in the \nScale\n column; this indicates that the service is running 1 copy:\n\n\nSERVICE ID    SERVICE NAME   ENVIRONMENT  LOADBALANCER  DEPLOYMENTS       SCALE\nguestbo9364b  guestbook-svc  demo-env     guestbook-lb  guestbook-dpl:1   1/1\n\n\n\n\n\n\n\nGet the Application's URL\n#\n\n\nOnce the service has been completely deployed, we can obtain the URL for the application and launch it in a browser.\n\n\nAt the command prompt, execute the following:\n\n\nl0 loadbalancer get demo-env:guestbook-lb\n\n\nWe should see output like the following:\n\n\nLOADBALANCER ID  LOADBALANCER NAME  ENVIRONMENT  SERVICE        PORTS       PUBLIC  URL\nguestbodb65a     guestbook-lb       demo-env     guestbook-svc  80:80/HTTP  true    <url>\n\n\n\n\n\nCopy the value shown in the \nURL\n column and paste it into a web browser.\nThe guestbook application will appear (once the service has completely finished deploying).\n\n\n\n\nLogs\n#\n\n\nOutput from a Service's docker containers may be acquired by running the following command:\n\n\nl0 service logs <SERVICE>\n\n\n\n\n\n\n\nCleanup\n#\n\n\nIf you're finished with the example and don't want to continue with this walkthrough, you can instruct Layer0 to delete the environment and terminate the application.\n\n\nl0 environment delete demo-env\n\n\nHowever, if you intend to continue through \nDeployment 2\n, you will want to keep the resources you made in this section.\n\n\n\n\nDeploy with Terraform\n#\n\n\nInstead of using the Layer0 CLI directly, you can instead use our Terraform provider, and deploy using Terraform \n(\nlearn more\n)\n.\nYou can use Terraform with Layer0 and AWS to create \"fire-and-forget\" deployments for your applications.\n\n\nIf you're following along, you'll want to be working in the \nwalkthrough/deployment-1/\n directory of your clone of the \nguides\n repo.\n\n\nWe use these files to set up a Layer0 environment with Terraform:\n\n\n\n\n\n\n\n\nFilename\n\n\nPurpose\n\n\n\n\n\n\n\n\n\n\nmain.tf\n\n\nProvisions resources; populates resources in template files\n\n\n\n\n\n\noutputs.tf\n\n\nValues that Terraform will yield during deployment\n\n\n\n\n\n\nterraform.tfstate\n\n\nTracks status of deployment \n(created and managed by Terraform)\n\n\n\n\n\n\nterraform.tfvars\n\n\nVariables specific to the environment and application(s)\n\n\n\n\n\n\nvariables.tf\n\n\nValues that Terraform will use during deployment\n\n\n\n\n\n\n\n\n*.tf\n: A Brief Aside\n#\n\n\nLet's take a moment to discuss the \n.tf\n files.\nThe names of these files (and even the fact that they are separated out into multiple files at all) are completely arbitrary and exist soley for human-readability.\nTerraform understands all \n.tf\n files in a directory all together.\n\n\nIn \nvariables.tf\n, you'll see \n\"endpoint\"\n and \n\"token\"\n variables.\n\n\nIn \noutputs.tf\n, you'll see that Terraform should spit out the url of the guestbook's load balancer once deployment has finished.\n\n\nIn \nmain.tf\n, you'll see the bulk of the deployment process.\nIf you've followed along with the Layer0 CLI deployment above, it should be fairly easy to see how blocks in this file map to steps in the CLI process.\nWhen we began the CLI deployment, our first step was to create an environment:\n\n\nl0 environment create demo-env\n\n\nThis command is recreated in \nmain.tf\n like so:\n\n\n# walkthrough/deployment-1/main.tf\n\nresource \"layer0_environment\" \"demo-env\" {\n    name = \"demo-env\"\n}\n\n\n\n\n\nWe've bundled up the heart of the Guestbook deployment (load balancer, deploy, service, etc.) into a \nTerraform module\n.\nTo use it, we declare a \nmodule\n block and pass in the source of the module as well as any configuration or variables that the module needs.\n\n\n# walkthrough/deployment-1/main.tf\n\nmodule \"guestbook\" {\n    source         = \"github.com/quintilesims/guides//guestbook/module\"\n    environment_id = \"\n${\nlayer0_environment\n.\ndemo\n.\nid\n}\n\"\n}\n\n\n\n\n\nYou can see that we pass in the ID of the environment we create.\nAll variables declared in this block are passed to the module, so the next file we should look at is \nvariables.tf\n inside of the module to get an idea of what the module is expecting.\n\n\nThere are a lot of variables here, but only one of them doesn't have a default value.\n\n\n# guestbook/module/variables.tf\n\nvariable \"environment_id\" {\n    description = \"id of the layer0 environment in which to create resources\"\n}\n\n\n\n\n\nYou'll notice that this is the variable that we're passing in.\nFor this particular deployment of the Guestbook, all of the default options are fine.\nWe could override any of them if we wanted to, just by specifying a new value for them back in \ndeployment-1/main.tf\n.\n\n\nNow that we've seen the variables that the module will have, let's take a look at part of \nmodule/main.tf\n and see how some of them might be used:\n\n\n# guestbook/module/main.tf\n\nresource \"layer0_load_balancer\" \"guestbook-lb\" {\n    name = \"\n${\nvar\n.\nload_balancer_name\n}\n\"\n    environment = \"\n${\nvar\n.\nenvironment_id\n}\n\"\n    port {\n        host_port = 80\n        container_port = 80\n        protocol = \"http\"\n    }\n}\n\n...\n\n\n\n\n\nYou can follow \nthis link\n to learn more about Layer0 resources in Terraform.\n\n\n\n\nPart 1: Terraform Get\n#\n\n\nThis deployment uses modules, so we'll need to fetch those source materials.\nAt the command prompt, execute the following command:\n\n\nterraform get\n\n\nWe should see output like the following:\n\n\nGet\n:\n \ngit\n::\nhttps\n://\ngithub\n.\ncom\n/quintilesims/g\nuides\n.\ngit\n\n\n\n\n\n\nWe should now have a new local directory called \n.terraform/\n.\nWe don't need to do anything with it; we just want to make sure it's there.\n\n\n\n\nPart 2: Terraform Init\n#\n\n\nThis deployment has provider dependencies so an init call must be made. \n(Terraform v0.11~ requries init)\nAt the command prompt, execute the following command:\n\n\nterraform init\n\n\nWe should see output like the following:\n\n\nInitializing modules...\n- module.guestbook\n\nInitializing provider plugins...\n- Checking for available provider plugins on https://releases.hashicorp.com...\n- Downloading plugin for provider \"template\" (1.0.0)...\n\nThe following providers do not have any version constraints in configuration,\nso the latest version was installed.\n\nTo prevent automatic upgrades to new major versions that may contain breaking\nchanges, it is recommended to add version = \"...\" constraints to the\ncorresponding provider blocks in configuration, with the constraint strings\nsuggested below.\n\n* provider.template: version = \"~> 1.0\"\n\nTerraform has been successfully initialized!\n\nYou may now begin working with Terraform. Try running \"terraform plan\" to see\nany changes that are required for your infrastructure. All Terraform commands\nshould now work.\n\nIf you ever set or change modules or backend configuration for Terraform,\nrerun this command to reinitialize your working directory. If you forget, other\ncommands will detect it and remind you to do so if necessary.\n\n\n\n\n\n\n\nPart 3: Terraform Plan\n#\n\n\nBefore we actually create/update/delete any resources, it's a good idea to find out what Terraform intends to do.\n\n\nRun \nterraform plan\n. Terraform will prompt you for configuration values that it does not have:\n\n\nvar.endpoint\n    Enter a value:\n\nvar.token\n    Enter a value:\n\n\n\n\n\nYou can find these values by running \nl0-setup endpoint <your layer0 prefix>\n.\n\n\n\n\nNote\n\n\nThere are a few ways to configure Terraform so that you don't have to keep entering these values every time you run a Terraform command (editing the \nterraform.tfvars\n file, or exporting evironment variables like \nTF_VAR_endpoint\n and \nTF_VAR_token\n, for example). See the \nTerraform Docs\n for more.\n\n\n\n\nThe \nplan\n command should give us output like the following:\n\n\nRefreshing Terraform state in-memory prior to plan...\nThe refreshed state will be used to calculate this plan, but will not be\npersisted to local or remote state storage.\n\ndata.template_file.guestbook: Refreshing state...\nThe Terraform execution plan has been generated and is shown below.\nResources are shown in alphabetical order for quick scanning. Green resources\nwill be created (or destroyed and then created if an existing resource\nexists), yellow resources are being changed in-place, and red resources\nwill be destroyed. Cyan entries are data sources to be read.\n\nNote: You didn't specify an \"-out\" parameter to save this plan, so when\n\"apply\" is called, Terraform can't guarantee this is what will execute.\n\n+ layer0_environment.demo\n    ami:               \"\n<computed>\n\"\n    cluster_count:     \"\n<computed>\n\"\n    links:             \"\n<computed>\n\"\n    name:              \"demo\"\n    os:                \"linux\"\n    security_group_id: \"\n<computed>\n\"\n    size:              \"m3.medium\"\n\n+ module.guestbook.layer0_deploy.guestbook\n    content: \"{\\n    \\\"AWSEBDockerrunVersion\\\": 2,\\n    \\\"containerDefinitions\\\": [\\n        {\\n            \\\"name\\\": \\\"guestbook\\\",\\n            \\\"image\\\": \\\"quintilesims/guestbook\\\",\\n            \\\"essential\\\": true,\\n      \\\"memory\\\": 128,\\n            \\\"environment\\\": [\\n                {\\n                    \\\"name\\\": \\\"GUESTBOOK_BACKEND_TYPE\\\",\\n                    \\\"value\\\": \\\"memory\\\"\\n                },\\n                {\\n          \\\"name\\\": \\\"GUESTBOOK_BACKEND_CONFIG\\\",\\n                    \\\"value\\\": \\\"\\\"\\n                },\\n           {\\n                    \\\"name\\\": \\\"AWS_ACCESS_KEY_ID\\\",\\n                    \\\"value\\\": \\\"\\\"\\n        },\\n                {\\n                    \\\"name\\\": \\\"AWS_SECRET_ACCESS_KEY\\\",\\n                    \\\"value\\\": \\\"\\\"\\n                },\\n                {\\n                    \\\"name\\\": \\\"AWS_REGION\\\",\\n      \\\"value\\\": \\\"us-west-2\\\"\\n                }\\n            ],\\n            \\\"portMappings\\\": [\\n   {\\n                    \\\"hostPort\\\": 80,\\n                    \\\"containerPort\\\": 80\\n                }\\n      ]\\n        }\\n    ]\\n}\\n\"\n    name:    \"guestbook\"\n\n+ module.guestbook.layer0_load_balancer.guestbook\n    environment:                    \"\n${\nvar\n.\nenvironment_id\n}\n\"\n    health_check.#:                 \"\n<computed>\n\"\n    name:                           \"guestbook\"\n    port.#:                         \"1\"\n    port.2027667003.certificate:    \"\"\n    port.2027667003.container_port: \"80\"\n    port.2027667003.host_port:      \"80\"\n    port.2027667003.protocol:       \"http\"\n    url:                            \"\n<computed>\n\"\n\n+ module.guestbook.layer0_service.guestbook\n    deploy:        \"\n${\n \nvar\n.\ndeploy_id\n \n==\n \\\n\"\n\\\"\n ? layer0_deploy.guestbook.id : var.deploy_id \n}\n\"\n    environment:   \"\n${\nvar\n.\nenvironment_id\n}\n\"\n    load_balancer: \"\n${\nlayer0_load_balancer\n.\nguestbook\n.\nid\n}\n\"\n    name:          \"guestbook\"\n    scale:         \"1\"\n    wait:          \"true\"\n\n\nPlan: 4 to add, 0 to change, 0 to destroy.\n\n\n\n\n\nThis shows you that Terraform intends to create a deploy, an environment, a load balancer, and a service, all through Layer0.\n\n\nIf you've gone through this deployment using the \nLayer0 CLI\n, you may notice that these resources appear out of order - that's fine. Terraform presents these resources in alphabetical order, but underneath, it knows the correct order in which to create them.\n\n\nOnce we're satisfied that Terraform will do what we want it to do, we can move on to actually making these things exist!\n\n\n\n\nPart 4: Terraform Apply\n#\n\n\nRun \nterraform apply\n to begin the process.\n\n\nWe should see output like the following:\n\n\nlayer0_environment.demo: Refreshing state...\n...\n...\n...\nlayer0_service.guestbook: Creation complete\n\nApply complete! Resources: 7 added, 0 changed, 0 destroyed.\n\nThe state of your infrastructure has been saved to the path\nbelow. This state is required to modify and destroy your\ninfrastructure, so keep it safe. To inspect the complete state\nuse the `terraform show` command.\n\nState path: terraform.tfstate\n\nOutputs:\n\nguestbook_url = <http endpoint for the sample application>\n\n\n\n\n\n\n\nNote\n\n\nIt may take a few minutes for the guestbook service to launch and the load balancer to become available. During that time you may get HTTP 503 errors when making HTTP requests against the load balancer URL.\n\n\n\n\nWhat's Happening\n#\n\n\nTerraform provisions the AWS resources through Layer0, configures environment variables for the application, and deploys the application into a Layer0 environment. Terraform also writes the state of your deployment to the \nterraform.tfstate\n file (creating a new one if it's not already there).\n\n\nCleanup\n#\n\n\nWhen you're finished with the example, you can instruct Terraform to destroy the Layer0 environment, and terminate the application. Execute the following command (in the same directory):\n\n\nterraform destroy\n\n\nIt's also now safe to remove the \n.terraform/\n directory and the \n*.tfstate*\n files.",
            "title": "Walkthrough: Deployment 1"
        },
        {
            "location": "/guides/walkthrough/deployment-1/#deployment-1-a-simple-guestbook-app",
            "text": "In this section you'll learn how different Layer0 commands work together to deploy applications to the cloud.\nThe example application in this section is a guestbook -- a web application that acts as a simple message board.\nYou can choose to complete this section using either  the Layer0 CLI  or  Terraform .",
            "title": "Deployment 1: A Simple Guestbook App"
        },
        {
            "location": "/guides/walkthrough/deployment-1/#deploy-with-layer0-cli",
            "text": "If you're following along, you'll want to be working in the  walkthrough/deployment-1/  directory of your clone of the  guides  repo.  Files used in this deployment:     Filename  Purpose      Guestbook.Dockerrun.aws.json  Template for running the Guestbook application",
            "title": "Deploy with Layer0 CLI"
        },
        {
            "location": "/guides/walkthrough/deployment-1/#part-1-create-the-environment",
            "text": "The first step in deploying an application with Layer0 is to create an environment.\nAn environment is a dedicated space in which one or more services can reside.\nHere, we'll create a new environment named  demo-env .\nAt the command prompt, execute the following:  l0 environment create demo-env  We should see output like the following:  ENVIRONMENT ID  ENVIRONMENT NAME  CLUSTER COUNT  INSTANCE SIZE  LINKS\ndemo00e6aa9     demo-env          0              m3.medium  We can inspect our environments in a couple of different ways:   l0 environment list  will give us a brief summary of all environments:   ENVIRONMENT ID  ENVIRONMENT NAME\ndemo00e6aa9     demo-env\napi             api   l0 environment get demo-env  will show us more information about the  demo-env  environment we just created:   ENVIRONMENT ID  ENVIRONMENT NAME  CLUSTER COUNT  INSTANCE SIZE  LINKS\ndemo00e6aa9     demo-env          0              m3.medium   l0 environment get \\*  illustrates wildcard matching (you could also have used  demo*  in the above command), and it will return detailed information for  each  environment, not just one - it's like a detailed  list :   ENVIRONMENT ID  ENVIRONMENT NAME  CLUSTER COUNT  INSTANCE SIZE  LINKS\ndemo00e6aa9     demo-env          0              m3.medium\napi             api               2              m3.medium",
            "title": "Part 1: Create the Environment"
        },
        {
            "location": "/guides/walkthrough/deployment-1/#part-2-create-the-load-balancer",
            "text": "In order to expose a web application to the public internet, we need to create a load balancer.\nA load balancer listens for web traffic at a specific address and directs that traffic to a Layer0 service.  A load balancer also has a notion of a health check - a way to assess whether or not the service is healthy and running properly.\nBy default, Layer0 configures the health check of a load balancer based upon a simple TCP ping to port 80 every thirty seconds.\nAlso by default, this ping will timeout after five seconds of no response from the service, and two consecutive successes or failures are required for the service to be considered healthy or unhealthy.  Here, we'll create a new load balancer named  guestbook-lb  inside of our environment named  demo-env .\nThe load balancer will listen on port 80, and forward that traffic along to port 80 in the Docker container using the HTTP protocol.\nSince the port configuration is already aligned with the default health check, we don't need to specify any health check configuration when we create this load balancer.\nAt the command prompt, execute the following:  l0 loadbalancer create --port 80:80/http demo-env guestbook-lb  We should see output like the following:  LOADBALANCER ID  LOADBALANCER NAME  ENVIRONMENT  SERVICE  PORTS       PUBLIC  URL\nguestbodb65a     guestbook-lb       demo-env              80:80/HTTP  true  The following is a summary of the arguments passed in the above command:   loadbalancer create : creates a new load balancer  --port 80:80/HTTP : instructs the load balancer to forward requests from port 80 on the load balancer to port 80 in the EC2 instance using the HTTP protocol  demo-env : the name of the environment in which you are creating the load balancer  guestbook-lb : a name for the load balancer itself   You can inspect load balancers in the same way that you inspected environments in Part 1.\nTry running the following commands to get an idea of the information available to you:   l0 loadbalancer list  l0 loadbalancer get guestbook-lb  l0 loadbalancer get gues*  l0 loadbalancer get \\*    Note  Notice that the load balancer  list  and  get  outputs list an  ENVIRONMENT  field - if you ever have load balancers (or other Layer0 entities) with the same name but in different environments, you can target a specific load balancer by qualifying it with its environment name:  `l0 loadbalancer get demo-env:guestbook-lb`",
            "title": "Part 2: Create the Load Balancer"
        },
        {
            "location": "/guides/walkthrough/deployment-1/#part-3-deploy-the-ecs-task-definition",
            "text": "The  deploy  command is used to specify the ECS task definition that outlines a web application.\nA deploy, once created, can be applied to multiple services - even across different environments!  Here, we'll create a new deploy called  guestbook-dpl  that refers to the  Guestbook.Dockerrun.aws.json  file found in the guides reposiory.\nAt the command prompt, execute the following:  l0 deploy create Guestbook.Dockerrun.aws.json guestbook-dpl  We should see output like the following:  DEPLOY ID        DEPLOY NAME    VERSION\nguestbook-dpl.1  guestbook-dpl  1  The following is a summary of the arguments passed in the above command:   deploy create : creates a new deployment and allows you to specify an ECS task definition  Guestbook.Dockerrun.aws.json : the file name of the ECS task definition (use the full path of the file if it is not in your current working directory)  guestbook-dpl : a name for the deploy, which you will use later when you create the service    Deploy Versioning  The  DEPLOY NAME  and  VERSION  are combined to create a unique identifier for a deploy.\nIf you create additional deploys named  guestbook-dpl , they will be assigned different version numbers.  You can always specify the latest version when targeting a deploy by using  <deploy name>:latest  -- for example,  guestbook-dpl:latest .   Deploys support the same methods of inspection as environments and load balancers:   l0 deploy list  l0 deploy get guestbook*  l0 deploy get guestbook:1  l0 deploy get guestbook:latest  l0 deploy get \\*",
            "title": "Part 3: Deploy the ECS Task Definition"
        },
        {
            "location": "/guides/walkthrough/deployment-1/#part-4-create-the-service",
            "text": "The final stage of the deployment process involves using the  service  command to create a new service and associate it with the environment, load balancer, and deploy that we created in the previous sections.\nThe service will execute the Docker containers which have been described in the deploy.  Here, we'll create a new service called  guestbook-svc . At the command prompt, execute the following:  l0 service create --loadbalancer demo-env:guestbook-lb demo-env guestbook-svc guestbook-dpl:latest  We should see output like the following:  SERVICE ID    SERVICE NAME   ENVIRONMENT  LOADBALANCER  DEPLOYMENTS       SCALE\nguestbo9364b  guestbook-svc  demo-env     guestbook-lb  guestbook-dpl:1*  0/1  The following is a summary of the arguments passed in the above command:   service create : creates a new service  --loadbalancer demo-env:guestbook-lb : the fully-qualified name of the load balancer; in this case, the load balancer named  guestbook-lb  in the environment named  demo-env .   (It is not strictly necessary to use the fully qualified name of the load balancer, unless another load balancer with exactly the same name exists in a different environment.)    demo-env : the name of the environment you created in Part 1  guestbook-svc : a name for the service you are creating  guestbook-dpl : the name of the deploy that you created in Part 3   Layer0 services can be queried using the same  get  and  list  commands that we've come to expect by now.",
            "title": "Part 4: Create the Service"
        },
        {
            "location": "/guides/walkthrough/deployment-1/#check-the-status-of-the-service",
            "text": "After a service has been created, it may take several minutes for that service to completely finish deploying.\nA service's status may be checked by using the  service get  command.  Let's take a peek at our  guestbook-svc  service.\nAt the command prompt, execute the following:  l0 service get demo-env:guestbook-svc  If we're quick enough, we'll be able to see the first stage of the process (this is what was output after running the  service create  command up in Part 4).\nWe should see an asterisk (*) next to the name of the  guestbook-dpl:1  deploy, which indicates that the service is in a transitional state:  SERVICE ID    SERVICE NAME   ENVIRONMENT  LOADBALANCER  DEPLOYMENTS       SCALE\nguestbo9364b  guestbook-svc  demo-env     guestbook-lb  guestbook-dpl:1*  0/1  In the next phase of deployment, if we execute the  service get  command again, we will see  (1)  in the  Scale  column; this indicates that 1 copy of the service is transitioning to an active state:  SERVICE ID    SERVICE NAME   ENVIRONMENT  LOADBALANCER  DEPLOYMENTS       SCALE\nguestbo9364b  guestbook-svc  demo-env     guestbook-lb  guestbook-dpl:1*  0/1 (1)  In the final phase of deployment, we will see  1/1  in the  Scale  column; this indicates that the service is running 1 copy:  SERVICE ID    SERVICE NAME   ENVIRONMENT  LOADBALANCER  DEPLOYMENTS       SCALE\nguestbo9364b  guestbook-svc  demo-env     guestbook-lb  guestbook-dpl:1   1/1",
            "title": "Check the Status of the Service"
        },
        {
            "location": "/guides/walkthrough/deployment-1/#get-the-applications-url",
            "text": "Once the service has been completely deployed, we can obtain the URL for the application and launch it in a browser.  At the command prompt, execute the following:  l0 loadbalancer get demo-env:guestbook-lb  We should see output like the following:  LOADBALANCER ID  LOADBALANCER NAME  ENVIRONMENT  SERVICE        PORTS       PUBLIC  URL\nguestbodb65a     guestbook-lb       demo-env     guestbook-svc  80:80/HTTP  true    <url>  Copy the value shown in the  URL  column and paste it into a web browser.\nThe guestbook application will appear (once the service has completely finished deploying).",
            "title": "Get the Application's URL"
        },
        {
            "location": "/guides/walkthrough/deployment-1/#logs",
            "text": "Output from a Service's docker containers may be acquired by running the following command:  l0 service logs <SERVICE>",
            "title": "Logs"
        },
        {
            "location": "/guides/walkthrough/deployment-1/#cleanup",
            "text": "If you're finished with the example and don't want to continue with this walkthrough, you can instruct Layer0 to delete the environment and terminate the application.  l0 environment delete demo-env  However, if you intend to continue through  Deployment 2 , you will want to keep the resources you made in this section.",
            "title": "Cleanup"
        },
        {
            "location": "/guides/walkthrough/deployment-1/#deploy-with-terraform",
            "text": "Instead of using the Layer0 CLI directly, you can instead use our Terraform provider, and deploy using Terraform  ( learn more ) .\nYou can use Terraform with Layer0 and AWS to create \"fire-and-forget\" deployments for your applications.  If you're following along, you'll want to be working in the  walkthrough/deployment-1/  directory of your clone of the  guides  repo.  We use these files to set up a Layer0 environment with Terraform:     Filename  Purpose      main.tf  Provisions resources; populates resources in template files    outputs.tf  Values that Terraform will yield during deployment    terraform.tfstate  Tracks status of deployment  (created and managed by Terraform)    terraform.tfvars  Variables specific to the environment and application(s)    variables.tf  Values that Terraform will use during deployment",
            "title": "Deploy with Terraform"
        },
        {
            "location": "/guides/walkthrough/deployment-1/#tf-a-brief-aside",
            "text": "Let's take a moment to discuss the  .tf  files.\nThe names of these files (and even the fact that they are separated out into multiple files at all) are completely arbitrary and exist soley for human-readability.\nTerraform understands all  .tf  files in a directory all together.  In  variables.tf , you'll see  \"endpoint\"  and  \"token\"  variables.  In  outputs.tf , you'll see that Terraform should spit out the url of the guestbook's load balancer once deployment has finished.  In  main.tf , you'll see the bulk of the deployment process.\nIf you've followed along with the Layer0 CLI deployment above, it should be fairly easy to see how blocks in this file map to steps in the CLI process.\nWhen we began the CLI deployment, our first step was to create an environment:  l0 environment create demo-env  This command is recreated in  main.tf  like so:  # walkthrough/deployment-1/main.tf\n\nresource \"layer0_environment\" \"demo-env\" {\n    name = \"demo-env\"\n}  We've bundled up the heart of the Guestbook deployment (load balancer, deploy, service, etc.) into a  Terraform module .\nTo use it, we declare a  module  block and pass in the source of the module as well as any configuration or variables that the module needs.  # walkthrough/deployment-1/main.tf\n\nmodule \"guestbook\" {\n    source         = \"github.com/quintilesims/guides//guestbook/module\"\n    environment_id = \" ${ layer0_environment . demo . id } \"\n}  You can see that we pass in the ID of the environment we create.\nAll variables declared in this block are passed to the module, so the next file we should look at is  variables.tf  inside of the module to get an idea of what the module is expecting.  There are a lot of variables here, but only one of them doesn't have a default value.  # guestbook/module/variables.tf\n\nvariable \"environment_id\" {\n    description = \"id of the layer0 environment in which to create resources\"\n}  You'll notice that this is the variable that we're passing in.\nFor this particular deployment of the Guestbook, all of the default options are fine.\nWe could override any of them if we wanted to, just by specifying a new value for them back in  deployment-1/main.tf .  Now that we've seen the variables that the module will have, let's take a look at part of  module/main.tf  and see how some of them might be used:  # guestbook/module/main.tf\n\nresource \"layer0_load_balancer\" \"guestbook-lb\" {\n    name = \" ${ var . load_balancer_name } \"\n    environment = \" ${ var . environment_id } \"\n    port {\n        host_port = 80\n        container_port = 80\n        protocol = \"http\"\n    }\n}\n\n...  You can follow  this link  to learn more about Layer0 resources in Terraform.",
            "title": "*.tf: A Brief Aside"
        },
        {
            "location": "/guides/walkthrough/deployment-1/#part-1-terraform-get",
            "text": "This deployment uses modules, so we'll need to fetch those source materials.\nAt the command prompt, execute the following command:  terraform get  We should see output like the following:  Get :   git :: https :// github . com /quintilesims/g uides . git   We should now have a new local directory called  .terraform/ .\nWe don't need to do anything with it; we just want to make sure it's there.",
            "title": "Part 1: Terraform Get"
        },
        {
            "location": "/guides/walkthrough/deployment-1/#part-2-terraform-init",
            "text": "This deployment has provider dependencies so an init call must be made. \n(Terraform v0.11~ requries init)\nAt the command prompt, execute the following command:  terraform init  We should see output like the following:  Initializing modules...\n- module.guestbook\n\nInitializing provider plugins...\n- Checking for available provider plugins on https://releases.hashicorp.com...\n- Downloading plugin for provider \"template\" (1.0.0)...\n\nThe following providers do not have any version constraints in configuration,\nso the latest version was installed.\n\nTo prevent automatic upgrades to new major versions that may contain breaking\nchanges, it is recommended to add version = \"...\" constraints to the\ncorresponding provider blocks in configuration, with the constraint strings\nsuggested below.\n\n* provider.template: version = \"~> 1.0\"\n\nTerraform has been successfully initialized!\n\nYou may now begin working with Terraform. Try running \"terraform plan\" to see\nany changes that are required for your infrastructure. All Terraform commands\nshould now work.\n\nIf you ever set or change modules or backend configuration for Terraform,\nrerun this command to reinitialize your working directory. If you forget, other\ncommands will detect it and remind you to do so if necessary.",
            "title": "Part 2: Terraform Init"
        },
        {
            "location": "/guides/walkthrough/deployment-1/#part-3-terraform-plan",
            "text": "Before we actually create/update/delete any resources, it's a good idea to find out what Terraform intends to do.  Run  terraform plan . Terraform will prompt you for configuration values that it does not have:  var.endpoint\n    Enter a value:\n\nvar.token\n    Enter a value:  You can find these values by running  l0-setup endpoint <your layer0 prefix> .   Note  There are a few ways to configure Terraform so that you don't have to keep entering these values every time you run a Terraform command (editing the  terraform.tfvars  file, or exporting evironment variables like  TF_VAR_endpoint  and  TF_VAR_token , for example). See the  Terraform Docs  for more.   The  plan  command should give us output like the following:  Refreshing Terraform state in-memory prior to plan...\nThe refreshed state will be used to calculate this plan, but will not be\npersisted to local or remote state storage.\n\ndata.template_file.guestbook: Refreshing state...\nThe Terraform execution plan has been generated and is shown below.\nResources are shown in alphabetical order for quick scanning. Green resources\nwill be created (or destroyed and then created if an existing resource\nexists), yellow resources are being changed in-place, and red resources\nwill be destroyed. Cyan entries are data sources to be read.\n\nNote: You didn't specify an \"-out\" parameter to save this plan, so when\n\"apply\" is called, Terraform can't guarantee this is what will execute.\n\n+ layer0_environment.demo\n    ami:               \" <computed> \"\n    cluster_count:     \" <computed> \"\n    links:             \" <computed> \"\n    name:              \"demo\"\n    os:                \"linux\"\n    security_group_id: \" <computed> \"\n    size:              \"m3.medium\"\n\n+ module.guestbook.layer0_deploy.guestbook\n    content: \"{\\n    \\\"AWSEBDockerrunVersion\\\": 2,\\n    \\\"containerDefinitions\\\": [\\n        {\\n            \\\"name\\\": \\\"guestbook\\\",\\n            \\\"image\\\": \\\"quintilesims/guestbook\\\",\\n            \\\"essential\\\": true,\\n      \\\"memory\\\": 128,\\n            \\\"environment\\\": [\\n                {\\n                    \\\"name\\\": \\\"GUESTBOOK_BACKEND_TYPE\\\",\\n                    \\\"value\\\": \\\"memory\\\"\\n                },\\n                {\\n          \\\"name\\\": \\\"GUESTBOOK_BACKEND_CONFIG\\\",\\n                    \\\"value\\\": \\\"\\\"\\n                },\\n           {\\n                    \\\"name\\\": \\\"AWS_ACCESS_KEY_ID\\\",\\n                    \\\"value\\\": \\\"\\\"\\n        },\\n                {\\n                    \\\"name\\\": \\\"AWS_SECRET_ACCESS_KEY\\\",\\n                    \\\"value\\\": \\\"\\\"\\n                },\\n                {\\n                    \\\"name\\\": \\\"AWS_REGION\\\",\\n      \\\"value\\\": \\\"us-west-2\\\"\\n                }\\n            ],\\n            \\\"portMappings\\\": [\\n   {\\n                    \\\"hostPort\\\": 80,\\n                    \\\"containerPort\\\": 80\\n                }\\n      ]\\n        }\\n    ]\\n}\\n\"\n    name:    \"guestbook\"\n\n+ module.guestbook.layer0_load_balancer.guestbook\n    environment:                    \" ${ var . environment_id } \"\n    health_check.#:                 \" <computed> \"\n    name:                           \"guestbook\"\n    port.#:                         \"1\"\n    port.2027667003.certificate:    \"\"\n    port.2027667003.container_port: \"80\"\n    port.2027667003.host_port:      \"80\"\n    port.2027667003.protocol:       \"http\"\n    url:                            \" <computed> \"\n\n+ module.guestbook.layer0_service.guestbook\n    deploy:        \" ${   var . deploy_id   ==  \\ \" \\\"  ? layer0_deploy.guestbook.id : var.deploy_id  } \"\n    environment:   \" ${ var . environment_id } \"\n    load_balancer: \" ${ layer0_load_balancer . guestbook . id } \"\n    name:          \"guestbook\"\n    scale:         \"1\"\n    wait:          \"true\"\n\n\nPlan: 4 to add, 0 to change, 0 to destroy.  This shows you that Terraform intends to create a deploy, an environment, a load balancer, and a service, all through Layer0.  If you've gone through this deployment using the  Layer0 CLI , you may notice that these resources appear out of order - that's fine. Terraform presents these resources in alphabetical order, but underneath, it knows the correct order in which to create them.  Once we're satisfied that Terraform will do what we want it to do, we can move on to actually making these things exist!",
            "title": "Part 3: Terraform Plan"
        },
        {
            "location": "/guides/walkthrough/deployment-1/#part-4-terraform-apply",
            "text": "Run  terraform apply  to begin the process.  We should see output like the following:  layer0_environment.demo: Refreshing state...\n...\n...\n...\nlayer0_service.guestbook: Creation complete\n\nApply complete! Resources: 7 added, 0 changed, 0 destroyed.\n\nThe state of your infrastructure has been saved to the path\nbelow. This state is required to modify and destroy your\ninfrastructure, so keep it safe. To inspect the complete state\nuse the `terraform show` command.\n\nState path: terraform.tfstate\n\nOutputs:\n\nguestbook_url = <http endpoint for the sample application>   Note  It may take a few minutes for the guestbook service to launch and the load balancer to become available. During that time you may get HTTP 503 errors when making HTTP requests against the load balancer URL.",
            "title": "Part 4: Terraform Apply"
        },
        {
            "location": "/guides/walkthrough/deployment-1/#whats-happening",
            "text": "Terraform provisions the AWS resources through Layer0, configures environment variables for the application, and deploys the application into a Layer0 environment. Terraform also writes the state of your deployment to the  terraform.tfstate  file (creating a new one if it's not already there).",
            "title": "What's Happening"
        },
        {
            "location": "/guides/walkthrough/deployment-1/#cleanup_1",
            "text": "When you're finished with the example, you can instruct Terraform to destroy the Layer0 environment, and terminate the application. Execute the following command (in the same directory):  terraform destroy  It's also now safe to remove the  .terraform/  directory and the  *.tfstate*  files.",
            "title": "Cleanup"
        },
        {
            "location": "/guides/walkthrough/deployment-2/",
            "text": "Deployment 2: Guestbook + Redis\n#\n\n\nIn this section, we're going to add some complexity to the previous deployment.\n\nDeployment 1\n saw us create a simple guestbook application which kept its data in memory.\nBut what if that ever came down, either by intention or accident?\nIt would be easy enough to redeploy it, but all of the entered data would be lost.\nWhat if we wanted to scale the application to run more than one copy?\nFor this deployment, we're going to separate the data store from the guestbook application by creating a second Layer0 service which will house a Redis database server and linking it to the first.\nYou can choose to complete this section using either \nthe Layer0 CLI\n or \nTerraform\n.\n\n\n\n\nDeploy with Layer0 CLI\n#\n\n\nFor this example, we'll be working in the \nwalkthrough/deployment-2/\n directory of the \nguides\n repo.\nWe assume that you've completed the \nLayer0 CLI\n section of Deployment 1.\n\n\nFiles used in this deployment:\n\n\n\n\n\n\n\n\nFilename\n\n\nPurpose\n\n\n\n\n\n\n\n\n\n\nGuestbook.Dockerrun.aws.json\n\n\nTemplate for running the Guestbook application\n\n\n\n\n\n\nRedis.Dockerrun.aws.json\n\n\nTemplate for running a Redis server\n\n\n\n\n\n\n\n\n\n\nPart 1: Create the Redis Load Balancer\n#\n\n\nBoth the Guestbook service and the Redis service will live in the same Layer0 environment, so we don't need to create one like we did in the first deployment.\nWe'll start by making a load balancer behind which the Redis service will be deployed.\n\n\nThe \nRedis.Dockerrun.aws.json\n task definition file we'll use is very simple - it just spins up a Redis server with the default configuration, which means that it will be serving on port 6379.\nOur load balancer needs to be able to forward TCP traffic to and from this port.\nAnd since we don't want the Redis server to be exposed to the public internet, we'll put it behind a private load balancer; private load balancers only accept traffic that originates from within their own environment.\nWe'll also need to specify a non-default healthcheck target, since the load balancer won't expose port 80.\nAt the command prompt, execute the following:\n\n\nl0 loadbalancer create --port 6379:6379/tcp --private --healthcheck-target tcp:6379 demo-env redis-lb\n\n\nWe should see output like the following:\n\n\nLOADBALANCER ID  LOADBALANCER NAME  ENVIRONMENT  SERVICE  PORTS          PUBLIC  URL\nredislb16ae6     redis-lb           demo-env              6378:6379:TCP  false\n\n\n\n\n\nThe following is a summary of the arguments passed in the above command:\n\n\n\n\nloadbalancer create\n: creates a new load balancer\n\n\n--port 6379:6379/TCP\n: instructs the load balancer to forward requests from port 6379 on the load balancer to port 6379 in the EC2 instance using the TCP protocol\n\n\n--private\n: instructs the load balancer to ignore external traffic\n\n\n--healthcheck-target tcp:6379\n: instructs the load balancer to check the health of the service via TCP pings to port 6379\n\n\ndemo-env\n: the name of the environment in which the load balancer is being created\n\n\nredis-lb\n: a name for the load balancer itself\n\n\n\n\n\n\nPart 2: Deploy the ECS Task Definition\n#\n\n\nHere, we just need to create the deploy using the \nRedis.Dockerrun.aws.json\n task definition file.\nAt the command prompt, execute the following:\n\n\nl0 deploy create Redis.Dockerrun.aws.json redis-dpl\n\n\nWe should see output like the following:\n\n\nDEPLOY ID    DEPLOY NAME  VERSION\nredis-dpl.1  redis-dpl    1\n\n\n\n\n\nThe following is a summary of the arguments passed in the above command:\n\n\n\n\ndeploy create\n: creates a new Layer0 Deploy and allows you to specify an ECS task definition\n\n\nRedis.Dockerrun.aws.json\n: the file name of the ECS task definition (use the full path of the file if it is not in your current working directory)\n\n\nredis-dpl\n: a name for the deploy, which we will use later when we create the service\n\n\n\n\n\n\nPart 3: Create the Redis Service\n#\n\n\nHere, we just need to pull the previous resources together into a service.\nAt the command prompt, execute the following:\n\n\nl0 service create --wait --loadbalancer demo-env:redis-lb demo-env redis-svc redis-dpl:latest\n\n\nWe should see output like the following:\n\n\nSERVICE ID    SERVICE NAME  ENVIRONMENT  LOADBALANCER  DEPLOYMENTS  SCALE\nredislb16ae6  redis-svc     demo-env     redis-lb      redis-dpl:1  0/1\n\n\n\n\n\nThe following is a summary of the arguments passed in the above commands:\n\n\n\n\nservice create\n: creates a new Layer0 Service\n\n\n--wait\n:  instructs the CLI to keep hold of the shell until the service has been successfully deployed\n\n\n--loadbalancer demo-env:redis-lb\n: the fully-qualified name of the load balancer; in this case, the load balancer named \nredis-lb\n in the environment named \ndemo-env\n\n\n(Again, it's not strictly necessary to use the fully-qualified name of the load balancer as long as there isn't another load balancer with the same name in a different environment)\n\n\n\n\n\n\ndemo-env\n: the name of the environment in which the service is to reside\n\n\nredis-svc\n: a name for the service we're creating\n\n\nredis-dpl:latest\n: the name of the deploy the service will put into action\n\n\n(We use \n:\n to specify which deploy we want - \n:latest\n will always give us the most recently-created one.)\n\n\n\n\n\n\n\n\n\n\nPart 4: Check the Status of the Redis Service\n#\n\n\nAs in the first deployment, we can keep an eye on our service by using the \nservice get\n command:\n\n\nl0 service get redis-svc\n\n\nOnce the service has finished scaling, try looking at the service's logs to see the output that the Redis server creates:\n\n\nl0 service logs redis-svc\n\n\nAmong some warnings and information not important to this exercise and a fun bit of ASCII art, you should see something like the following:\n\n\n... # words and ASCII art\n1:M 05 Apr 23:29:47.333 * The server is now ready to accept connections on port 6379\n\n\n\n\n\nNow we just need to teach the Guestbook application how to talk with our Redis service.\n\n\n\n\nPart 5: Update the Guestbook Deploy\n#\n\n\nYou should see in \nwalkthrough/deployment-2/\n another \nGuestbook.Dockerrun.aws.json\n file.\nThis file is very similar to but not the same as the one in \ndeployment-1/\n - if you open it up, you can see the following additions:\n\n\n    ...\n    \"environment\": [\n        {\n            \"name\": \"GUESTBOOK_BACKEND_TYPE\",\n            \"value\": \"redis\"\n        },\n        {\n            \"name\": \"GUESTBOOK_BACKEND_CONFIG\",\n            \"value\": \"<redis host and port here>\"\n        }\n    ],\n    ...\n\n\n\n\n\nThe \n\"GUESTBOOK_BACKEND_CONFIG\"\n variable is what will point the Guestbook application towards the Redis server.\nThe \n<redis host and port here>\n section needs to be replaced and populated in the following format:\n\n\n\"value\": \"ADDRESS_OF_REDIS_SERVER:PORT_THE_SERVER_IS_SERVING_ON\"\n\n\n\n\n\nWe already know that Redis is serving on port 6379, so let's go find the server's address.\nRemember, it lives behind a load balancer that we made, so run the following command:\n\n\nl0 loadbalancer get redis-lb\n\n\nWe should see output like the following:\n\n\nLOADBALANCER ID  LOADBALANCER NAME  ENVIRONMENT  SERVICE    PORTS          PUBLIC  URL\nredislb16ae6     redis-lb           demo-env     redis-svc  6379:6379/TCP  false   internal-l0-<yadda-yadda>.elb.amazonaws.com\n\n\n\n\n\nCopy that \nURL\n value, replace \n<redis host and port here>\n with the \nURL\n value in \nGuestbook.Dockerrun.aws.json\n, append \n:6379\n to it, and save the file.\nIt should look something like the following:\n\n\n    ...\n    \"environment\": [\n        {\n            \"name\": \"GUESTBOOK_BACKEND_CONFIG\",\n            \"value\": \"internal-l0-<yadda-yadda>.elb.amazonaws.com:6379\"\n        }\n    ],\n    ...\n\n\n\n\n\nNow, we can create an updated deploy:\n\n\nl0 deploy create Guestbook.Dockerrun.aws.json guestbook-dpl\n\n\nWe should see output like the following:\n\n\nDEPLOY ID        DEPLOY NAME    VERSION\nguestbook-dpl.2  guestbook-dpl  2\n\n\n\n\n\n\n\nPart 6: Update the Guestbook Service\n#\n\n\nAlmost all the pieces are in place!\nNow we just need to apply the new Guestbook deploy to the running Guestbook service:\n\n\nl0 service update guestbook-svc guestbook-dpl:latest\n\n\nAs the Guestbook service moves through the phases of its update process, we should see outputs like the following (if we keep an eye on the service with \nl0 service get guestbook-svc\n, that is):\n\n\nSERVICE ID    SERVICE NAME   ENVIRONMENT  LOADBALANCER  DEPLOYMENTS       SCALE\nguestbo5fadd  guestbook-svc  demo-env     guestbook-lb  guestbook-dpl:2*  1/1\n                                                        guestbook-dpl:1\n\n\n\n\n\nabove: \nguestbook-dpl:2\n is in a transitional state\n\n\nSERVICE ID    SERVICE NAME   ENVIRONMENT  LOADBALANCER  DEPLOYMENTS      SCALE\nguestbo5fadd  guestbook-svc  demo-env     guestbook-lb  guestbook-dpl:2  2/1\n                                                        guestbook-dpl:1\n\n\n\n\n\nabove: both versions of the deployment are running at scale\n\n\nSERVICE ID    SERVICE NAME   ENVIRONMENT  LOADBALANCER  DEPLOYMENTS       SCALE\nguestbo5fadd  guestbook-svc  demo-env     guestbook-lb  guestbook-dpl:2   1/1\n                                                        guestbook-dpl:1*\n\n\n\n\n\nabove: \nguestbook-dpl:1\n is in a transitional state\n\n\nSERVICE ID    SERVICE NAME   ENVIRONMENT  LOADBALANCER  DEPLOYMENTS      SCALE\nguestbo5fadd  guestbook-svc  demo-env     guestbook-lb  guestbook-dpl:2  1/1\n\n\n\n\n\nabove: \nguestbook-dpl:1\n has been removed, and only \nguestbook-dpl:2\n remains\n\n\n\n\nPart 7: Prove It\n#\n\n\nYou should now be able to point your browser at the URL for the Guestbook load balancer (run \nl0 loadbalancer get guestbook-lb\n to find it) and see what looks like the same Guestbook application you deployed in the first section of the walkthrough.\nGo ahead and add a few entries, make sure it's functioning properly.\nWe'll wait.\n\n\nNow, let's prove that we've actually separated the data from the application by deleting and redeploying the Guestbook application:\n\n\nl0 service delete --wait guestbook-svc\n\n\n(We'll leave the \ndeploy\n intact so we can spin up a new service easily, and we'll leave the environment untouched because it also contained the Redis server.\nWe'll also pass the \n--wait\n flag so that we don't need to keep checking on the status of the job to know when it's complete.)\n\n\nOnce those resources have been deleted, we can recreate them!\n\n\nCreate another service, using the \nguestbook-dpl\n deploy we kept around:\n\n\nl0 service create --loadbalancer demo-env:guestbook-lb demo-env guestbook-svc guestbook-dpl:latest\n\n\nWait for everything to spin up, and hit that new load balancer's url (\nl0 loadbalancer get guestbook-lb\n) with your browser.\nYour data should still be there!\n\n\n\n\nCleanup\n#\n\n\nWhen you're finished with the example, instruct Layer0 to delete the environment and terminate the application:\n\n\nl0 environment delete demo-env\n\n\n\n\nDeploy with Terraform\n#\n\n\nAs before, we can complete this deployment using Terraform and the Layer0 provider instead of the Layer0 CLI. As before, we will assume that you've cloned the \nguides\n repo and are working in the \nwalkthrough/deployment-2/\n directory.\n\n\nWe'll use these files to manage our deployment with Terraform:\n\n\n\n\n\n\n\n\nFilename\n\n\nPurpose\n\n\n\n\n\n\n\n\n\n\nmain.tf\n\n\nProvisions resources; populates variables in template files\n\n\n\n\n\n\noutputs.tf\n\n\nValues that Terraform will yield during deployment\n\n\n\n\n\n\nterraform.tfstate\n\n\nTracks status of deployment \n(created and managed by Terraform)\n\n\n\n\n\n\nterraform.tfvars\n\n\nVariables specific to the environment and application(s)\n\n\n\n\n\n\nvariables.tf\n\n\nValues that Terraform will use during deployment\n\n\n\n\n\n\n\n\n\n\n*.tf\n: A Brief Aside: Revisited\n#\n\n\nNot much is changed from \nDeployment 1\n.\nIn \nmain.tf\n, we pull in a new, second module that will deploy Redis for us.\nWe maintain this module as well; you can inspect \nthe repo\n if you'd like.\n\n\nIn \nmain.tf\n where we pull in the Guestbook module, you'll see that we're supplying more values than we did last time, because we need some additional configuration to let the Guestbook application use a Redis backend instead of its default in-memory storage.\n\n\n\n\nPart 1: Terraform Get\n#\n\n\nRun \nterraform get\n to pull down the source materials Terraform will use for deployment.\nThis will create a local \n.terraform/\n directory.\n\n\n\n\nPart 2: Terraform Init\n#\n\n\nThis deployment has provider dependencies so an init call must be made. \n(Terraform v0.11~ requries init)\nAt the command prompt, execute the following command:\n\n\nterraform init\n\n\nWe should see output like the following:\n\n\nInitializing modules...\n- module.redis\n  Getting source \"github.com/quintilesims/redis//terraform\"\n- module.guestbook\n  Getting source \"github.com/quintilesims/guides//guestbook/module\"\n\nInitializing provider plugins...\n- Checking for available provider plugins on https://releases.hashicorp.com...\n- Downloading plugin for provider \"template\" (1.0.0)...\n\nThe following providers do not have any version constraints in configuration,\nso the latest version was installed.\n\nTo prevent automatic upgrades to new major versions that may contain breaking\nchanges, it is recommended to add version = \"...\" constraints to the\ncorresponding provider blocks in configuration, with the constraint strings\nsuggested below.\n\n* provider.template: version = \"~> 1.0\"\n\nTerraform has been successfully initialized!\n\nYou may now begin working with Terraform. Try running \"terraform plan\" to see\nany changes that are required for your infrastructure. All Terraform commands\nshould now work.\n\nIf you ever set or change modules or backend configuration for Terraform,\nrerun this command to reinitialize your working directory. If you forget, other\ncommands will detect it and remind you to do so if necessary.\n\n\n\n\n\n\n\nPart 3: Terraform Plan\n#\n\n\nIt's always a good idea to find out what Terraform intends to do, so let's do that:\n\n\nterraform plan\n\n\nAs before, we'll be prompted for any variables Terraform needs and doesn't have (see the note in \nDeployment 1\n for configuring Terraform variables).\nWe'll see output similar to the following:\n\n\nRefreshing Terraform state in-memory prior to plan...\nThe refreshed state will be used to calculate this plan, but will not be\npersisted to local or remote state storage.\n\ndata.template_file.redis: Refreshing state...\nThe Terraform execution plan has been generated and is shown below.\nResources are shown in alphabetical order for quick scanning. Green resources\nwill be created (or destroyed and then created if an existing resource\nexists), yellow resources are being changed in-place, and red resources\nwill be destroyed. Cyan entries are data sources to be read.\n\nNote: You didn't specify an \"-out\" parameter to save this plan, so when\n\"apply\" is called, Terraform can't guarantee this is what will execute.\n\n+ layer0_environment.demo\n    ami:               \"\n<computed>\n\"\n    cluster_count:     \"\n<computed>\n\"\n    links:             \"\n<computed>\n\"\n    name:              \"demo\"\n    os:                \"linux\"\n    security_group_id: \"\n<computed>\n\"\n    size:              \"m3.medium\"\n\n+ module.redis.layer0_deploy.redis\n    content: \"{\\n    \\\"AWSEBDockerrunVersion\\\": 2,\\n    \\\"containerDefinitions\\\": [\\n        {\\n            \\\"name\\\": \\\"redis\\\",\\n            \\\"image\\\": \\\"redis:3.2-alpine\\\",\\n            \\\"essential\\\": true,\\n            \\\"memory\\\": 128,\\n            \\\"portMappings\\\": [\\n                {\\n                    \\\"hostPort\\\": 6379,\\n              \\\"containerPort\\\": 6379\\n                }\\n            ]\\n        }\\n    ]\\n}\\n\\n\"\n    name:    \"redis\"\n\n+ module.redis.layer0_load_balancer.redis\n    environment:                    \"\n${\nvar\n.\nenvironment_id\n}\n\"\n    health_check.#:                 \"\n<computed>\n\"\n    name:                           \"redis\"\n    port.#:                         \"1\"\n    port.1072619732.certificate:    \"\"\n    port.1072619732.container_port: \"6379\"\n    port.1072619732.host_port:      \"6379\"\n    port.1072619732.protocol:       \"tcp\"\n    private:                        \"true\"\n    url:                            \"\n<computed>\n\"\n\n+ module.redis.layer0_service.redis\n    deploy:        \"\n${\n \nvar\n.\ndeploy_id\n \n==\n \\\n\"\n\\\"\n ? layer0_deploy.redis.id : var.deploy_id \n}\n\"\n    environment:   \"\n${\nvar\n.\nenvironment_id\n}\n\"\n    load_balancer: \"\n${\nlayer0_load_balancer\n.\nredis\n.\nid\n}\n\"\n    name:          \"redis\"\n    scale:         \"1\"\n    wait:          \"true\"\n\n\n<\n= module.guestbook.data.template_file.guestbook\n    rendered: \"\n<computed>\n\"\n    template: \"{\\n    \\\"AWSEBDockerrunVersion\\\": 2,\\n    \\\"containerDefinitions\\\": [\\n        {\\n            \\\"name\\\": \\\"guestbook\\\",\\n            \\\"image\\\": \\\"quintilesims/guestbook\\\",\\n            \\\"essential\\\": true,\\n       \\\"memory\\\": 128,\\n            \\\"environment\\\": [\\n                {\\n                    \\\"name\\\": \\\"GUESTBOOK_BACKEND_TYPE\\\",\\n                    \\\"value\\\": \\\"\n${\nbackend_type\n}\n\\\"\\n                },\\n                {\\n                    \\\"name\\\": \\\"GUESTBOOK_BACKEND_CONFIG\\\",\\n                    \\\"value\\\": \\\"\n${\nbackend_config\n}\n\\\"\\n                },\\n                {\\n                    \\\"name\\\": \\\"AWS_ACCESS_KEY_ID\\\",\\n  \\\"value\\\": \\\"\n${\naccess_key\n}\n\\\"\\n                },\\n                {\\n                    \\\"name\\\": \\\"AWS_SECRET_ACCESS_KEY\\\",\\n                    \\\"value\\\": \\\"\n${\nsecret_key\n}\n\\\"\\n                },\\n                {\\n            \\\"name\\\": \\\"AWS_REGION\\\",\\n                    \\\"value\\\": \\\"\n${\nregion\n}\n\\\"\\n                }\\n   ],\\n            \\\"portMappings\\\": [\\n                {\\n                    \\\"hostPort\\\": 80,\\n     \\\"containerPort\\\": 80\\n                }\\n            ]\\n        }\\n    ]\\n}\\n\"\n    vars.%:   \"\n<computed>\n\"\n\n+ module.guestbook.layer0_deploy.guestbook\n    content: \"\n${\ndata\n.\ntemplate_file\n.\nguestbook\n.\nrendered\n}\n\"\n    name:    \"guestbook\"\n\n+ module.guestbook.layer0_load_balancer.guestbook\n    environment:                    \"\n${\nvar\n.\nenvironment_id\n}\n\"\n    health_check.#:                 \"\n<computed>\n\"\n    name:                           \"guestbook\"\n    port.#:                         \"1\"\n    port.2027667003.certificate:    \"\"\n    port.2027667003.container_port: \"80\"\n    port.2027667003.host_port:      \"80\"\n    port.2027667003.protocol:       \"http\"\n    url:                            \"\n<computed>\n\"\n\n+ module.guestbook.layer0_service.guestbook\n    deploy:        \"\n${\n \nvar\n.\ndeploy_id\n \n==\n \\\n\"\n\\\"\n ? layer0_deploy.guestbook.id : var.deploy_id \n}\n\"\n    environment:   \"\n${\nvar\n.\nenvironment_id\n}\n\"\n    load_balancer: \"\n${\nlayer0_load_balancer\n.\nguestbook\n.\nid\n}\n\"\n    name:          \"guestbook\"\n    scale:         \"2\"\n    wait:          \"true\"\n\n\nPlan: 7 to add, 0 to change, 0 to destroy.\n\n\n\n\n\nWe should see that Terraform intends to add 7 new resources, some of which are for the Guestbook deployment and some of which are for the Redis deployment.\n\n\n\n\nPart 4: Terraform Apply\n#\n\n\nRun \nterraform apply\n, and we should see output similar to the following:\n\n\ndata.template_file.redis: Refreshing state...\nlayer0_deploy.redis-dpl: Creating...\n\n...\n...\n...\n\nlayer0_service.guestbook-svc: Creation complete\n\nApply complete! Resources: 7 added, 0 changed, 0 destroyed.\n\nThe state of your infrastructure has been saved to the path\nbelow. This state is required to modify and destroy your\ninfrastructure, so keep it safe. To inspect the complete state\nuse the `terraform show` command.\n\nState path: terraform.tfstate\n\nOutputs:\n\nguestbook_url = <http endpoint for the sample application>\n\n\n\n\n\n\n\nNote\n\n\nIt may take a few minutes for the guestbook service to launch and the load balancer to become available.\nDuring that time you may get HTTP 503 errors when making HTTP requests against the load balancer URL.\n\n\n\n\nWhat's Happening\n#\n\n\nTerraform provisions the AWS resources through Layer0, configures environment variables for the application, and deploys the application into a Layer0 environment.\nTerraform also writes the state of your deployment to the \nterraform.tfstate\n file (creating a new one if it's not already there).\n\n\nCleanup\n#\n\n\nWhen you're finished with the example, you can instruct Terraform to destroy the Layer0 environment, and terminate the application.\nExecute the following command (in the same directory):\n\n\nterraform destroy\n\n\nIt's also now safe to remove the \n.terraform/\n directory and the \n*.tfstate*\n files.",
            "title": "Walkthrough: Deployment 2"
        },
        {
            "location": "/guides/walkthrough/deployment-2/#deployment-2-guestbook-redis",
            "text": "In this section, we're going to add some complexity to the previous deployment. Deployment 1  saw us create a simple guestbook application which kept its data in memory.\nBut what if that ever came down, either by intention or accident?\nIt would be easy enough to redeploy it, but all of the entered data would be lost.\nWhat if we wanted to scale the application to run more than one copy?\nFor this deployment, we're going to separate the data store from the guestbook application by creating a second Layer0 service which will house a Redis database server and linking it to the first.\nYou can choose to complete this section using either  the Layer0 CLI  or  Terraform .",
            "title": "Deployment 2: Guestbook + Redis"
        },
        {
            "location": "/guides/walkthrough/deployment-2/#deploy-with-layer0-cli",
            "text": "For this example, we'll be working in the  walkthrough/deployment-2/  directory of the  guides  repo.\nWe assume that you've completed the  Layer0 CLI  section of Deployment 1.  Files used in this deployment:     Filename  Purpose      Guestbook.Dockerrun.aws.json  Template for running the Guestbook application    Redis.Dockerrun.aws.json  Template for running a Redis server",
            "title": "Deploy with Layer0 CLI"
        },
        {
            "location": "/guides/walkthrough/deployment-2/#part-1-create-the-redis-load-balancer",
            "text": "Both the Guestbook service and the Redis service will live in the same Layer0 environment, so we don't need to create one like we did in the first deployment.\nWe'll start by making a load balancer behind which the Redis service will be deployed.  The  Redis.Dockerrun.aws.json  task definition file we'll use is very simple - it just spins up a Redis server with the default configuration, which means that it will be serving on port 6379.\nOur load balancer needs to be able to forward TCP traffic to and from this port.\nAnd since we don't want the Redis server to be exposed to the public internet, we'll put it behind a private load balancer; private load balancers only accept traffic that originates from within their own environment.\nWe'll also need to specify a non-default healthcheck target, since the load balancer won't expose port 80.\nAt the command prompt, execute the following:  l0 loadbalancer create --port 6379:6379/tcp --private --healthcheck-target tcp:6379 demo-env redis-lb  We should see output like the following:  LOADBALANCER ID  LOADBALANCER NAME  ENVIRONMENT  SERVICE  PORTS          PUBLIC  URL\nredislb16ae6     redis-lb           demo-env              6378:6379:TCP  false  The following is a summary of the arguments passed in the above command:   loadbalancer create : creates a new load balancer  --port 6379:6379/TCP : instructs the load balancer to forward requests from port 6379 on the load balancer to port 6379 in the EC2 instance using the TCP protocol  --private : instructs the load balancer to ignore external traffic  --healthcheck-target tcp:6379 : instructs the load balancer to check the health of the service via TCP pings to port 6379  demo-env : the name of the environment in which the load balancer is being created  redis-lb : a name for the load balancer itself",
            "title": "Part 1: Create the Redis Load Balancer"
        },
        {
            "location": "/guides/walkthrough/deployment-2/#part-2-deploy-the-ecs-task-definition",
            "text": "Here, we just need to create the deploy using the  Redis.Dockerrun.aws.json  task definition file.\nAt the command prompt, execute the following:  l0 deploy create Redis.Dockerrun.aws.json redis-dpl  We should see output like the following:  DEPLOY ID    DEPLOY NAME  VERSION\nredis-dpl.1  redis-dpl    1  The following is a summary of the arguments passed in the above command:   deploy create : creates a new Layer0 Deploy and allows you to specify an ECS task definition  Redis.Dockerrun.aws.json : the file name of the ECS task definition (use the full path of the file if it is not in your current working directory)  redis-dpl : a name for the deploy, which we will use later when we create the service",
            "title": "Part 2: Deploy the ECS Task Definition"
        },
        {
            "location": "/guides/walkthrough/deployment-2/#part-3-create-the-redis-service",
            "text": "Here, we just need to pull the previous resources together into a service.\nAt the command prompt, execute the following:  l0 service create --wait --loadbalancer demo-env:redis-lb demo-env redis-svc redis-dpl:latest  We should see output like the following:  SERVICE ID    SERVICE NAME  ENVIRONMENT  LOADBALANCER  DEPLOYMENTS  SCALE\nredislb16ae6  redis-svc     demo-env     redis-lb      redis-dpl:1  0/1  The following is a summary of the arguments passed in the above commands:   service create : creates a new Layer0 Service  --wait :  instructs the CLI to keep hold of the shell until the service has been successfully deployed  --loadbalancer demo-env:redis-lb : the fully-qualified name of the load balancer; in this case, the load balancer named  redis-lb  in the environment named  demo-env  (Again, it's not strictly necessary to use the fully-qualified name of the load balancer as long as there isn't another load balancer with the same name in a different environment)    demo-env : the name of the environment in which the service is to reside  redis-svc : a name for the service we're creating  redis-dpl:latest : the name of the deploy the service will put into action  (We use  :  to specify which deploy we want -  :latest  will always give us the most recently-created one.)",
            "title": "Part 3: Create the Redis Service"
        },
        {
            "location": "/guides/walkthrough/deployment-2/#part-4-check-the-status-of-the-redis-service",
            "text": "As in the first deployment, we can keep an eye on our service by using the  service get  command:  l0 service get redis-svc  Once the service has finished scaling, try looking at the service's logs to see the output that the Redis server creates:  l0 service logs redis-svc  Among some warnings and information not important to this exercise and a fun bit of ASCII art, you should see something like the following:  ... # words and ASCII art\n1:M 05 Apr 23:29:47.333 * The server is now ready to accept connections on port 6379  Now we just need to teach the Guestbook application how to talk with our Redis service.",
            "title": "Part 4: Check the Status of the Redis Service"
        },
        {
            "location": "/guides/walkthrough/deployment-2/#part-5-update-the-guestbook-deploy",
            "text": "You should see in  walkthrough/deployment-2/  another  Guestbook.Dockerrun.aws.json  file.\nThis file is very similar to but not the same as the one in  deployment-1/  - if you open it up, you can see the following additions:      ...\n    \"environment\": [\n        {\n            \"name\": \"GUESTBOOK_BACKEND_TYPE\",\n            \"value\": \"redis\"\n        },\n        {\n            \"name\": \"GUESTBOOK_BACKEND_CONFIG\",\n            \"value\": \"<redis host and port here>\"\n        }\n    ],\n    ...  The  \"GUESTBOOK_BACKEND_CONFIG\"  variable is what will point the Guestbook application towards the Redis server.\nThe  <redis host and port here>  section needs to be replaced and populated in the following format:  \"value\": \"ADDRESS_OF_REDIS_SERVER:PORT_THE_SERVER_IS_SERVING_ON\"  We already know that Redis is serving on port 6379, so let's go find the server's address.\nRemember, it lives behind a load balancer that we made, so run the following command:  l0 loadbalancer get redis-lb  We should see output like the following:  LOADBALANCER ID  LOADBALANCER NAME  ENVIRONMENT  SERVICE    PORTS          PUBLIC  URL\nredislb16ae6     redis-lb           demo-env     redis-svc  6379:6379/TCP  false   internal-l0-<yadda-yadda>.elb.amazonaws.com  Copy that  URL  value, replace  <redis host and port here>  with the  URL  value in  Guestbook.Dockerrun.aws.json , append  :6379  to it, and save the file.\nIt should look something like the following:      ...\n    \"environment\": [\n        {\n            \"name\": \"GUESTBOOK_BACKEND_CONFIG\",\n            \"value\": \"internal-l0-<yadda-yadda>.elb.amazonaws.com:6379\"\n        }\n    ],\n    ...  Now, we can create an updated deploy:  l0 deploy create Guestbook.Dockerrun.aws.json guestbook-dpl  We should see output like the following:  DEPLOY ID        DEPLOY NAME    VERSION\nguestbook-dpl.2  guestbook-dpl  2",
            "title": "Part 5: Update the Guestbook Deploy"
        },
        {
            "location": "/guides/walkthrough/deployment-2/#part-6-update-the-guestbook-service",
            "text": "Almost all the pieces are in place!\nNow we just need to apply the new Guestbook deploy to the running Guestbook service:  l0 service update guestbook-svc guestbook-dpl:latest  As the Guestbook service moves through the phases of its update process, we should see outputs like the following (if we keep an eye on the service with  l0 service get guestbook-svc , that is):  SERVICE ID    SERVICE NAME   ENVIRONMENT  LOADBALANCER  DEPLOYMENTS       SCALE\nguestbo5fadd  guestbook-svc  demo-env     guestbook-lb  guestbook-dpl:2*  1/1\n                                                        guestbook-dpl:1  above:  guestbook-dpl:2  is in a transitional state  SERVICE ID    SERVICE NAME   ENVIRONMENT  LOADBALANCER  DEPLOYMENTS      SCALE\nguestbo5fadd  guestbook-svc  demo-env     guestbook-lb  guestbook-dpl:2  2/1\n                                                        guestbook-dpl:1  above: both versions of the deployment are running at scale  SERVICE ID    SERVICE NAME   ENVIRONMENT  LOADBALANCER  DEPLOYMENTS       SCALE\nguestbo5fadd  guestbook-svc  demo-env     guestbook-lb  guestbook-dpl:2   1/1\n                                                        guestbook-dpl:1*  above:  guestbook-dpl:1  is in a transitional state  SERVICE ID    SERVICE NAME   ENVIRONMENT  LOADBALANCER  DEPLOYMENTS      SCALE\nguestbo5fadd  guestbook-svc  demo-env     guestbook-lb  guestbook-dpl:2  1/1  above:  guestbook-dpl:1  has been removed, and only  guestbook-dpl:2  remains",
            "title": "Part 6: Update the Guestbook Service"
        },
        {
            "location": "/guides/walkthrough/deployment-2/#part-7-prove-it",
            "text": "You should now be able to point your browser at the URL for the Guestbook load balancer (run  l0 loadbalancer get guestbook-lb  to find it) and see what looks like the same Guestbook application you deployed in the first section of the walkthrough.\nGo ahead and add a few entries, make sure it's functioning properly.\nWe'll wait.  Now, let's prove that we've actually separated the data from the application by deleting and redeploying the Guestbook application:  l0 service delete --wait guestbook-svc  (We'll leave the  deploy  intact so we can spin up a new service easily, and we'll leave the environment untouched because it also contained the Redis server.\nWe'll also pass the  --wait  flag so that we don't need to keep checking on the status of the job to know when it's complete.)  Once those resources have been deleted, we can recreate them!  Create another service, using the  guestbook-dpl  deploy we kept around:  l0 service create --loadbalancer demo-env:guestbook-lb demo-env guestbook-svc guestbook-dpl:latest  Wait for everything to spin up, and hit that new load balancer's url ( l0 loadbalancer get guestbook-lb ) with your browser.\nYour data should still be there!",
            "title": "Part 7: Prove It"
        },
        {
            "location": "/guides/walkthrough/deployment-2/#cleanup",
            "text": "When you're finished with the example, instruct Layer0 to delete the environment and terminate the application:  l0 environment delete demo-env",
            "title": "Cleanup"
        },
        {
            "location": "/guides/walkthrough/deployment-2/#deploy-with-terraform",
            "text": "As before, we can complete this deployment using Terraform and the Layer0 provider instead of the Layer0 CLI. As before, we will assume that you've cloned the  guides  repo and are working in the  walkthrough/deployment-2/  directory.  We'll use these files to manage our deployment with Terraform:     Filename  Purpose      main.tf  Provisions resources; populates variables in template files    outputs.tf  Values that Terraform will yield during deployment    terraform.tfstate  Tracks status of deployment  (created and managed by Terraform)    terraform.tfvars  Variables specific to the environment and application(s)    variables.tf  Values that Terraform will use during deployment",
            "title": "Deploy with Terraform"
        },
        {
            "location": "/guides/walkthrough/deployment-2/#tf-a-brief-aside-revisited",
            "text": "Not much is changed from  Deployment 1 .\nIn  main.tf , we pull in a new, second module that will deploy Redis for us.\nWe maintain this module as well; you can inspect  the repo  if you'd like.  In  main.tf  where we pull in the Guestbook module, you'll see that we're supplying more values than we did last time, because we need some additional configuration to let the Guestbook application use a Redis backend instead of its default in-memory storage.",
            "title": "*.tf: A Brief Aside: Revisited"
        },
        {
            "location": "/guides/walkthrough/deployment-2/#part-1-terraform-get",
            "text": "Run  terraform get  to pull down the source materials Terraform will use for deployment.\nThis will create a local  .terraform/  directory.",
            "title": "Part 1: Terraform Get"
        },
        {
            "location": "/guides/walkthrough/deployment-2/#part-2-terraform-init",
            "text": "This deployment has provider dependencies so an init call must be made. \n(Terraform v0.11~ requries init)\nAt the command prompt, execute the following command:  terraform init  We should see output like the following:  Initializing modules...\n- module.redis\n  Getting source \"github.com/quintilesims/redis//terraform\"\n- module.guestbook\n  Getting source \"github.com/quintilesims/guides//guestbook/module\"\n\nInitializing provider plugins...\n- Checking for available provider plugins on https://releases.hashicorp.com...\n- Downloading plugin for provider \"template\" (1.0.0)...\n\nThe following providers do not have any version constraints in configuration,\nso the latest version was installed.\n\nTo prevent automatic upgrades to new major versions that may contain breaking\nchanges, it is recommended to add version = \"...\" constraints to the\ncorresponding provider blocks in configuration, with the constraint strings\nsuggested below.\n\n* provider.template: version = \"~> 1.0\"\n\nTerraform has been successfully initialized!\n\nYou may now begin working with Terraform. Try running \"terraform plan\" to see\nany changes that are required for your infrastructure. All Terraform commands\nshould now work.\n\nIf you ever set or change modules or backend configuration for Terraform,\nrerun this command to reinitialize your working directory. If you forget, other\ncommands will detect it and remind you to do so if necessary.",
            "title": "Part 2: Terraform Init"
        },
        {
            "location": "/guides/walkthrough/deployment-2/#part-3-terraform-plan",
            "text": "It's always a good idea to find out what Terraform intends to do, so let's do that:  terraform plan  As before, we'll be prompted for any variables Terraform needs and doesn't have (see the note in  Deployment 1  for configuring Terraform variables).\nWe'll see output similar to the following:  Refreshing Terraform state in-memory prior to plan...\nThe refreshed state will be used to calculate this plan, but will not be\npersisted to local or remote state storage.\n\ndata.template_file.redis: Refreshing state...\nThe Terraform execution plan has been generated and is shown below.\nResources are shown in alphabetical order for quick scanning. Green resources\nwill be created (or destroyed and then created if an existing resource\nexists), yellow resources are being changed in-place, and red resources\nwill be destroyed. Cyan entries are data sources to be read.\n\nNote: You didn't specify an \"-out\" parameter to save this plan, so when\n\"apply\" is called, Terraform can't guarantee this is what will execute.\n\n+ layer0_environment.demo\n    ami:               \" <computed> \"\n    cluster_count:     \" <computed> \"\n    links:             \" <computed> \"\n    name:              \"demo\"\n    os:                \"linux\"\n    security_group_id: \" <computed> \"\n    size:              \"m3.medium\"\n\n+ module.redis.layer0_deploy.redis\n    content: \"{\\n    \\\"AWSEBDockerrunVersion\\\": 2,\\n    \\\"containerDefinitions\\\": [\\n        {\\n            \\\"name\\\": \\\"redis\\\",\\n            \\\"image\\\": \\\"redis:3.2-alpine\\\",\\n            \\\"essential\\\": true,\\n            \\\"memory\\\": 128,\\n            \\\"portMappings\\\": [\\n                {\\n                    \\\"hostPort\\\": 6379,\\n              \\\"containerPort\\\": 6379\\n                }\\n            ]\\n        }\\n    ]\\n}\\n\\n\"\n    name:    \"redis\"\n\n+ module.redis.layer0_load_balancer.redis\n    environment:                    \" ${ var . environment_id } \"\n    health_check.#:                 \" <computed> \"\n    name:                           \"redis\"\n    port.#:                         \"1\"\n    port.1072619732.certificate:    \"\"\n    port.1072619732.container_port: \"6379\"\n    port.1072619732.host_port:      \"6379\"\n    port.1072619732.protocol:       \"tcp\"\n    private:                        \"true\"\n    url:                            \" <computed> \"\n\n+ module.redis.layer0_service.redis\n    deploy:        \" ${   var . deploy_id   ==  \\ \" \\\"  ? layer0_deploy.redis.id : var.deploy_id  } \"\n    environment:   \" ${ var . environment_id } \"\n    load_balancer: \" ${ layer0_load_balancer . redis . id } \"\n    name:          \"redis\"\n    scale:         \"1\"\n    wait:          \"true\" < = module.guestbook.data.template_file.guestbook\n    rendered: \" <computed> \"\n    template: \"{\\n    \\\"AWSEBDockerrunVersion\\\": 2,\\n    \\\"containerDefinitions\\\": [\\n        {\\n            \\\"name\\\": \\\"guestbook\\\",\\n            \\\"image\\\": \\\"quintilesims/guestbook\\\",\\n            \\\"essential\\\": true,\\n       \\\"memory\\\": 128,\\n            \\\"environment\\\": [\\n                {\\n                    \\\"name\\\": \\\"GUESTBOOK_BACKEND_TYPE\\\",\\n                    \\\"value\\\": \\\" ${ backend_type } \\\"\\n                },\\n                {\\n                    \\\"name\\\": \\\"GUESTBOOK_BACKEND_CONFIG\\\",\\n                    \\\"value\\\": \\\" ${ backend_config } \\\"\\n                },\\n                {\\n                    \\\"name\\\": \\\"AWS_ACCESS_KEY_ID\\\",\\n  \\\"value\\\": \\\" ${ access_key } \\\"\\n                },\\n                {\\n                    \\\"name\\\": \\\"AWS_SECRET_ACCESS_KEY\\\",\\n                    \\\"value\\\": \\\" ${ secret_key } \\\"\\n                },\\n                {\\n            \\\"name\\\": \\\"AWS_REGION\\\",\\n                    \\\"value\\\": \\\" ${ region } \\\"\\n                }\\n   ],\\n            \\\"portMappings\\\": [\\n                {\\n                    \\\"hostPort\\\": 80,\\n     \\\"containerPort\\\": 80\\n                }\\n            ]\\n        }\\n    ]\\n}\\n\"\n    vars.%:   \" <computed> \"\n\n+ module.guestbook.layer0_deploy.guestbook\n    content: \" ${ data . template_file . guestbook . rendered } \"\n    name:    \"guestbook\"\n\n+ module.guestbook.layer0_load_balancer.guestbook\n    environment:                    \" ${ var . environment_id } \"\n    health_check.#:                 \" <computed> \"\n    name:                           \"guestbook\"\n    port.#:                         \"1\"\n    port.2027667003.certificate:    \"\"\n    port.2027667003.container_port: \"80\"\n    port.2027667003.host_port:      \"80\"\n    port.2027667003.protocol:       \"http\"\n    url:                            \" <computed> \"\n\n+ module.guestbook.layer0_service.guestbook\n    deploy:        \" ${   var . deploy_id   ==  \\ \" \\\"  ? layer0_deploy.guestbook.id : var.deploy_id  } \"\n    environment:   \" ${ var . environment_id } \"\n    load_balancer: \" ${ layer0_load_balancer . guestbook . id } \"\n    name:          \"guestbook\"\n    scale:         \"2\"\n    wait:          \"true\"\n\n\nPlan: 7 to add, 0 to change, 0 to destroy.  We should see that Terraform intends to add 7 new resources, some of which are for the Guestbook deployment and some of which are for the Redis deployment.",
            "title": "Part 3: Terraform Plan"
        },
        {
            "location": "/guides/walkthrough/deployment-2/#part-4-terraform-apply",
            "text": "Run  terraform apply , and we should see output similar to the following:  data.template_file.redis: Refreshing state...\nlayer0_deploy.redis-dpl: Creating...\n\n...\n...\n...\n\nlayer0_service.guestbook-svc: Creation complete\n\nApply complete! Resources: 7 added, 0 changed, 0 destroyed.\n\nThe state of your infrastructure has been saved to the path\nbelow. This state is required to modify and destroy your\ninfrastructure, so keep it safe. To inspect the complete state\nuse the `terraform show` command.\n\nState path: terraform.tfstate\n\nOutputs:\n\nguestbook_url = <http endpoint for the sample application>   Note  It may take a few minutes for the guestbook service to launch and the load balancer to become available.\nDuring that time you may get HTTP 503 errors when making HTTP requests against the load balancer URL.",
            "title": "Part 4: Terraform Apply"
        },
        {
            "location": "/guides/walkthrough/deployment-2/#whats-happening",
            "text": "Terraform provisions the AWS resources through Layer0, configures environment variables for the application, and deploys the application into a Layer0 environment.\nTerraform also writes the state of your deployment to the  terraform.tfstate  file (creating a new one if it's not already there).",
            "title": "What's Happening"
        },
        {
            "location": "/guides/walkthrough/deployment-2/#cleanup_1",
            "text": "When you're finished with the example, you can instruct Terraform to destroy the Layer0 environment, and terminate the application.\nExecute the following command (in the same directory):  terraform destroy  It's also now safe to remove the  .terraform/  directory and the  *.tfstate*  files.",
            "title": "Cleanup"
        },
        {
            "location": "/guides/one_off_task/",
            "text": "Deployment guide: Guestbook one-off task\n#\n\n\nIn this example, you will learn how to use layer0 to run a one-off task. A task is used to run a single instance of your Task Definition and is typically a short running job that will be stopped once finished.\n\n\n\n\nBefore you start\n#\n\n\nIn order to complete the procedures in this section, you must install and configure Layer0 v0.8.4 or later. If you have not already configured Layer0, see the \ninstallation guide\n. If you are running an older version of Layer0, see the \nupgrade instructions\n.\n\n\nPart 1: Prepare the task definition\n#\n\n\n\n\nDownload the \nGuestbook One-off Task Definition\n and save it to your computer as \nDockerrun.aws.json\n.\n\n\n\n\nPart 2: Create a deploy\n#\n\n\nNext, you will create a new deploy for the task using the \ndeploy create\n command. At the command prompt, run the following command:\n\n\nl0 deploy create Dockerrun.aws.json one-off-task-dpl\n\n\nYou will see the following output:\n\n\nDEPLOY ID           DEPLOY NAME        VERSION\none-off-task-dpl.1  one-off-task-dpl   1\n\n\n\n\n\nPart 3: Create the task\n#\n\n\nAt this point, you can use the \ntask create\n command to run a copy of the task.\n\n\nTo run the task, use the following command:\n\n\nl0 task create demo-env echo-tsk one-off-task-dpl:latest --wait\n\n\nYou will see the following output:\n\n\nTASK ID       TASK NAME         ENVIRONMENT  DEPLOY              SCALE\none-off851c9  echo-tsk          demo-env     one-off-task-dpl:1  0/1 (1)\n\n\n\n\n\nThe \nSCALE\n column shows the running, desired and pending counts. A value of \n0/1 (1)\n indicates that running = 0, desired = 1 and (1) for 1 pending task that is about to transition to running state. After your task has finished running, note that the desired count will remain 1 and pending value will no longer be shown, so the value will be \n0/1\n for a finished task.\n\n\nPart 4: Check the status of the task\n#\n\n\nTo view the logs for this task, and evaluate its progress, you can use the \ntask logs\n command:\n\n\nl0 task logs one-off-task-tsk\n  \n\n\nYou will see the following output:\n\n\nalpine\n\n\n------\n\nTask finished!\n\n\n\n\n\nYou can also use the following command for more information in the task.\n\n\nl0 -o json task get echo-tsk\n\n\nOutputs:\n\n\n[\n    {\n        \"copies\": [\n            {\n                \"details\": [],\n                \"reason\": \"Waiting for cluster capacity to run\",\n                \"task_copy_id\": \"\"\n            }\n        ],\n        \"deploy_id\": \"one-off-task-dpl.2\",\n        \"deploy_name\": \"one-off-task-dpl\",\n        \"deploy_version\": \"2\",\n        \"desired_count\": 1,\n        \"environment_id\": \"demoenv669e4\",\n        \"environment_name\": \"demo-env\",\n        \"pending_count\": 1,\n        \"running_count\": 0,\n        \"task_id\": \"echotsk1facd\",\n        \"task_name\": \"echo-tsk\"\n    }\n]\n\n\n\n\n\nAfter the task has finished, running \nl0 -o json task get echo-tsk\n will show a pending_count of 0.\n\n\nOutputs:\n\n\n...\n\"copies\": [\n    {\n        \"details\": [\n            {\n                \"container_name\": \"alpine\",\n                \"exit_code\": 0,\n                \"last_status\": \"STOPPED\",\n                \"reason\": \"\"\n            }\n        ],\n        \"reason\": \"Essential container in task exited\",\n        \"task_copy_id\": \"arn:aws:ecs:us-west-2:856306994068:task/0e723c3e-9cd1-4914-8393-b59abd40eb89\"\n    }\n],\n...\n\"pending_count\": 0,\n\"running_count\": 0,\n...",
            "title": "One-off Task"
        },
        {
            "location": "/guides/one_off_task/#deployment-guide-guestbook-one-off-task",
            "text": "In this example, you will learn how to use layer0 to run a one-off task. A task is used to run a single instance of your Task Definition and is typically a short running job that will be stopped once finished.",
            "title": "Deployment guide: Guestbook one-off task"
        },
        {
            "location": "/guides/one_off_task/#before-you-start",
            "text": "In order to complete the procedures in this section, you must install and configure Layer0 v0.8.4 or later. If you have not already configured Layer0, see the  installation guide . If you are running an older version of Layer0, see the  upgrade instructions .",
            "title": "Before you start"
        },
        {
            "location": "/guides/one_off_task/#part-1-prepare-the-task-definition",
            "text": "Download the  Guestbook One-off Task Definition  and save it to your computer as  Dockerrun.aws.json .",
            "title": "Part 1: Prepare the task definition"
        },
        {
            "location": "/guides/one_off_task/#part-2-create-a-deploy",
            "text": "Next, you will create a new deploy for the task using the  deploy create  command. At the command prompt, run the following command:  l0 deploy create Dockerrun.aws.json one-off-task-dpl  You will see the following output:  DEPLOY ID           DEPLOY NAME        VERSION\none-off-task-dpl.1  one-off-task-dpl   1",
            "title": "Part 2: Create a deploy"
        },
        {
            "location": "/guides/one_off_task/#part-3-create-the-task",
            "text": "At this point, you can use the  task create  command to run a copy of the task.  To run the task, use the following command:  l0 task create demo-env echo-tsk one-off-task-dpl:latest --wait  You will see the following output:  TASK ID       TASK NAME         ENVIRONMENT  DEPLOY              SCALE\none-off851c9  echo-tsk          demo-env     one-off-task-dpl:1  0/1 (1)  The  SCALE  column shows the running, desired and pending counts. A value of  0/1 (1)  indicates that running = 0, desired = 1 and (1) for 1 pending task that is about to transition to running state. After your task has finished running, note that the desired count will remain 1 and pending value will no longer be shown, so the value will be  0/1  for a finished task.",
            "title": "Part 3: Create the task"
        },
        {
            "location": "/guides/one_off_task/#part-4-check-the-status-of-the-task",
            "text": "To view the logs for this task, and evaluate its progress, you can use the  task logs  command:  l0 task logs one-off-task-tsk     You will see the following output:  alpine  ------ \nTask finished!  You can also use the following command for more information in the task.  l0 -o json task get echo-tsk  Outputs:  [\n    {\n        \"copies\": [\n            {\n                \"details\": [],\n                \"reason\": \"Waiting for cluster capacity to run\",\n                \"task_copy_id\": \"\"\n            }\n        ],\n        \"deploy_id\": \"one-off-task-dpl.2\",\n        \"deploy_name\": \"one-off-task-dpl\",\n        \"deploy_version\": \"2\",\n        \"desired_count\": 1,\n        \"environment_id\": \"demoenv669e4\",\n        \"environment_name\": \"demo-env\",\n        \"pending_count\": 1,\n        \"running_count\": 0,\n        \"task_id\": \"echotsk1facd\",\n        \"task_name\": \"echo-tsk\"\n    }\n]  After the task has finished, running  l0 -o json task get echo-tsk  will show a pending_count of 0.  Outputs:  ...\n\"copies\": [\n    {\n        \"details\": [\n            {\n                \"container_name\": \"alpine\",\n                \"exit_code\": 0,\n                \"last_status\": \"STOPPED\",\n                \"reason\": \"\"\n            }\n        ],\n        \"reason\": \"Essential container in task exited\",\n        \"task_copy_id\": \"arn:aws:ecs:us-west-2:856306994068:task/0e723c3e-9cd1-4914-8393-b59abd40eb89\"\n    }\n],\n...\n\"pending_count\": 0,\n\"running_count\": 0,\n...",
            "title": "Part 4: Check the status of the task"
        },
        {
            "location": "/reference/cli/",
            "text": "Layer0 CLI Reference\n#\n\n\nGlobal options\n#\n\n\nThe \nl0\n application is designed to be used with one of several commands: \nadmin\n, \ndeploy\n, \nenvironment\n, \njob\n, \nloadbalancer\n, \nservice\n, and \ntask\n. These commands are detailed in the sections below. There are, however, some global parameters that you may specify whenever using \nl0\n.\n\n\nUsage\n#\n\n\nl0\n \n[\nglobal\n \noptions\n]\n \ncommand\n \nsubcommand\n \n[\nsubcommand\n \noptions\n]\n \nparams\n\n\n\n\n\n\nGlobal options\n#\n\n\n\n\n-o [text|json], --output [text|json]\n - Specify the format of Layer0 outputs. By default, Layer0 outputs unformatted text; by issuing the \n--output json\n option, you can force \nl0\n to output JSON-formatted text.\n\n\n-t value, --timeout value\n - Specify the timeout for running \nl0\n commands. Values can be in h, m, s, or ms.\n\n\n-d, --debug\n - Print debug statements\n\n\n-v, --version\n - Display the version number of the \nl0\n application.\n\n\n\n\n\n\nAdmin\n#\n\n\nThe \nadmin\n command is used to manage the Layer0 API server. This command is used with the following subcommands: \ndebug\n, \nsql\n, and \nversion\n.\n\n\nadmin debug\n#\n\n\nUse the \ndebug\n subcommand to view the running version of your Layer0 API server and CLI.\n\n\nUsage\n#\n\n\nl0 admin debug\n\n\n\n\n\nadmin sql\n#\n\n\nUse the \nsql\n subcommand to initialize the Layer0 API database.\n\n\nUsage\n#\n\n\nl0 admin sql\n\n\n\n\n\nAdditional information\n#\n\n\nThe \nsql\n subcommand is automatically executed during the Layer0 installation process; we recommend that you do not use this subcommand unless specifically directed to do so.\n\n\nadmin version\n#\n\n\nUse the \nversion\n subcommand to display the current version of the Layer0 API.\n\n\nUsage\n#\n\n\nl0 admin version \n\n\n\n\n\n\n\nDeploy\n#\n\n\nDeploys are ECS Task Definitions. They are configuration files that detail how to deploy your application.\nThe \ndeploy\n command is used to manage Layer0 environments. This command is used with the following subcommands: \ncreate\n, \ndelete\n, \nget\n, and \nlist\n.\n\n\ndeploy create\n#\n\n\nUse the \ncreate\n subcommand to upload a Docker task definition into Layer0. \n\n\nUsage\n#\n\n\nl0 deploy create taskDefPath deployName\n\n\n\n\n\nRequired parameters\n#\n\n\n\n\ntaskDefPath\n - The path to the Docker task definition that you want to upload.\n\n\ndeployName\n - A name for the deploy.\n\n\n\n\nAdditional information\n#\n\n\nIf \ndeployName\n exactly matches the name of an existing Layer0 deploy, then the version number of that deploy will increase by 1, and the task definition you specified will replace the task definition specified in the previous version.\n\n\nIf you use Visual Studio to modify or create your Dockerrun file, you may see an \"Invalid Dockerrun.aws.json\" error. This error is caused by the default encoding used by Visual Studio. See the \n\"Common issues\" page\n for steps to resolve this issue.\n\n\nDeploys created through Layer0 are rendered with a \nlogConfiguration\n section for each container.\nIf a \nlogConfiguration\n section already exists, no changes are made to the section.\nThe additional section enables logs from each container to be sent to the the Layer0 log group.\nThis is where logs are looked up during \nl0 <entity> logs\n commands.\nThe added \nlogConfiguration\n section uses the following template:\n\n\n\"logConfiguration\": {\n    \"logDriver\": \"awslogs\",\n        \"options\": {\n            \"awslogs-group\": \"l0-<prefix>\",\n            \"awslogs-region\": \"<region>\",\n            \"awslogs-stream-prefix\": \"l0\"\n        }\n    }\n}\n\n\n\n\n\ndeploy delete\n#\n\n\nUse the \ndelete\n subcommand to delete a version of a Layer0 deploy.\n\n\nUsage\n#\n\n\nl0 deploy delete deployName\n\n\n\n\n\nRequired parameters\n#\n\n\n\n\ndeployName\n - The name of the Layer0 deploy you want to delete.\n\n\n\n\ndeploy get\n#\n\n\nUse the \nget\n subcommand to view information about an existing Layer0 deploy.\n\n\nUsage\n#\n\n\nl0 deploy get deployName\n\n\n\n\n\nRequired parameters\n#\n\n\n\n\ndeployName\n - The name of the Layer0 deploy for which you want to view additional information.\n\n\n\n\nAdditional information\n#\n\n\nThe \nget\n subcommand supports wildcard matching: \nl0 deploy get dep*\n would return all deploys beginning with \ndep\n.\n\n\ndeploy list\n#\n\n\nUse the \nlist\n subcommand to view a list of deploys in your instance of Layer0.\n\n\nUsage\n#\n\n\nl0 deploy list\n\n\n\n\n\n\n\nEnvironment\n#\n\n\nLayer0 environments allow you to isolate services and load balancers for specific applications.\nThe \nenvironment\n command is used to manage Layer0 environments. This command is used with the following subcommands: \ncreate\n, \ndelete\n, \nget\n, \nlist\n, and \nsetmincount\n.\n\n\nenvironment create\n#\n\n\nUse the \ncreate\n subcommand to create a new Layer0 environment.\n\n\nUsage\n#\n\n\nl0 environment create [--size size | --min-count mincount | \n    --user-data path | --os os | --ami amiID] environmentName\n\n\n\n\n\nRequired parameters\n#\n\n\n\n\nenvironmentName\n - A name for the environment.\n\n\n\n\nOptional arguments\n#\n\n\n\n\n--size size\n - The instance size of the EC2 instances to create in your environment (default: m3.medium).\n\n\n--min-count mincount\n - The minimum number of EC2 instances allowed in the environment's autoscaling group (default: 0).\n\n\n--user-data path\n - The user data template file to use for the environment's autoscaling group.\n\n\n--os os\n - The operating system used in the environment. Options are \"linux\" or \"windows\" (default: linux). More information on windows environments is documented below.\n\n\nami amiID\n - A custom EC2 AMI ID to use in the environment. If not specified, Layer0 will use its default AMI ID for the specified operating system.\n\n\n\n\nThe user data template can be used to add custom configuration to your Layer0 environment. They are usually scripts that are executed at instance launch time to ensure an EC2 instance is in the correct state after the provisioning process finishes.\nLayer0 uses \nGo Templates\n to render user data.\nCurrently, two variables are passed into the template: \nECSEnvironmentID\n and \nS3Bucket\n.\n\n\n\n\nDanger\n\n\nPlease review the \nECS Tutorial\n\nto better understand how to write a user data template, and use at your own risk!\n\n\n\n\nLinux Environments\n: The default Layer0 user data template is:\n\n\n#!/bin/bash\n\n\necho\n \nECS_CLUSTER\n={{\n .ECSEnvironmentID \n}}\n >> /etc/ecs/ecs.config\n\necho\n \nECS_ENGINE_AUTH_TYPE\n=\ndockercfg >> /etc/ecs/ecs.config\nyum install -y aws-cli awslogs jq\naws s3 cp s3://\n{{\n .S3Bucket \n}}\n/bootstrap/dockercfg dockercfg\n\ncfg\n=\n$(\ncat dockercfg\n)\n\n\necho\n \nECS_ENGINE_AUTH_DATA\n=\n$cfg\n >> /etc/ecs/ecs.config\ndocker pull amazon/amazon-ecs-agent:latest\nstart ecs\n\n\n\n\n\nWindows Environments\n: The default Layer0 user data template is:\n\n\n<\npowershell\n>\n\n\n# Set agent env variables for the Machine context (durable)\n\n\n$clusterName\n \n=\n \n\"{{ .ECSEnvironmentID }}\"\n\n\nWrite-Host\n \nCluster\n \nname\n \nset\n \nas\n:\n \n$clusterName\n \n-foreground\n \ngreen\n\n\n\n[Environment]\n::\nSetEnvironmentVariable\n(\n\"ECS_CLUSTER\"\n,\n \n$clusterName\n,\n \n\"Machine\"\n)\n\n\n[Environment]\n::\nSetEnvironmentVariable\n(\n\"ECS_ENABLE_TASK_IAM_ROLE\"\n,\n \n\"false\"\n,\n \n\"Machine\"\n)\n\n\n$agentVersion\n \n=\n \n'v1.5.2'\n\n\n$agentZipUri\n \n=\n \n\"https://s3.amazonaws.com/amazon-ecs-agent/ecs-agent-windows-$agentVersion.zip\"\n\n\n$agentZipMD5Uri\n \n=\n \n\"$agentZipUri.md5\"\n\n\n\n# Configure docker auth\n\n\nRead-S3Object\n \n-BucketName\n \n{{\n \n.\nS3Bucket\n \n}}\n \n-Key\n \nbootstrap\n/\ndockercfg\n \n-File\n \ndockercfg\n.\njson\n\n\n$dockercfgContent\n \n=\n \n[IO.File]\n::\nReadAllText\n(\n\"dockercfg.json\"\n)\n\n\n[Environment]\n::\nSetEnvironmentVariable\n(\n\"ECS_ENGINE_AUTH_DATA\"\n,\n \n$dockercfgContent\n,\n \n\"Machine\"\n)\n\n\n[Environment]\n::\nSetEnvironmentVariable\n(\n\"ECS_ENGINE_AUTH_TYPE\"\n,\n \n\"dockercfg\"\n,\n \n\"Machine\"\n)\n\n\n\n### --- Nothing user configurable after this point ---\n\n\n$ecsExeDir\n \n=\n \n\"$env:ProgramFiles\\Amazon\\ECS\"\n\n\n$zipFile\n \n=\n \n\"$env:TEMP\\ecs-agent.zip\"\n\n\n$md5File\n \n=\n \n\"$env:TEMP\\ecs-agent.zip.md5\"\n\n\n\n### Get the files from S3\n\n\nInvoke-RestMethod\n \n-OutFile\n \n$zipFile\n \n-Uri\n \n$agentZipUri\n\n\nInvoke-RestMethod\n \n-OutFile\n \n$md5File\n \n-Uri\n \n$agentZipMD5Uri\n\n\n\n## MD5 Checksum\n\n\n$expectedMD5\n \n=\n \n(\nGet-Content\n \n$md5File\n)\n\n\n$md5\n \n=\n \nNew-Object\n \n-TypeName\n \nSystem\n.\nSecurity\n.\nCryptography\n.\nMD5CryptoServiceProvider\n\n\n$actualMD5\n \n=\n \n[System.BitConverter]\n::\nToString\n(\n$md5\n.\nComputeHash\n(\n[System.IO.File]\n::\nReadAllBytes\n(\n$zipFile\n))).\nreplace\n(\n'-'\n,\n \n''\n)\n\n\nif\n(\n$expectedMD5\n \n-ne\n \n$actualMD5\n)\n \n{\n\n    \necho\n \n\"Download doesn't match hash.\"\n\n    \necho\n \n\"Expected: $expectedMD5 - Got: $actualMD5\"\n\n    \nexit\n \n1\n\n\n}\n\n\n\n## Put the executables in the executable directory.\n\n\nExpand-Archive\n \n-Path\n \n$zipFile\n \n-DestinationPath\n \n$ecsExeDir\n \n-Force\n\n\n\n## Start the agent script in the background.\n\n\n$jobname\n \n=\n \n\"ECS-Agent-Init\"\n\n\n$script\n \n=\n  \n\"cd '$ecsExeDir'; .\\amazon-ecs-agent.ps1\"\n\n\n$repeat\n \n=\n \n(\nNew-TimeSpan\n \n-Minutes\n \n1\n)\n\n\n$jobpath\n \n=\n \n$env:LOCALAPPDATA\n \n+\n \n\"\\Microsoft\\Windows\\PowerShell\\ScheduledJobs\\$jobname\\ScheduledJobDefinition.xml\"\n\n\n\nif\n($(\nTest-Path\n \n-Path\n \n$jobpath\n))\n \n{\n\n  \necho\n \n\"Job definition already present\"\n\n  \nexit\n \n0\n\n\n}\n\n\n\n$scriptblock\n \n=\n \n[scriptblock]\n::\nCreate\n(\n\"$script\"\n)\n\n\n$trigger\n \n=\n \nNew-JobTrigger\n \n-At\n \n(\nGet-Date\n).\nDate\n \n-RepeatIndefinitely\n \n-RepetitionInterval\n \n$repeat\n \n-Once\n\n\n$options\n \n=\n \nNew-ScheduledJobOption\n \n-RunElevated\n \n-ContinueIfGoingOnBattery\n \n-StartIfOnBattery\n\n\nRegister-ScheduledJob\n \n-Name\n \n$jobname\n \n-ScriptBlock\n \n$scriptblock\n \n-Trigger\n \n$trigger\n \n-ScheduledJobOption\n \n$options\n \n-RunNow\n\n\nAdd-JobTrigger\n \n-Name\n \n$jobname\n \n-Trigger\n \n(\nNew-JobTrigger\n \n-AtStartup\n \n-RandomDelay\n \n00\n:\n1\n:\n00\n)\n\n\n</\npowershell\n>\n\n\n<\npersist\n>\ntrue\n</\npersist\n>\n\n\n\n\n\n\n\n\nWindows Environments\nWindows containers are still in beta. \n\n\n\n\n\n\nYou can view the documented caveats with ECS \nhere\n.\nWhen creating Windows environments in Layer0, the root volume sizes for instances are 200GiB to accommodate the large size of the containers.\n\nIt can take as long as 45 minutes for a new windows container to come online. \n\n\nenvironment delete\n#\n\n\nUse the \ndelete\n subcommand to delete an existing Layer0 environment.\n\n\nUsage\n#\n\n\nl0 environment delete [--wait] environmentName\n\n\n\n\n\nRequired parameters\n#\n\n\n\n\nenvironmentName\n - The name of the Layer0 environment that you want to delete.\n\n\n\n\nOptional arguments\n#\n\n\n\n\n--wait\n - Wait until the deletion is complete before exiting.\n\n\n\n\nAdditional information\n#\n\n\nThis operation performs several tasks asynchronously. When run without the \n--wait\n option, this operation will most likely exit before all of these tasks are complete; when run with the \n--wait\n option, this operation will only exit once these tasks have completed.\n\n\nenvironment get\n#\n\n\nUse the \nget\n subcommand to display information about an existing Layer0 environment.\n\n\nUsage\n#\n\n\nl0 environment get environmentName\n\n\n\n\n\nRequired parameters\n#\n\n\n\n\nenvironmentName\n - The name of the Layer0 environment for which you want to view additional information.\n\n\n\n\nAdditional information\n#\n\n\nThe \nget\n subcommand supports wildcard matching: \nl0 environment get test*\n would return all environments beginning with \ntest\n.\n\n\nenvironment list\n#\n\n\nUse the \nlist\n subcommand to display a list of environments in your instance of Layer0.\n\n\nUsage\n#\n\n\nl0 environment list\n\n\n\n\n\nenvironment setmincount\n#\n\n\nUse the \nsetmincount\n subcommand to set the minimum number of EC2 instances allowed the environment's autoscaling group.\n\n\nUsage\n#\n\n\nl0 environment setmincount environmentName count\n\n\n\n\n\nRequired parameters\n#\n\n\n\n\nenvironmentName\n - The name of the Layer0 environment that you want to adjust.\n\n\ncount\n - The minimum number of instances allowed in the environment's autoscaling group.\n\n\n\n\nenvironment link\n#\n\n\nUse the \nlink\n subcommand to link two environments together. \nWhen environments are linked, services inside the environments are allowed to communicate with each other as if they were in the same environment. \nThis link is bidirectional. \nThis command is idempotent; it will succeed even if the two specified environments are already linked.\n\n\nUsage\n#\n\n\nl0 environment link sourceEnvironmentName destEnvironmentName\n\n\n\n\n\nRequired parameters\n#\n\n\n\n\nsourceEnvironmentName\n - The name of the source environment to link.\n\n\ndestEnvironmentName\n - The name of the destination environment to link.\n\n\n\n\nenvironment unlink\n#\n\n\nUse the \nunlink\n subcommand to remove the link between two environments.\nThis command is idempotent; it will succeed even if the link does not exist.\n\n\nUsage\n#\n\n\nl0 environment unlink sourceEnvironmentName destEnvironmentName\n\n\n\n\n\nRequired parameters\n#\n\n\n\n\nsourceEnvironmentName\n - The name of the source environment to unlink.\n\n\ndestEnvironmentName\n - The name of the destination environment to unlink.\n\n\n\n\n\n\nJob\n#\n\n\nA Job is a long-running unit of work performed on behalf of the Layer0 API.\nJobs are executed as Layer0 tasks that run in the \napi\n environment.\nThe \njob\n command is used with the following subcommands: \nlogs\n, \ndelete\n, \nget\n, and \nlist\n.\n\n\njob logs\n#\n\n\nUse the \nlogs\n subcommand to display the logs from a Layer0 job that is currently running.\n\n\nUsage\n#\n\n\nl0 job logs [--start MM/DD HH:MM | --end MM/DD HH:MM | --tail=N] jobName\n\n\n\n\n\nRequired parameters\n#\n\n\n\n\njobName\n - The name of the Layer0 job for which you want to view logs.\n\n\n\n\nOptional arguments\n#\n\n\n\n\n--start MM/DD HH:MM\n - The start of the time range to fetch logs.\n\n\n--end MM/DD HH:MM\n - The end of the time range to fetch logs.\n\n\n--tail=N\n - Display only the last \nN\n lines of the log.\n\n\n\n\njob delete\n#\n\n\nUse the \ndelete\n subcommand to delete an existing job.\n\n\nUsage\n#\n\n\nl0 job delete jobName\n\n\n\n\n\nRequired parameters\n#\n\n\n\n\njobName\n - The name of the job that you want to delete.\n\n\n\n\njob get\n#\n\n\nUse the \nget\n subcommand to display information about an existing Layer0 job.\n\n\nUsage\n#\n\n\nl0 job get jobName\n\n\n\n\n\nRequired parameters\n#\n\n\n\n\njobName\n - The name of an existing Layer0 job to display.\n\n\n\n\nAdditional information\n#\n\n\nThe \nget\n subcommand supports wildcard matching: \nl0 job get 2a55*\n would return all jobs beginning with \n2a55\n.\n\n\njob list\n#\n\n\nUse the \nlist\n subcommand to display information about all of the existing jobs in an instance of Layer0.\n\n\nUsage\n#\n\n\nl0 job list\n\n\n\n\n\n\n\nLoad Balancer\n#\n\n\nA load balancer is a component of a Layer0 environment. Load balancers listen for traffic on certain ports, and then forward that traffic to Layer0 \nservices\n. The \nloadbalancer\n command is used with the following subcommands: \ncreate\n, \ndelete\n, \naddport\n, \ndropport\n, \nget\n, \nlist\n, and \nhealthcheck\n.\n\n\nloadbalancer create\n#\n\n\nUse the \ncreate\n subcommand to create a new load balancer.\n\n\nUsage\n#\n\n\nl0 loadbalancer create [--port port ... | --certificate certifiateName | \n    --private | --healthcheck-target target | --healthcheck-interval interval | \n    --healthcheck-timeout timeout | --healthcheck-healthy-threshold healthyThreshold | \n    --healthcheck-unhealthy-threshold unhealthyThreshold] environmentName loadBalancerName\n\n\n\n\n\nRequired parameters\n#\n\n\n\n\nenvironmentName\n - The name of the existing Layer0 environment in which you want to create the load balancer.\n\n\nloadBalancerName\n - A name for the load balancer you are creating.\n\n\n\n\nOptional arguments\n#\n\n\n\n\n--port port ...\n - The port configuration for the listener of the load balancer. Valid pattern is \nhostPort:containerPort/protocol\n. Multiple ports can be specified using \n--port port1 --port port2 ...\n (default: \n80/80:TCP\n).\n\n\nhostPort\n - The port that the load balancer will listen for traffic on.\n\n\ncontainerPort\n - The port that the load balancer will forward traffic to.\n\n\nprotocol\n - The protocol to use when forwarding traffic (acceptable values: TCP, SSL, HTTP, and HTTPS).\n\n\n\n\n\n\n--certificate certificateName\n - The name of an existing Layer0 certificate. You must include this option if you are using an HTTPS port configuration.\n\n\n--private\n - When you use this option, the load balancer will only be accessible from within the Layer0 environment.\n\n\n--healthcheck-target target\n - The target of the check. Valid pattern is \nPROTOCOL:PORT/PATH\n (default: \n\"TCP:80\"\n). \n\n\nIf \nPROTOCOL\n is \nHTTP\n or \nHTTPS\n, both \nPORT\n and \nPATH\n are required. Example: \nHTTP:80/admin/healthcheck\n. \n\n\nIf \nPROTOCOL\n is \nTCP\n or \nSSL\n, \nPORT\n is required and \nPATH\n is not used. Example: \nTCP:80\n\n\n\n\n\n\n--healthcheck-interval interval\n - The interval between checks (default: \n30\n).\n\n\n--healthcheck-timeout timeout\n - The length of time before the check times out (default: \n5\n).\n\n\n--healthcheck-healthy-threshold healthyThreshold\n - The number of checks before the instance is declared healthy (default: \n2\n).\n\n\n--healthcheck-unhealthy-threshold unhealthyThreshold\n - The number of checks before the instance is declared unhealthy (default: \n2\n).\n\n\n\n\n\n\nPorts and Health Checks\n\n\nWhen both the \n--port\n and the \n--healthcheck-target\n options are omitted, Layer0 configures the load balancer with some default values: \n80:80/TCP\n for ports and \nTCP:80\n for healthcheck target.\nThese default values together create a load balancer configured with a simple but functioning health check, opening up a set of ports that allows traffic to the target of the healthcheck.\n(\n--healthcheck-target TCP:80\n tells the load balancer to ping its services at port 80 to determine their status, and \n--port 80:80/TCP\n configures a security group to allow traffic to pass between port 80 of the load balancer and port 80 of its services)\n\n\nWhen creating a load balancer with non-default configurations for either \n--port\n or \n--healthcheck-target\n, make sure that a valid \n--port\n and \n--healthcheck-target\n pairing is also created.\n\n\n\n\nloadbalancer delete\n#\n\n\nUse the \ndelete\n subcommand to delete an existing load balancer.\n\n\nUsage\n#\n\n\nl0 loadbalancer delete [--wait] loadBalancerName\n\n\n\n\n\nRequired parameters\n#\n\n\n\n\nloadBalancerName\n - The name of the load balancer that you want to delete.\n\n\n\n\nOptional arguments\n#\n\n\n\n\n--wait\n - Wait until the deletion is complete before exiting.\n\n\n\n\nAdditional information\n#\n\n\nIn order to delete a load balancer that is already attached to a service, you must first delete the service that uses the load balancer.\n\n\nThis operation performs several tasks asynchronously. When run without the \n--wait\n option, this operation will most likely exit before all of these tasks are complete; when run with the \n--wait\n option, this operation will only exit once these tasks have completed\n.\n\n\nloadbalancer addport\n#\n\n\nUse the \naddport\n subcommand to add a new port configuration to an existing Layer0 load balancer.\n\n\nUsage\n#\n\n\nl0 loadbalancer addport [--certificate certificateName] loadBalancerName port\n\n\n\n\n\nRequired parameters\n#\n\n\n\n\nloadBalancerName\n - The name of an existing Layer0 load balancer in which you want to add the port configuration.\n\n\nport\n - The port configuration for the listener of the load balancer. Valid pattern is \nhostPort:containerPort/protocol\n.\n\n\nhostPort\n - The port that the load balancer will listen for traffic on.\n\n\ncontainerPort\n - The port that the load balancer will forward traffic to.\n\n\nprotocol\n - The protocol to use when forwarding traffic (acceptable values: TCP, SSL, HTTP, and HTTPS).\n\n\n\n\n\n\n\n\nOptional arguments\n#\n\n\n\n\n--certificate certificateName\n - The name of an existing Layer0 certificate. You must include this option if you are using an HTTPS port configuration.\n\n\n\n\nAdditional information\n#\n\n\nThe port configuration you specify must not already be in use by the load balancer you specify.\n\n\nloadbalancer dropport\n#\n\n\nUse the \ndropport\n subcommand to remove a port configuration from an existing Layer0 load balancer.\n\n\nUsage\n#\n\n\nl0 loadbalancer dropport loadBalancerName hostPort\n\n\n\n\n\nRequired parameters\n#\n\n\n\n\nloadBalancerName\n- The name of an existing Layer0 load balancer from which you want to remove the port configuration.\n\n\nhostPort\n- The host port to remove from the load balancer.\n\n\n\n\nloadbalancer get\n#\n\n\nUse the \nget\n subcommand to display information about an existing Layer0 load balancer.\n\n\nUsage\n#\n\n\nl0 loadbalancer get [environmentName:]loadBalancerName\n\n\n\n\n\nRequired parameters\n#\n\n\n\n\n[environmentName:]loadBalancerName\n - The name of an existing Layer0 load balancer. You can optionally provide the Layer0 environment (\nenvironmentName\n) associated with the Load Balancer\n\n\n\n\nAdditional information\n#\n\n\nThe \nget\n subcommand supports wildcard matching: \nl0 loadbalancer get entrypoint*\n would return all jobs beginning with \nentrypoint\n.\n\n\nloadbalancer list\n#\n\n\nUse the \nlist\n subcommand to display information about all of the existing load balancers in an instance of Layer0.\n\n\nUsage\n#\n\n\nl0 loadbalancer list\n\n\n\n\n\nloadbalancer healthcheck\n#\n\n\nUse the \nhealthcheck\n subcommand to display information about or update the configuration of a load balancer's health check.\n\n\nUsage\n#\n\n\nl0 loadbalancer healthcheck [--set-target target | --set-interval interval | \n    --set-timeout timeout | --set-healthy-threshold healthyThreshold | \n    --set-unhealthy-threshold unhealthyThreshold] loadbalancerName\n\n\n\n\n\nRequired parameters\n#\n\n\n\n\nloadBalancerName\n - The name of the existing Layer0 load balancer you are modifying.\n\n\n\n\nOptional arguments\n#\n\n\n\n\n--set-target target\n - The target of the check. Valid pattern is \nPROTOCOL:PORT/PATH\n.\n\n\nIf \nPROTOCOL\n is \nHTTP\n or \nHTTPS\n, both \nPORT\n and \nPATH\n are required. Example: \nHTTP:80/admin/healthcheck\n.\n\n\nIf \nPROTOCOL\n is \nTCP\n or \nSSL\n, \nPORT\n is required and \nPATH\n is not used. Example: \nTCP:80\n\n\n\n\n\n\n--set-interval interval\n - The interval between health checks.\n\n\n--set-timeout timeout\n - The length of time in seconds before the health check times out.\n\n\n--set-healthy-threshold healthyThreshold\n - The number of checks before the instance is declared healthy.\n\n\n--set-unhealthy-threshold unhealthyThreshold\n - The number of checks before the instance is declared unhealthy.\n\n\n\n\nAdditional information\n#\n\n\nCalling the subcommand without flags will display the current configuration of the load balancer's health check. Setting any of the flags will update the corresponding field in the health check, and all omitted flags will leave the corresponding fields unchanged.\n\n\n\n\nService\n#\n\n\nA service is a component of a Layer0 environment. The purpose of a service is to execute a Docker image specified in a \ndeploy\n. In order to create a service, you must first create an \nenvironment\n and a \ndeploy\n; in most cases, you should also create a \nload balancer\n before creating the service.\n\n\nThe \nservice\n command is used with the following subcommands: \ncreate\n, \ndelete\n, \nget\n, \nupdate\n, \nlist\n, \nlogs\n, and \nscale\n.\n\n\nservice create\n#\n\n\nUse the \ncreate\n subcommand to create a Layer0 service.\n\n\nUsage\n#\n\n\nl0 service create [--loadbalancer [environmentName:]loadBalancerName | \n    --no-logs] environmentName serviceName deployName[:deployVersion]\n\n\n\n\n\nRequired parameters\n#\n\n\n\n\nserviceName\n - A name for the service that you are creating.\n\n\nenvironmentName\n - The name of an existing Layer0 environment.\n\n\ndeployName[:deployVersion]\n - The name of a Layer0 deploy that exists in the environment \nenvironmentName\n. You can optionally specify the version number of the Layer0 deploy that you want to deploy. If you do not specify a version number, the latest version of the deploy will be used.\n\n\n\n\nOptional arguments\n#\n\n\n\n\n--loadbalancer [environmentName:]loadBalancerName\n - Place the new service behind an existing load balancer \nloadBalancerName\n. You can optionally specify the Layer0 environment (\nenvironmentName\n) where the load balancer exists.\n\n\n--no-logs\n - Disable cloudwatch logging for the service\n\n\n\n\nservice update\n#\n\n\nUse the \nupdate\n subcommand to apply an existing Layer0 Deploy to an existing Layer0 service.\n\n\nUsage\n#\n\n\nl0 service update [--no-logs] [environmentName:]serviceName deployName[:deployVersion]\n\n\n\n\n\nRequired parameters\n#\n\n\n\n\n[environmentName:]serviceName\n - The name of an existing Layer0 service into which you want to apply the deploy. You can optionally specify the Layer0 environment (\nenvironmentName\n) of the service.\n\n\ndeployName[:deployVersion]\n - The name of the Layer0 deploy that you want to apply to the service. You can optionally specify a specific version of the deploy (\ndeployVersion\n). If you do not specify a version number, the latest version of the deploy will be applied.\n\n\n\n\nOptional arguments\n#\n\n\n\n\n--no-logs\n - Disable cloudwatch logging for the service\n\n\n\n\nAdditional information\n#\n\n\nIf your service uses a load balancer, when you update the task definition for the service, the container name and container port that were specified when the service was created must remain the same in the task definition. In other words, if your service has a load balancer, you cannot apply any deploy you want to that service. If you are varying the container name or exposed ports, you must create a new service instead.\n\n\nservice delete\n#\n\n\nUse the \ndelete\n subcommand to delete an existing Layer0 service.\n\n\nUsage\n#\n\n\nl0 service delete [--wait] [environmentName:]serviceName\n\n\n\n\n\nRequired parameters\n#\n\n\n\n\n[environmentName:]serviceName\n - The name of the Layer0 service that you want to delete. You can optionally provide the Layer0 environment (\nenvironmentName\n) of the service.\n\n\n\n\nOptional arguments\n#\n\n\n\n\n--wait\n - Wait until the deletion is complete before exiting.\n\n\n\n\nAdditional information\n#\n\n\nThis operation performs several tasks asynchronously. When run without the \n--wait\n option, this operation will most likely exit before all of these tasks are complete; when run with the \n--wait\n option, this operation will only exit once these tasks have completed.\n\n\nservice get\n#\n\n\nUse the \nget\n subcommand to display information about an existing Layer0 service.\n\n\nUsage\n#\n\n\nl0 service get [environmentName:]serviceName\n\n\n\n\n\nRequired parameters\n#\n\n\n\n\n[environmentName:]serviceName\n - The name of an existing Layer0 service. You can optionally provide the Layer0 environment (\nenvironmentName\n) of the service.\n\n\n\n\nservice list\n#\n\n\nUse the \nlist\n subcommand to list all of the existing services in your Layer0 instance.\n\n\nUsage\n#\n\n\nl0 service get list\n\n\n\n\n\nservice logs\n#\n\n\nUse the \nlogs\n subcommand to display the logs from a Layer0 service that is currently running.\n\n\nUsage\n#\n\n\nl0 service logs [--start MM/DD HH:MM | --end MM/DD HH:MM | --tail=N] serviceName\n\n\n\n\n\nRequired parameters\n#\n\n\n\n\nserviceName\n - The name of the Layer0 service for which you want to view logs.\n\n\n\n\nOptional arguments\n#\n\n\n\n\n--start MM/DD HH:MM\n - The start of the time range to fetch logs.\n\n\n--end MM/DD HH:MM\n - The end of the time range to fetch logs.\n\n\n--tail=N\n - Display only the last \nN\n lines of the log.\n\n\n\n\nservice scale\n#\n\n\nUse the \nscale\n subcommand to specify how many copies of an existing Layer0 service should run.\n\n\nUsage\n#\n\n\nl0 service scale [environmentName:]serviceName copies\n\n\n\n\n\nRequired parameters\n#\n\n\n\n\n[environmentName:]serviceName\n - The name of the Layer0 service that you want to scale up. You can optionally provide the Layer0 environment (\nenvironmentName\n) of the service.\n\n\ncopies\n - The number of copies of the specified service that should be run.\n\n\n\n\n\n\nTask\n#\n\n\nA Layer0 task is a component of an environment. A task executes the contents of a Docker image, as specified in a deploy. A task differs from a service in that a task does not restart after exiting. Additionally, ports are not exposed when using a task.\n\n\nThe \ntask\n command is used with the following subcommands: \ncreate\n, \ndelete\n, \nget\n, \nlist\n, and \nlogs\n.\n\n\ntask create\n#\n\n\nUse the \ncreate\n subcommand to create a Layer0 task.\n\n\nUsage\n#\n\n\nl0 task create [--copies copies | --no-logs] environmentName taskName deployName\n\n\n\n\n\nRequired parameters\n#\n\n\n\n\nenvironmentName\n - The name of the existing Layer0 environment in which you want to create the task.\n\n\ntaskName\n - A name for the task.\n\n\ndeployName\n - The name of an existing Layer0 deploy that the task should use.\n\n\n\n\nOptional arguments\n#\n\n\n\n\n--copies copies\n - The number of copies of the task to run (default: 1).\n\n\n--no-logs\n - Disable cloudwatch logging for the service.\n\n\n\n\ntask delete\n#\n\n\nUse the \ndelete\n subcommand to delete an existing Layer0 task.\n\n\nUsage\n#\n\n\nl0 task delete [environmentName:]taskName\n\n\n\n\n\nRequired parameters\n#\n\n\n\n\n[environmentName:]taskName\n - The name of the Layer0 task that you want to delete. You can optionally specify the name of the Layer0 environment that contains the task. This parameter is only required if mulitiple environments contain tasks with exactly the same name.\n\n\n\n\nAdditional information\n#\n\n\nUntil the record has been purged, the API may indicate that the task is still running. Task records are typically purged within an hour.\n\n\ntask get\n#\n\n\nUse the \nget\n subcommand to display information about an existing Layer0 task (\ntaskName\n).\n\n\nUsage\n#\n\n\nl0 task get [environmentName:]taskName\n\n\n\n\n\nRequired parameters\n#\n\n\n\n\n[environmentName:]taskName\n - The name of a Layer0 task for which you want to see information. You can optionally specify the name of the Layer0 Environment that contains the task.\n\n\n\n\nAdditional information\n#\n\n\nThe value of \ntaskName\n does not need to exactly match the name of an existing task. If multiple results are found that match the pattern you specified in \ntaskName\n, then information about all matching tasks will be returned.\n\n\ntask list\n#\n\n\nUse the \ntask\n subcommand to display a list of running tasks in your Layer0.\n\n\nUsage\n#\n\n\nl0 task list\n\n\n\n\n\ntask logs\n#\n\n\nUse the \nlogs\n subcommand to display logs for a running Layer0 task.\n\n\nUsage\n#\n\n\nl0 task logs [--start MM/DD HH:MM | --end MM/DD HH:MM | --tail=N] taskName\n\n\n\n\n\nRequired parameters\n#\n\n\n\n\ntaskName\n - The name of an existing Layer0 task.\n\n\n\n\nOptional arguments\n#\n\n\n\n\n--start MM/DD HH:MM\n - The start of the time range to fetch logs.\n\n\n--end MM/DD HH:MM\n - The end of the time range to fetch logs.\n\n\n--tail=N\n - Display only the last \nN\n lines of the log.\n\n\n\n\nAdditional information\n#\n\n\nThe value of \ntaskName\n does not need to exactly match the name of an existing task. If multiple results are found that match the pattern you specified in \ntaskName\n, then information about all matching tasks will be returned.\n\n\ntask list\n#\n\n\nUse the \nlist\n subcommand to display a list of running tasks in your Layer0.\n\n\nUsage\n#\n\n\nl0 task list",
            "title": "Layer0 CLI"
        },
        {
            "location": "/reference/cli/#layer0-cli-reference",
            "text": "",
            "title": "Layer0 CLI Reference"
        },
        {
            "location": "/reference/cli/#global-options",
            "text": "The  l0  application is designed to be used with one of several commands:  admin ,  deploy ,  environment ,  job ,  loadbalancer ,  service , and  task . These commands are detailed in the sections below. There are, however, some global parameters that you may specify whenever using  l0 .",
            "title": "Global options"
        },
        {
            "location": "/reference/cli/#usage",
            "text": "l0   [ global   options ]   command   subcommand   [ subcommand   options ]   params",
            "title": "Usage"
        },
        {
            "location": "/reference/cli/#global-options_1",
            "text": "-o [text|json], --output [text|json]  - Specify the format of Layer0 outputs. By default, Layer0 outputs unformatted text; by issuing the  --output json  option, you can force  l0  to output JSON-formatted text.  -t value, --timeout value  - Specify the timeout for running  l0  commands. Values can be in h, m, s, or ms.  -d, --debug  - Print debug statements  -v, --version  - Display the version number of the  l0  application.",
            "title": "Global options"
        },
        {
            "location": "/reference/cli/#admin",
            "text": "The  admin  command is used to manage the Layer0 API server. This command is used with the following subcommands:  debug ,  sql , and  version .",
            "title": "Admin"
        },
        {
            "location": "/reference/cli/#admin-debug",
            "text": "Use the  debug  subcommand to view the running version of your Layer0 API server and CLI.",
            "title": "admin debug"
        },
        {
            "location": "/reference/cli/#usage_1",
            "text": "l0 admin debug",
            "title": "Usage"
        },
        {
            "location": "/reference/cli/#admin-sql",
            "text": "Use the  sql  subcommand to initialize the Layer0 API database.",
            "title": "admin sql"
        },
        {
            "location": "/reference/cli/#usage_2",
            "text": "l0 admin sql",
            "title": "Usage"
        },
        {
            "location": "/reference/cli/#additional-information",
            "text": "The  sql  subcommand is automatically executed during the Layer0 installation process; we recommend that you do not use this subcommand unless specifically directed to do so.",
            "title": "Additional information"
        },
        {
            "location": "/reference/cli/#admin-version",
            "text": "Use the  version  subcommand to display the current version of the Layer0 API.",
            "title": "admin version"
        },
        {
            "location": "/reference/cli/#usage_3",
            "text": "l0 admin version",
            "title": "Usage"
        },
        {
            "location": "/reference/cli/#deploy",
            "text": "Deploys are ECS Task Definitions. They are configuration files that detail how to deploy your application.\nThe  deploy  command is used to manage Layer0 environments. This command is used with the following subcommands:  create ,  delete ,  get , and  list .",
            "title": "Deploy"
        },
        {
            "location": "/reference/cli/#deploy-create",
            "text": "Use the  create  subcommand to upload a Docker task definition into Layer0.",
            "title": "deploy create"
        },
        {
            "location": "/reference/cli/#usage_4",
            "text": "l0 deploy create taskDefPath deployName",
            "title": "Usage"
        },
        {
            "location": "/reference/cli/#required-parameters",
            "text": "taskDefPath  - The path to the Docker task definition that you want to upload.  deployName  - A name for the deploy.",
            "title": "Required parameters"
        },
        {
            "location": "/reference/cli/#additional-information_1",
            "text": "If  deployName  exactly matches the name of an existing Layer0 deploy, then the version number of that deploy will increase by 1, and the task definition you specified will replace the task definition specified in the previous version.  If you use Visual Studio to modify or create your Dockerrun file, you may see an \"Invalid Dockerrun.aws.json\" error. This error is caused by the default encoding used by Visual Studio. See the  \"Common issues\" page  for steps to resolve this issue.  Deploys created through Layer0 are rendered with a  logConfiguration  section for each container.\nIf a  logConfiguration  section already exists, no changes are made to the section.\nThe additional section enables logs from each container to be sent to the the Layer0 log group.\nThis is where logs are looked up during  l0 <entity> logs  commands.\nThe added  logConfiguration  section uses the following template:  \"logConfiguration\": {\n    \"logDriver\": \"awslogs\",\n        \"options\": {\n            \"awslogs-group\": \"l0-<prefix>\",\n            \"awslogs-region\": \"<region>\",\n            \"awslogs-stream-prefix\": \"l0\"\n        }\n    }\n}",
            "title": "Additional information"
        },
        {
            "location": "/reference/cli/#deploy-delete",
            "text": "Use the  delete  subcommand to delete a version of a Layer0 deploy.",
            "title": "deploy delete"
        },
        {
            "location": "/reference/cli/#usage_5",
            "text": "l0 deploy delete deployName",
            "title": "Usage"
        },
        {
            "location": "/reference/cli/#required-parameters_1",
            "text": "deployName  - The name of the Layer0 deploy you want to delete.",
            "title": "Required parameters"
        },
        {
            "location": "/reference/cli/#deploy-get",
            "text": "Use the  get  subcommand to view information about an existing Layer0 deploy.",
            "title": "deploy get"
        },
        {
            "location": "/reference/cli/#usage_6",
            "text": "l0 deploy get deployName",
            "title": "Usage"
        },
        {
            "location": "/reference/cli/#required-parameters_2",
            "text": "deployName  - The name of the Layer0 deploy for which you want to view additional information.",
            "title": "Required parameters"
        },
        {
            "location": "/reference/cli/#additional-information_2",
            "text": "The  get  subcommand supports wildcard matching:  l0 deploy get dep*  would return all deploys beginning with  dep .",
            "title": "Additional information"
        },
        {
            "location": "/reference/cli/#deploy-list",
            "text": "Use the  list  subcommand to view a list of deploys in your instance of Layer0.",
            "title": "deploy list"
        },
        {
            "location": "/reference/cli/#usage_7",
            "text": "l0 deploy list",
            "title": "Usage"
        },
        {
            "location": "/reference/cli/#environment",
            "text": "Layer0 environments allow you to isolate services and load balancers for specific applications.\nThe  environment  command is used to manage Layer0 environments. This command is used with the following subcommands:  create ,  delete ,  get ,  list , and  setmincount .",
            "title": "Environment"
        },
        {
            "location": "/reference/cli/#environment-create",
            "text": "Use the  create  subcommand to create a new Layer0 environment.",
            "title": "environment create"
        },
        {
            "location": "/reference/cli/#usage_8",
            "text": "l0 environment create [--size size | --min-count mincount | \n    --user-data path | --os os | --ami amiID] environmentName",
            "title": "Usage"
        },
        {
            "location": "/reference/cli/#required-parameters_3",
            "text": "environmentName  - A name for the environment.",
            "title": "Required parameters"
        },
        {
            "location": "/reference/cli/#optional-arguments",
            "text": "--size size  - The instance size of the EC2 instances to create in your environment (default: m3.medium).  --min-count mincount  - The minimum number of EC2 instances allowed in the environment's autoscaling group (default: 0).  --user-data path  - The user data template file to use for the environment's autoscaling group.  --os os  - The operating system used in the environment. Options are \"linux\" or \"windows\" (default: linux). More information on windows environments is documented below.  ami amiID  - A custom EC2 AMI ID to use in the environment. If not specified, Layer0 will use its default AMI ID for the specified operating system.   The user data template can be used to add custom configuration to your Layer0 environment. They are usually scripts that are executed at instance launch time to ensure an EC2 instance is in the correct state after the provisioning process finishes.\nLayer0 uses  Go Templates  to render user data.\nCurrently, two variables are passed into the template:  ECSEnvironmentID  and  S3Bucket .   Danger  Please review the  ECS Tutorial \nto better understand how to write a user data template, and use at your own risk!   Linux Environments : The default Layer0 user data template is:  #!/bin/bash  echo   ECS_CLUSTER ={{  .ECSEnvironmentID  }}  >> /etc/ecs/ecs.config echo   ECS_ENGINE_AUTH_TYPE = dockercfg >> /etc/ecs/ecs.config\nyum install -y aws-cli awslogs jq\naws s3 cp s3:// {{  .S3Bucket  }} /bootstrap/dockercfg dockercfg cfg = $( cat dockercfg )  echo   ECS_ENGINE_AUTH_DATA = $cfg  >> /etc/ecs/ecs.config\ndocker pull amazon/amazon-ecs-agent:latest\nstart ecs  Windows Environments : The default Layer0 user data template is:  < powershell >  # Set agent env variables for the Machine context (durable)  $clusterName   =   \"{{ .ECSEnvironmentID }}\"  Write-Host   Cluster   name   set   as :   $clusterName   -foreground   green  [Environment] :: SetEnvironmentVariable ( \"ECS_CLUSTER\" ,   $clusterName ,   \"Machine\" )  [Environment] :: SetEnvironmentVariable ( \"ECS_ENABLE_TASK_IAM_ROLE\" ,   \"false\" ,   \"Machine\" )  $agentVersion   =   'v1.5.2'  $agentZipUri   =   \"https://s3.amazonaws.com/amazon-ecs-agent/ecs-agent-windows-$agentVersion.zip\"  $agentZipMD5Uri   =   \"$agentZipUri.md5\"  # Configure docker auth  Read-S3Object   -BucketName   {{   . S3Bucket   }}   -Key   bootstrap / dockercfg   -File   dockercfg . json  $dockercfgContent   =   [IO.File] :: ReadAllText ( \"dockercfg.json\" )  [Environment] :: SetEnvironmentVariable ( \"ECS_ENGINE_AUTH_DATA\" ,   $dockercfgContent ,   \"Machine\" )  [Environment] :: SetEnvironmentVariable ( \"ECS_ENGINE_AUTH_TYPE\" ,   \"dockercfg\" ,   \"Machine\" )  ### --- Nothing user configurable after this point ---  $ecsExeDir   =   \"$env:ProgramFiles\\Amazon\\ECS\"  $zipFile   =   \"$env:TEMP\\ecs-agent.zip\"  $md5File   =   \"$env:TEMP\\ecs-agent.zip.md5\"  ### Get the files from S3  Invoke-RestMethod   -OutFile   $zipFile   -Uri   $agentZipUri  Invoke-RestMethod   -OutFile   $md5File   -Uri   $agentZipMD5Uri  ## MD5 Checksum  $expectedMD5   =   ( Get-Content   $md5File )  $md5   =   New-Object   -TypeName   System . Security . Cryptography . MD5CryptoServiceProvider  $actualMD5   =   [System.BitConverter] :: ToString ( $md5 . ComputeHash ( [System.IO.File] :: ReadAllBytes ( $zipFile ))). replace ( '-' ,   '' )  if ( $expectedMD5   -ne   $actualMD5 )   { \n     echo   \"Download doesn't match hash.\" \n     echo   \"Expected: $expectedMD5 - Got: $actualMD5\" \n     exit   1  }  ## Put the executables in the executable directory.  Expand-Archive   -Path   $zipFile   -DestinationPath   $ecsExeDir   -Force  ## Start the agent script in the background.  $jobname   =   \"ECS-Agent-Init\"  $script   =    \"cd '$ecsExeDir'; .\\amazon-ecs-agent.ps1\"  $repeat   =   ( New-TimeSpan   -Minutes   1 )  $jobpath   =   $env:LOCALAPPDATA   +   \"\\Microsoft\\Windows\\PowerShell\\ScheduledJobs\\$jobname\\ScheduledJobDefinition.xml\"  if ($( Test-Path   -Path   $jobpath ))   { \n   echo   \"Job definition already present\" \n   exit   0  }  $scriptblock   =   [scriptblock] :: Create ( \"$script\" )  $trigger   =   New-JobTrigger   -At   ( Get-Date ). Date   -RepeatIndefinitely   -RepetitionInterval   $repeat   -Once  $options   =   New-ScheduledJobOption   -RunElevated   -ContinueIfGoingOnBattery   -StartIfOnBattery  Register-ScheduledJob   -Name   $jobname   -ScriptBlock   $scriptblock   -Trigger   $trigger   -ScheduledJobOption   $options   -RunNow  Add-JobTrigger   -Name   $jobname   -Trigger   ( New-JobTrigger   -AtStartup   -RandomDelay   00 : 1 : 00 )  </ powershell >  < persist > true </ persist >    Windows Environments Windows containers are still in beta.     You can view the documented caveats with ECS  here .\nWhen creating Windows environments in Layer0, the root volume sizes for instances are 200GiB to accommodate the large size of the containers. \nIt can take as long as 45 minutes for a new windows container to come online.",
            "title": "Optional arguments"
        },
        {
            "location": "/reference/cli/#environment-delete",
            "text": "Use the  delete  subcommand to delete an existing Layer0 environment.",
            "title": "environment delete"
        },
        {
            "location": "/reference/cli/#usage_9",
            "text": "l0 environment delete [--wait] environmentName",
            "title": "Usage"
        },
        {
            "location": "/reference/cli/#required-parameters_4",
            "text": "environmentName  - The name of the Layer0 environment that you want to delete.",
            "title": "Required parameters"
        },
        {
            "location": "/reference/cli/#optional-arguments_1",
            "text": "--wait  - Wait until the deletion is complete before exiting.",
            "title": "Optional arguments"
        },
        {
            "location": "/reference/cli/#additional-information_3",
            "text": "This operation performs several tasks asynchronously. When run without the  --wait  option, this operation will most likely exit before all of these tasks are complete; when run with the  --wait  option, this operation will only exit once these tasks have completed.",
            "title": "Additional information"
        },
        {
            "location": "/reference/cli/#environment-get",
            "text": "Use the  get  subcommand to display information about an existing Layer0 environment.",
            "title": "environment get"
        },
        {
            "location": "/reference/cli/#usage_10",
            "text": "l0 environment get environmentName",
            "title": "Usage"
        },
        {
            "location": "/reference/cli/#required-parameters_5",
            "text": "environmentName  - The name of the Layer0 environment for which you want to view additional information.",
            "title": "Required parameters"
        },
        {
            "location": "/reference/cli/#additional-information_4",
            "text": "The  get  subcommand supports wildcard matching:  l0 environment get test*  would return all environments beginning with  test .",
            "title": "Additional information"
        },
        {
            "location": "/reference/cli/#environment-list",
            "text": "Use the  list  subcommand to display a list of environments in your instance of Layer0.",
            "title": "environment list"
        },
        {
            "location": "/reference/cli/#usage_11",
            "text": "l0 environment list",
            "title": "Usage"
        },
        {
            "location": "/reference/cli/#environment-setmincount",
            "text": "Use the  setmincount  subcommand to set the minimum number of EC2 instances allowed the environment's autoscaling group.",
            "title": "environment setmincount"
        },
        {
            "location": "/reference/cli/#usage_12",
            "text": "l0 environment setmincount environmentName count",
            "title": "Usage"
        },
        {
            "location": "/reference/cli/#required-parameters_6",
            "text": "environmentName  - The name of the Layer0 environment that you want to adjust.  count  - The minimum number of instances allowed in the environment's autoscaling group.",
            "title": "Required parameters"
        },
        {
            "location": "/reference/cli/#environment-link",
            "text": "Use the  link  subcommand to link two environments together. \nWhen environments are linked, services inside the environments are allowed to communicate with each other as if they were in the same environment. \nThis link is bidirectional. \nThis command is idempotent; it will succeed even if the two specified environments are already linked.",
            "title": "environment link"
        },
        {
            "location": "/reference/cli/#usage_13",
            "text": "l0 environment link sourceEnvironmentName destEnvironmentName",
            "title": "Usage"
        },
        {
            "location": "/reference/cli/#required-parameters_7",
            "text": "sourceEnvironmentName  - The name of the source environment to link.  destEnvironmentName  - The name of the destination environment to link.",
            "title": "Required parameters"
        },
        {
            "location": "/reference/cli/#environment-unlink",
            "text": "Use the  unlink  subcommand to remove the link between two environments.\nThis command is idempotent; it will succeed even if the link does not exist.",
            "title": "environment unlink"
        },
        {
            "location": "/reference/cli/#usage_14",
            "text": "l0 environment unlink sourceEnvironmentName destEnvironmentName",
            "title": "Usage"
        },
        {
            "location": "/reference/cli/#required-parameters_8",
            "text": "sourceEnvironmentName  - The name of the source environment to unlink.  destEnvironmentName  - The name of the destination environment to unlink.",
            "title": "Required parameters"
        },
        {
            "location": "/reference/cli/#job",
            "text": "A Job is a long-running unit of work performed on behalf of the Layer0 API.\nJobs are executed as Layer0 tasks that run in the  api  environment.\nThe  job  command is used with the following subcommands:  logs ,  delete ,  get , and  list .",
            "title": "Job"
        },
        {
            "location": "/reference/cli/#job-logs",
            "text": "Use the  logs  subcommand to display the logs from a Layer0 job that is currently running.",
            "title": "job logs"
        },
        {
            "location": "/reference/cli/#usage_15",
            "text": "l0 job logs [--start MM/DD HH:MM | --end MM/DD HH:MM | --tail=N] jobName",
            "title": "Usage"
        },
        {
            "location": "/reference/cli/#required-parameters_9",
            "text": "jobName  - The name of the Layer0 job for which you want to view logs.",
            "title": "Required parameters"
        },
        {
            "location": "/reference/cli/#optional-arguments_2",
            "text": "--start MM/DD HH:MM  - The start of the time range to fetch logs.  --end MM/DD HH:MM  - The end of the time range to fetch logs.  --tail=N  - Display only the last  N  lines of the log.",
            "title": "Optional arguments"
        },
        {
            "location": "/reference/cli/#job-delete",
            "text": "Use the  delete  subcommand to delete an existing job.",
            "title": "job delete"
        },
        {
            "location": "/reference/cli/#usage_16",
            "text": "l0 job delete jobName",
            "title": "Usage"
        },
        {
            "location": "/reference/cli/#required-parameters_10",
            "text": "jobName  - The name of the job that you want to delete.",
            "title": "Required parameters"
        },
        {
            "location": "/reference/cli/#job-get",
            "text": "Use the  get  subcommand to display information about an existing Layer0 job.",
            "title": "job get"
        },
        {
            "location": "/reference/cli/#usage_17",
            "text": "l0 job get jobName",
            "title": "Usage"
        },
        {
            "location": "/reference/cli/#required-parameters_11",
            "text": "jobName  - The name of an existing Layer0 job to display.",
            "title": "Required parameters"
        },
        {
            "location": "/reference/cli/#additional-information_5",
            "text": "The  get  subcommand supports wildcard matching:  l0 job get 2a55*  would return all jobs beginning with  2a55 .",
            "title": "Additional information"
        },
        {
            "location": "/reference/cli/#job-list",
            "text": "Use the  list  subcommand to display information about all of the existing jobs in an instance of Layer0.",
            "title": "job list"
        },
        {
            "location": "/reference/cli/#usage_18",
            "text": "l0 job list",
            "title": "Usage"
        },
        {
            "location": "/reference/cli/#load-balancer",
            "text": "A load balancer is a component of a Layer0 environment. Load balancers listen for traffic on certain ports, and then forward that traffic to Layer0  services . The  loadbalancer  command is used with the following subcommands:  create ,  delete ,  addport ,  dropport ,  get ,  list , and  healthcheck .",
            "title": "Load Balancer"
        },
        {
            "location": "/reference/cli/#loadbalancer-create",
            "text": "Use the  create  subcommand to create a new load balancer.",
            "title": "loadbalancer create"
        },
        {
            "location": "/reference/cli/#usage_19",
            "text": "l0 loadbalancer create [--port port ... | --certificate certifiateName | \n    --private | --healthcheck-target target | --healthcheck-interval interval | \n    --healthcheck-timeout timeout | --healthcheck-healthy-threshold healthyThreshold | \n    --healthcheck-unhealthy-threshold unhealthyThreshold] environmentName loadBalancerName",
            "title": "Usage"
        },
        {
            "location": "/reference/cli/#required-parameters_12",
            "text": "environmentName  - The name of the existing Layer0 environment in which you want to create the load balancer.  loadBalancerName  - A name for the load balancer you are creating.",
            "title": "Required parameters"
        },
        {
            "location": "/reference/cli/#optional-arguments_3",
            "text": "--port port ...  - The port configuration for the listener of the load balancer. Valid pattern is  hostPort:containerPort/protocol . Multiple ports can be specified using  --port port1 --port port2 ...  (default:  80/80:TCP ).  hostPort  - The port that the load balancer will listen for traffic on.  containerPort  - The port that the load balancer will forward traffic to.  protocol  - The protocol to use when forwarding traffic (acceptable values: TCP, SSL, HTTP, and HTTPS).    --certificate certificateName  - The name of an existing Layer0 certificate. You must include this option if you are using an HTTPS port configuration.  --private  - When you use this option, the load balancer will only be accessible from within the Layer0 environment.  --healthcheck-target target  - The target of the check. Valid pattern is  PROTOCOL:PORT/PATH  (default:  \"TCP:80\" ).   If  PROTOCOL  is  HTTP  or  HTTPS , both  PORT  and  PATH  are required. Example:  HTTP:80/admin/healthcheck .   If  PROTOCOL  is  TCP  or  SSL ,  PORT  is required and  PATH  is not used. Example:  TCP:80    --healthcheck-interval interval  - The interval between checks (default:  30 ).  --healthcheck-timeout timeout  - The length of time before the check times out (default:  5 ).  --healthcheck-healthy-threshold healthyThreshold  - The number of checks before the instance is declared healthy (default:  2 ).  --healthcheck-unhealthy-threshold unhealthyThreshold  - The number of checks before the instance is declared unhealthy (default:  2 ).    Ports and Health Checks  When both the  --port  and the  --healthcheck-target  options are omitted, Layer0 configures the load balancer with some default values:  80:80/TCP  for ports and  TCP:80  for healthcheck target.\nThese default values together create a load balancer configured with a simple but functioning health check, opening up a set of ports that allows traffic to the target of the healthcheck.\n( --healthcheck-target TCP:80  tells the load balancer to ping its services at port 80 to determine their status, and  --port 80:80/TCP  configures a security group to allow traffic to pass between port 80 of the load balancer and port 80 of its services)  When creating a load balancer with non-default configurations for either  --port  or  --healthcheck-target , make sure that a valid  --port  and  --healthcheck-target  pairing is also created.",
            "title": "Optional arguments"
        },
        {
            "location": "/reference/cli/#loadbalancer-delete",
            "text": "Use the  delete  subcommand to delete an existing load balancer.",
            "title": "loadbalancer delete"
        },
        {
            "location": "/reference/cli/#usage_20",
            "text": "l0 loadbalancer delete [--wait] loadBalancerName",
            "title": "Usage"
        },
        {
            "location": "/reference/cli/#required-parameters_13",
            "text": "loadBalancerName  - The name of the load balancer that you want to delete.",
            "title": "Required parameters"
        },
        {
            "location": "/reference/cli/#optional-arguments_4",
            "text": "--wait  - Wait until the deletion is complete before exiting.",
            "title": "Optional arguments"
        },
        {
            "location": "/reference/cli/#additional-information_6",
            "text": "In order to delete a load balancer that is already attached to a service, you must first delete the service that uses the load balancer.  This operation performs several tasks asynchronously. When run without the  --wait  option, this operation will most likely exit before all of these tasks are complete; when run with the  --wait  option, this operation will only exit once these tasks have completed\n.",
            "title": "Additional information"
        },
        {
            "location": "/reference/cli/#loadbalancer-addport",
            "text": "Use the  addport  subcommand to add a new port configuration to an existing Layer0 load balancer.",
            "title": "loadbalancer addport"
        },
        {
            "location": "/reference/cli/#usage_21",
            "text": "l0 loadbalancer addport [--certificate certificateName] loadBalancerName port",
            "title": "Usage"
        },
        {
            "location": "/reference/cli/#required-parameters_14",
            "text": "loadBalancerName  - The name of an existing Layer0 load balancer in which you want to add the port configuration.  port  - The port configuration for the listener of the load balancer. Valid pattern is  hostPort:containerPort/protocol .  hostPort  - The port that the load balancer will listen for traffic on.  containerPort  - The port that the load balancer will forward traffic to.  protocol  - The protocol to use when forwarding traffic (acceptable values: TCP, SSL, HTTP, and HTTPS).",
            "title": "Required parameters"
        },
        {
            "location": "/reference/cli/#optional-arguments_5",
            "text": "--certificate certificateName  - The name of an existing Layer0 certificate. You must include this option if you are using an HTTPS port configuration.",
            "title": "Optional arguments"
        },
        {
            "location": "/reference/cli/#additional-information_7",
            "text": "The port configuration you specify must not already be in use by the load balancer you specify.",
            "title": "Additional information"
        },
        {
            "location": "/reference/cli/#loadbalancer-dropport",
            "text": "Use the  dropport  subcommand to remove a port configuration from an existing Layer0 load balancer.",
            "title": "loadbalancer dropport"
        },
        {
            "location": "/reference/cli/#usage_22",
            "text": "l0 loadbalancer dropport loadBalancerName hostPort",
            "title": "Usage"
        },
        {
            "location": "/reference/cli/#required-parameters_15",
            "text": "loadBalancerName - The name of an existing Layer0 load balancer from which you want to remove the port configuration.  hostPort - The host port to remove from the load balancer.",
            "title": "Required parameters"
        },
        {
            "location": "/reference/cli/#loadbalancer-get",
            "text": "Use the  get  subcommand to display information about an existing Layer0 load balancer.",
            "title": "loadbalancer get"
        },
        {
            "location": "/reference/cli/#usage_23",
            "text": "l0 loadbalancer get [environmentName:]loadBalancerName",
            "title": "Usage"
        },
        {
            "location": "/reference/cli/#required-parameters_16",
            "text": "[environmentName:]loadBalancerName  - The name of an existing Layer0 load balancer. You can optionally provide the Layer0 environment ( environmentName ) associated with the Load Balancer",
            "title": "Required parameters"
        },
        {
            "location": "/reference/cli/#additional-information_8",
            "text": "The  get  subcommand supports wildcard matching:  l0 loadbalancer get entrypoint*  would return all jobs beginning with  entrypoint .",
            "title": "Additional information"
        },
        {
            "location": "/reference/cli/#loadbalancer-list",
            "text": "Use the  list  subcommand to display information about all of the existing load balancers in an instance of Layer0.",
            "title": "loadbalancer list"
        },
        {
            "location": "/reference/cli/#usage_24",
            "text": "l0 loadbalancer list",
            "title": "Usage"
        },
        {
            "location": "/reference/cli/#loadbalancer-healthcheck",
            "text": "Use the  healthcheck  subcommand to display information about or update the configuration of a load balancer's health check.",
            "title": "loadbalancer healthcheck"
        },
        {
            "location": "/reference/cli/#usage_25",
            "text": "l0 loadbalancer healthcheck [--set-target target | --set-interval interval | \n    --set-timeout timeout | --set-healthy-threshold healthyThreshold | \n    --set-unhealthy-threshold unhealthyThreshold] loadbalancerName",
            "title": "Usage"
        },
        {
            "location": "/reference/cli/#required-parameters_17",
            "text": "loadBalancerName  - The name of the existing Layer0 load balancer you are modifying.",
            "title": "Required parameters"
        },
        {
            "location": "/reference/cli/#optional-arguments_6",
            "text": "--set-target target  - The target of the check. Valid pattern is  PROTOCOL:PORT/PATH .  If  PROTOCOL  is  HTTP  or  HTTPS , both  PORT  and  PATH  are required. Example:  HTTP:80/admin/healthcheck .  If  PROTOCOL  is  TCP  or  SSL ,  PORT  is required and  PATH  is not used. Example:  TCP:80    --set-interval interval  - The interval between health checks.  --set-timeout timeout  - The length of time in seconds before the health check times out.  --set-healthy-threshold healthyThreshold  - The number of checks before the instance is declared healthy.  --set-unhealthy-threshold unhealthyThreshold  - The number of checks before the instance is declared unhealthy.",
            "title": "Optional arguments"
        },
        {
            "location": "/reference/cli/#additional-information_9",
            "text": "Calling the subcommand without flags will display the current configuration of the load balancer's health check. Setting any of the flags will update the corresponding field in the health check, and all omitted flags will leave the corresponding fields unchanged.",
            "title": "Additional information"
        },
        {
            "location": "/reference/cli/#service",
            "text": "A service is a component of a Layer0 environment. The purpose of a service is to execute a Docker image specified in a  deploy . In order to create a service, you must first create an  environment  and a  deploy ; in most cases, you should also create a  load balancer  before creating the service.  The  service  command is used with the following subcommands:  create ,  delete ,  get ,  update ,  list ,  logs , and  scale .",
            "title": "Service"
        },
        {
            "location": "/reference/cli/#service-create",
            "text": "Use the  create  subcommand to create a Layer0 service.",
            "title": "service create"
        },
        {
            "location": "/reference/cli/#usage_26",
            "text": "l0 service create [--loadbalancer [environmentName:]loadBalancerName | \n    --no-logs] environmentName serviceName deployName[:deployVersion]",
            "title": "Usage"
        },
        {
            "location": "/reference/cli/#required-parameters_18",
            "text": "serviceName  - A name for the service that you are creating.  environmentName  - The name of an existing Layer0 environment.  deployName[:deployVersion]  - The name of a Layer0 deploy that exists in the environment  environmentName . You can optionally specify the version number of the Layer0 deploy that you want to deploy. If you do not specify a version number, the latest version of the deploy will be used.",
            "title": "Required parameters"
        },
        {
            "location": "/reference/cli/#optional-arguments_7",
            "text": "--loadbalancer [environmentName:]loadBalancerName  - Place the new service behind an existing load balancer  loadBalancerName . You can optionally specify the Layer0 environment ( environmentName ) where the load balancer exists.  --no-logs  - Disable cloudwatch logging for the service",
            "title": "Optional arguments"
        },
        {
            "location": "/reference/cli/#service-update",
            "text": "Use the  update  subcommand to apply an existing Layer0 Deploy to an existing Layer0 service.",
            "title": "service update"
        },
        {
            "location": "/reference/cli/#usage_27",
            "text": "l0 service update [--no-logs] [environmentName:]serviceName deployName[:deployVersion]",
            "title": "Usage"
        },
        {
            "location": "/reference/cli/#required-parameters_19",
            "text": "[environmentName:]serviceName  - The name of an existing Layer0 service into which you want to apply the deploy. You can optionally specify the Layer0 environment ( environmentName ) of the service.  deployName[:deployVersion]  - The name of the Layer0 deploy that you want to apply to the service. You can optionally specify a specific version of the deploy ( deployVersion ). If you do not specify a version number, the latest version of the deploy will be applied.",
            "title": "Required parameters"
        },
        {
            "location": "/reference/cli/#optional-arguments_8",
            "text": "--no-logs  - Disable cloudwatch logging for the service",
            "title": "Optional arguments"
        },
        {
            "location": "/reference/cli/#additional-information_10",
            "text": "If your service uses a load balancer, when you update the task definition for the service, the container name and container port that were specified when the service was created must remain the same in the task definition. In other words, if your service has a load balancer, you cannot apply any deploy you want to that service. If you are varying the container name or exposed ports, you must create a new service instead.",
            "title": "Additional information"
        },
        {
            "location": "/reference/cli/#service-delete",
            "text": "Use the  delete  subcommand to delete an existing Layer0 service.",
            "title": "service delete"
        },
        {
            "location": "/reference/cli/#usage_28",
            "text": "l0 service delete [--wait] [environmentName:]serviceName",
            "title": "Usage"
        },
        {
            "location": "/reference/cli/#required-parameters_20",
            "text": "[environmentName:]serviceName  - The name of the Layer0 service that you want to delete. You can optionally provide the Layer0 environment ( environmentName ) of the service.",
            "title": "Required parameters"
        },
        {
            "location": "/reference/cli/#optional-arguments_9",
            "text": "--wait  - Wait until the deletion is complete before exiting.",
            "title": "Optional arguments"
        },
        {
            "location": "/reference/cli/#additional-information_11",
            "text": "This operation performs several tasks asynchronously. When run without the  --wait  option, this operation will most likely exit before all of these tasks are complete; when run with the  --wait  option, this operation will only exit once these tasks have completed.",
            "title": "Additional information"
        },
        {
            "location": "/reference/cli/#service-get",
            "text": "Use the  get  subcommand to display information about an existing Layer0 service.",
            "title": "service get"
        },
        {
            "location": "/reference/cli/#usage_29",
            "text": "l0 service get [environmentName:]serviceName",
            "title": "Usage"
        },
        {
            "location": "/reference/cli/#required-parameters_21",
            "text": "[environmentName:]serviceName  - The name of an existing Layer0 service. You can optionally provide the Layer0 environment ( environmentName ) of the service.",
            "title": "Required parameters"
        },
        {
            "location": "/reference/cli/#service-list",
            "text": "Use the  list  subcommand to list all of the existing services in your Layer0 instance.",
            "title": "service list"
        },
        {
            "location": "/reference/cli/#usage_30",
            "text": "l0 service get list",
            "title": "Usage"
        },
        {
            "location": "/reference/cli/#service-logs",
            "text": "Use the  logs  subcommand to display the logs from a Layer0 service that is currently running.",
            "title": "service logs"
        },
        {
            "location": "/reference/cli/#usage_31",
            "text": "l0 service logs [--start MM/DD HH:MM | --end MM/DD HH:MM | --tail=N] serviceName",
            "title": "Usage"
        },
        {
            "location": "/reference/cli/#required-parameters_22",
            "text": "serviceName  - The name of the Layer0 service for which you want to view logs.",
            "title": "Required parameters"
        },
        {
            "location": "/reference/cli/#optional-arguments_10",
            "text": "--start MM/DD HH:MM  - The start of the time range to fetch logs.  --end MM/DD HH:MM  - The end of the time range to fetch logs.  --tail=N  - Display only the last  N  lines of the log.",
            "title": "Optional arguments"
        },
        {
            "location": "/reference/cli/#service-scale",
            "text": "Use the  scale  subcommand to specify how many copies of an existing Layer0 service should run.",
            "title": "service scale"
        },
        {
            "location": "/reference/cli/#usage_32",
            "text": "l0 service scale [environmentName:]serviceName copies",
            "title": "Usage"
        },
        {
            "location": "/reference/cli/#required-parameters_23",
            "text": "[environmentName:]serviceName  - The name of the Layer0 service that you want to scale up. You can optionally provide the Layer0 environment ( environmentName ) of the service.  copies  - The number of copies of the specified service that should be run.",
            "title": "Required parameters"
        },
        {
            "location": "/reference/cli/#task",
            "text": "A Layer0 task is a component of an environment. A task executes the contents of a Docker image, as specified in a deploy. A task differs from a service in that a task does not restart after exiting. Additionally, ports are not exposed when using a task.  The  task  command is used with the following subcommands:  create ,  delete ,  get ,  list , and  logs .",
            "title": "Task"
        },
        {
            "location": "/reference/cli/#task-create",
            "text": "Use the  create  subcommand to create a Layer0 task.",
            "title": "task create"
        },
        {
            "location": "/reference/cli/#usage_33",
            "text": "l0 task create [--copies copies | --no-logs] environmentName taskName deployName",
            "title": "Usage"
        },
        {
            "location": "/reference/cli/#required-parameters_24",
            "text": "environmentName  - The name of the existing Layer0 environment in which you want to create the task.  taskName  - A name for the task.  deployName  - The name of an existing Layer0 deploy that the task should use.",
            "title": "Required parameters"
        },
        {
            "location": "/reference/cli/#optional-arguments_11",
            "text": "--copies copies  - The number of copies of the task to run (default: 1).  --no-logs  - Disable cloudwatch logging for the service.",
            "title": "Optional arguments"
        },
        {
            "location": "/reference/cli/#task-delete",
            "text": "Use the  delete  subcommand to delete an existing Layer0 task.",
            "title": "task delete"
        },
        {
            "location": "/reference/cli/#usage_34",
            "text": "l0 task delete [environmentName:]taskName",
            "title": "Usage"
        },
        {
            "location": "/reference/cli/#required-parameters_25",
            "text": "[environmentName:]taskName  - The name of the Layer0 task that you want to delete. You can optionally specify the name of the Layer0 environment that contains the task. This parameter is only required if mulitiple environments contain tasks with exactly the same name.",
            "title": "Required parameters"
        },
        {
            "location": "/reference/cli/#additional-information_12",
            "text": "Until the record has been purged, the API may indicate that the task is still running. Task records are typically purged within an hour.",
            "title": "Additional information"
        },
        {
            "location": "/reference/cli/#task-get",
            "text": "Use the  get  subcommand to display information about an existing Layer0 task ( taskName ).",
            "title": "task get"
        },
        {
            "location": "/reference/cli/#usage_35",
            "text": "l0 task get [environmentName:]taskName",
            "title": "Usage"
        },
        {
            "location": "/reference/cli/#required-parameters_26",
            "text": "[environmentName:]taskName  - The name of a Layer0 task for which you want to see information. You can optionally specify the name of the Layer0 Environment that contains the task.",
            "title": "Required parameters"
        },
        {
            "location": "/reference/cli/#additional-information_13",
            "text": "The value of  taskName  does not need to exactly match the name of an existing task. If multiple results are found that match the pattern you specified in  taskName , then information about all matching tasks will be returned.",
            "title": "Additional information"
        },
        {
            "location": "/reference/cli/#task-list",
            "text": "Use the  task  subcommand to display a list of running tasks in your Layer0.",
            "title": "task list"
        },
        {
            "location": "/reference/cli/#usage_36",
            "text": "l0 task list",
            "title": "Usage"
        },
        {
            "location": "/reference/cli/#task-logs",
            "text": "Use the  logs  subcommand to display logs for a running Layer0 task.",
            "title": "task logs"
        },
        {
            "location": "/reference/cli/#usage_37",
            "text": "l0 task logs [--start MM/DD HH:MM | --end MM/DD HH:MM | --tail=N] taskName",
            "title": "Usage"
        },
        {
            "location": "/reference/cli/#required-parameters_27",
            "text": "taskName  - The name of an existing Layer0 task.",
            "title": "Required parameters"
        },
        {
            "location": "/reference/cli/#optional-arguments_12",
            "text": "--start MM/DD HH:MM  - The start of the time range to fetch logs.  --end MM/DD HH:MM  - The end of the time range to fetch logs.  --tail=N  - Display only the last  N  lines of the log.",
            "title": "Optional arguments"
        },
        {
            "location": "/reference/cli/#additional-information_14",
            "text": "The value of  taskName  does not need to exactly match the name of an existing task. If multiple results are found that match the pattern you specified in  taskName , then information about all matching tasks will be returned.",
            "title": "Additional information"
        },
        {
            "location": "/reference/cli/#task-list_1",
            "text": "Use the  list  subcommand to display a list of running tasks in your Layer0.",
            "title": "task list"
        },
        {
            "location": "/reference/cli/#usage_38",
            "text": "l0 task list",
            "title": "Usage"
        },
        {
            "location": "/reference/setup-cli/",
            "text": "Layer0 Setup Reference\n#\n\n\nThe Layer0 Setup application (commonly called \nl0-setup\n), is used for administrative tasks on Layer0 instances.\n\n\nGlobal options\n#\n\n\nl0-setup\n can be used with one of several commands: \ninit\n, \nplan\n, \napply\n, \nlist\n, \npush\n, \npull\n, \nendpoint\n, \ndestroy\n, \nupgrade\n, and \nset\n. These commands are detailed in teh sections below. There are, however, some global paramters that you may specify whenever using \nl0-setup\n\n\nUsage\n#\n\n\nl0\n-\nsetup\n \n[\nglobal\n \noptions\n]\n \ncommand\n \n[\ncommand\n \noptions\n]\n \nparams\n\n\n\n\n\n\nGlobal options\n#\n\n\n\n\n-l value, --log value\n - The log level to display on the console when you run commands. (default: info)\n\n\n--version\n - Display the version number of the \nl0-setup\n application.\n\n\n\n\n\n\nInit\n#\n\n\nThe \ninit\n command is used to initialize or reconfigure a Layer0 instance. \nThis command will prompt the user for inputs required to create/update a Layer0 instance. \nEach of the inputs can be specified through an optional flag.\n\n\nUsage\n#\n\n\nl0-setup init [--docker-path path | --module-source path | \n    --version version | --aws-region region | --aws-access-key accessKey | \n    --aws-secret-key secretKey] instanceName\n\n\n\n\n\nOptional arguments\n#\n\n\n\n\n--docker-path\n - Path to docker config.json file. This is used to include private Docker Registry authentication for this Layer0 instance.\n\n\n--module-source\n - The source input variable is the path to the Terraform Layer0. By default, this points to the Layer0 github repository. Using values other than the default may result in undesired consequences.\n\n\n--version\n - The version input variable specifies the tag to use for the Layer0 Docker images: \nquintilesims/l0-api\n and \nquintilesims/l0-runner\n.\n\n\n--aws-ssh-key-pair\n - The ssh_key_pair input variable specifies the name of the ssh key pair to include in EC2 instances provisioned by Layer0. This key pair must already exist in the AWS account.  The names of existing key pairs can be found in the EC2 dashboard.\n\n\n--aws-access-key\n - The access_key input variable is used to provision the AWS resources required for Layer0. This corresponds to the Access Key ID portion of an AWS Access Key. It is recommended this key has the \nAdministratorAccess\n policy.\n\n\n--aws-secret-key\n - The secret_key input variable is used to provision the AWS resources required for Layer0. This corresponds to the Secret Access Key portion of an AWS Access Key. It is recommended this key has the \nAdministratorAccess\n policy.\n\n\n\n\n\n\nPlan\n#\n\n\nThe \nplan\n command is used to show the planned operation(s) to run during the next \napply\n on a Layer0 instance without actually executing any actions\n\n\nUsage\n#\n\n\nl0-setup plan instanceName\n\n\n\n\n\n\n\nApply\n#\n\n\nThe \napply\n command is used to create and update Layer0 instances. Note that the default behavior of apply is to push the layer0 configuration to an S3 bucket unless the \n--push=false\n flag is set to false. Pushing the configuration to an S3 bucket requires aws credentials which if not set via the optional \n--aws-*\n flags, are read from the environment variables or a credentials file. \n\n\nUsage\n#\n\n\nl0-setup apply [--quick | --push=false | --aws-access-key accessKey | \n    --aws-secret-key secretKey] instanceName\n\n\n\n\n\nOptional arguments\n#\n\n\n\n\n--quick\n - Skips verification checks that normally run after \nterraform apply\n has completed\n\n\n--push=false\n - Skips uploading local Layer0 configuration files to an S3 bucket\n\n\n--aws-access-key\n - The access_key input variable is used to provision the AWS resources required for Layer0. This corresponds to the Access Key ID portion of an AWS Access Key. It is recommended this key has the \nAdministratorAccess\n policy.\n\n\n--aws-secret-key\n - The secret_key input variable is used to provision the AWS resources required for Layer0. This corresponds to the Secret Access Key portion of an AWS Access Key. It is recommended this key has the \nAdministratorAccess\n policy.\n\n\n\n\n\n\nList\n#\n\n\nThe \nlist\n command is used to list local and remote Layer0 instances.\n\n\nUsage\n#\n\n\nl0-setup list [--local=false | --remote=false | --aws-access-key accessKey | \n    --aws-secret-key secretKey]\n\n\n\n\n\nOptional arguments\n#\n\n\n\n\n-l, --local\n - Show local Layer0 instances. This value is true by default.\n\n\n-r, --remote\n - Show remote Layer0 instances. This value is true by default. \n\n\n\n\n\n\nPush\n#\n\n\nThe \npush\n command is used to back up your Layer0 configuration files to an S3 bucket.\n\n\nUsage\n#\n\n\nl0-setup push [--aws-access-key accessKey | \n    --aws-secret-key secretKey] instanceName\n\n\n\n\n\nOptional arguments\n#\n\n\n\n\n--aws-access-key\n - The access_key input variable is used to provision the AWS resources required for Layer0. This corresponds to the Access Key ID portion of an AWS Access Key. It is recommended this key has the \nAdministratorAccess\n policy.\n\n\n--aws-secret-key\n - The secret_key input variable is used to provision the AWS resources required for Layer0. This corresponds to the Secret Access Key portion of an AWS Access Key. It is recommended this key has the \nAdministratorAccess\n policy.\n\n\n\n\n\n\nPull\n#\n\n\nThe \npull\n command is used copy Layer0 configuration files from an S3 bucket.\n\n\nUsage\n#\n\n\nl0-setup pull [--aws-access-key accessKey | \n    --aws-secret-key secretKey] instanceName\n\n\n\n\n\nOptional arguments\n#\n\n\n\n\n--aws-access-key\n - The access_key input variable is used to provision the AWS resources required for Layer0. This corresponds to the Access Key ID portion of an AWS Access Key. It is recommended this key has the \nAdministratorAccess\n policy.\n\n\n--aws-secret-key\n - The secret_key input variable is used to provision the AWS resources required for Layer0. This corresponds to the Secret Access Key portion of an AWS Access Key. It is recommended this key has the \nAdministratorAccess\n policy.\n\n\n\n\n\n\nEndpoint\n#\n\n\nThe \nendpoint\n command is used to show environment variables used to connect to a Layer0 instance\n\n\nUsage\n#\n\n\nl0-setup endpoint [-i | -d | -s syntax] instanceName\n\n\n\n\n\nOptional arguments\n#\n\n\n\n\n-i, --insecure\n - Show environment variables that allow for insecure settings\n\n\n-d, --dev\n - Show environment variables that are required for local development\n\n\n-s --syntax\n - Choose the syntax to display environment variables \n(choices: \nbash\n, \ncmd\n, \npowershell\n) (default: \nbash\n)\n\n\n\n\n\n\nDestroy\n#\n\n\nThe \ndestroy\n command is used to destroy all resources associated with a Layer0 instance.\n\n\n\n\nCaution\n\n\nDestroying a Layer0 instance cannot be undone. If you created backups of your Layer0 configuration using the \npush\n command, those backups will also be deleted when you run the \ndestroy\n command.\n\n\n\n\nUsage\n#\n\n\nl0-setup destroy [--force] instanceName\n\n\n\n\n\nOptional arguments\n#\n\n\n\n\n--force\n - Skips confirmation prompt\n\n\n\n\n\n\nUpgrade\n#\n\n\nThe \nupgrade\n command is used to upgrade a Layer0 instance to a new version.\nYou will need to run an \napply\n after this command has completed. \n\n\nUsage\n#\n\n\nl0-setup upgrade [--force] instanceName version\n\n\n\n\n\nOptional arguments\n#\n\n\n\n\n--force\n - Skips confirmation prompt\n\n\n\n\n\n\nSet\n#\n\n\nThe \nset\n command is used set input variable(s) for a Layer0 instance's Terraform module.\nThis command can be used to shorthand the \ninit\n and \nupgrade\n commands, and can also be used with custom Layer0 modules. \nYou will need to run an \napply\n after this command has completed. \n\n\nUsage\n#\n\n\nl0-setup set [--input key=value] instanceName\n\n\n\n\n\nOptions\n#\n\n\n\n\n--input key=val\n - Specify an input using \nkey=val\n format\n\n\n\n\nExample Usage\n#\n\n\nl0-setup set --input username=admin --input password=pass123 mylayer0",
            "title": "Layer0 Setup CLI"
        },
        {
            "location": "/reference/setup-cli/#layer0-setup-reference",
            "text": "The Layer0 Setup application (commonly called  l0-setup ), is used for administrative tasks on Layer0 instances.",
            "title": "Layer0 Setup Reference"
        },
        {
            "location": "/reference/setup-cli/#global-options",
            "text": "l0-setup  can be used with one of several commands:  init ,  plan ,  apply ,  list ,  push ,  pull ,  endpoint ,  destroy ,  upgrade , and  set . These commands are detailed in teh sections below. There are, however, some global paramters that you may specify whenever using  l0-setup",
            "title": "Global options"
        },
        {
            "location": "/reference/setup-cli/#usage",
            "text": "l0 - setup   [ global   options ]   command   [ command   options ]   params",
            "title": "Usage"
        },
        {
            "location": "/reference/setup-cli/#global-options_1",
            "text": "-l value, --log value  - The log level to display on the console when you run commands. (default: info)  --version  - Display the version number of the  l0-setup  application.",
            "title": "Global options"
        },
        {
            "location": "/reference/setup-cli/#init",
            "text": "The  init  command is used to initialize or reconfigure a Layer0 instance. \nThis command will prompt the user for inputs required to create/update a Layer0 instance. \nEach of the inputs can be specified through an optional flag.",
            "title": "Init"
        },
        {
            "location": "/reference/setup-cli/#usage_1",
            "text": "l0-setup init [--docker-path path | --module-source path | \n    --version version | --aws-region region | --aws-access-key accessKey | \n    --aws-secret-key secretKey] instanceName",
            "title": "Usage"
        },
        {
            "location": "/reference/setup-cli/#optional-arguments",
            "text": "--docker-path  - Path to docker config.json file. This is used to include private Docker Registry authentication for this Layer0 instance.  --module-source  - The source input variable is the path to the Terraform Layer0. By default, this points to the Layer0 github repository. Using values other than the default may result in undesired consequences.  --version  - The version input variable specifies the tag to use for the Layer0 Docker images:  quintilesims/l0-api  and  quintilesims/l0-runner .  --aws-ssh-key-pair  - The ssh_key_pair input variable specifies the name of the ssh key pair to include in EC2 instances provisioned by Layer0. This key pair must already exist in the AWS account.  The names of existing key pairs can be found in the EC2 dashboard.  --aws-access-key  - The access_key input variable is used to provision the AWS resources required for Layer0. This corresponds to the Access Key ID portion of an AWS Access Key. It is recommended this key has the  AdministratorAccess  policy.  --aws-secret-key  - The secret_key input variable is used to provision the AWS resources required for Layer0. This corresponds to the Secret Access Key portion of an AWS Access Key. It is recommended this key has the  AdministratorAccess  policy.",
            "title": "Optional arguments"
        },
        {
            "location": "/reference/setup-cli/#plan",
            "text": "The  plan  command is used to show the planned operation(s) to run during the next  apply  on a Layer0 instance without actually executing any actions",
            "title": "Plan"
        },
        {
            "location": "/reference/setup-cli/#usage_2",
            "text": "l0-setup plan instanceName",
            "title": "Usage"
        },
        {
            "location": "/reference/setup-cli/#apply",
            "text": "The  apply  command is used to create and update Layer0 instances. Note that the default behavior of apply is to push the layer0 configuration to an S3 bucket unless the  --push=false  flag is set to false. Pushing the configuration to an S3 bucket requires aws credentials which if not set via the optional  --aws-*  flags, are read from the environment variables or a credentials file.",
            "title": "Apply"
        },
        {
            "location": "/reference/setup-cli/#usage_3",
            "text": "l0-setup apply [--quick | --push=false | --aws-access-key accessKey | \n    --aws-secret-key secretKey] instanceName",
            "title": "Usage"
        },
        {
            "location": "/reference/setup-cli/#optional-arguments_1",
            "text": "--quick  - Skips verification checks that normally run after  terraform apply  has completed  --push=false  - Skips uploading local Layer0 configuration files to an S3 bucket  --aws-access-key  - The access_key input variable is used to provision the AWS resources required for Layer0. This corresponds to the Access Key ID portion of an AWS Access Key. It is recommended this key has the  AdministratorAccess  policy.  --aws-secret-key  - The secret_key input variable is used to provision the AWS resources required for Layer0. This corresponds to the Secret Access Key portion of an AWS Access Key. It is recommended this key has the  AdministratorAccess  policy.",
            "title": "Optional arguments"
        },
        {
            "location": "/reference/setup-cli/#list",
            "text": "The  list  command is used to list local and remote Layer0 instances.",
            "title": "List"
        },
        {
            "location": "/reference/setup-cli/#usage_4",
            "text": "l0-setup list [--local=false | --remote=false | --aws-access-key accessKey | \n    --aws-secret-key secretKey]",
            "title": "Usage"
        },
        {
            "location": "/reference/setup-cli/#optional-arguments_2",
            "text": "-l, --local  - Show local Layer0 instances. This value is true by default.  -r, --remote  - Show remote Layer0 instances. This value is true by default.",
            "title": "Optional arguments"
        },
        {
            "location": "/reference/setup-cli/#push",
            "text": "The  push  command is used to back up your Layer0 configuration files to an S3 bucket.",
            "title": "Push"
        },
        {
            "location": "/reference/setup-cli/#usage_5",
            "text": "l0-setup push [--aws-access-key accessKey | \n    --aws-secret-key secretKey] instanceName",
            "title": "Usage"
        },
        {
            "location": "/reference/setup-cli/#optional-arguments_3",
            "text": "--aws-access-key  - The access_key input variable is used to provision the AWS resources required for Layer0. This corresponds to the Access Key ID portion of an AWS Access Key. It is recommended this key has the  AdministratorAccess  policy.  --aws-secret-key  - The secret_key input variable is used to provision the AWS resources required for Layer0. This corresponds to the Secret Access Key portion of an AWS Access Key. It is recommended this key has the  AdministratorAccess  policy.",
            "title": "Optional arguments"
        },
        {
            "location": "/reference/setup-cli/#pull",
            "text": "The  pull  command is used copy Layer0 configuration files from an S3 bucket.",
            "title": "Pull"
        },
        {
            "location": "/reference/setup-cli/#usage_6",
            "text": "l0-setup pull [--aws-access-key accessKey | \n    --aws-secret-key secretKey] instanceName",
            "title": "Usage"
        },
        {
            "location": "/reference/setup-cli/#optional-arguments_4",
            "text": "--aws-access-key  - The access_key input variable is used to provision the AWS resources required for Layer0. This corresponds to the Access Key ID portion of an AWS Access Key. It is recommended this key has the  AdministratorAccess  policy.  --aws-secret-key  - The secret_key input variable is used to provision the AWS resources required for Layer0. This corresponds to the Secret Access Key portion of an AWS Access Key. It is recommended this key has the  AdministratorAccess  policy.",
            "title": "Optional arguments"
        },
        {
            "location": "/reference/setup-cli/#endpoint",
            "text": "The  endpoint  command is used to show environment variables used to connect to a Layer0 instance",
            "title": "Endpoint"
        },
        {
            "location": "/reference/setup-cli/#usage_7",
            "text": "l0-setup endpoint [-i | -d | -s syntax] instanceName",
            "title": "Usage"
        },
        {
            "location": "/reference/setup-cli/#optional-arguments_5",
            "text": "-i, --insecure  - Show environment variables that allow for insecure settings  -d, --dev  - Show environment variables that are required for local development  -s --syntax  - Choose the syntax to display environment variables \n(choices:  bash ,  cmd ,  powershell ) (default:  bash )",
            "title": "Optional arguments"
        },
        {
            "location": "/reference/setup-cli/#destroy",
            "text": "The  destroy  command is used to destroy all resources associated with a Layer0 instance.   Caution  Destroying a Layer0 instance cannot be undone. If you created backups of your Layer0 configuration using the  push  command, those backups will also be deleted when you run the  destroy  command.",
            "title": "Destroy"
        },
        {
            "location": "/reference/setup-cli/#usage_8",
            "text": "l0-setup destroy [--force] instanceName",
            "title": "Usage"
        },
        {
            "location": "/reference/setup-cli/#optional-arguments_6",
            "text": "--force  - Skips confirmation prompt",
            "title": "Optional arguments"
        },
        {
            "location": "/reference/setup-cli/#upgrade",
            "text": "The  upgrade  command is used to upgrade a Layer0 instance to a new version.\nYou will need to run an  apply  after this command has completed.",
            "title": "Upgrade"
        },
        {
            "location": "/reference/setup-cli/#usage_9",
            "text": "l0-setup upgrade [--force] instanceName version",
            "title": "Usage"
        },
        {
            "location": "/reference/setup-cli/#optional-arguments_7",
            "text": "--force  - Skips confirmation prompt",
            "title": "Optional arguments"
        },
        {
            "location": "/reference/setup-cli/#set",
            "text": "The  set  command is used set input variable(s) for a Layer0 instance's Terraform module.\nThis command can be used to shorthand the  init  and  upgrade  commands, and can also be used with custom Layer0 modules. \nYou will need to run an  apply  after this command has completed.",
            "title": "Set"
        },
        {
            "location": "/reference/setup-cli/#usage_10",
            "text": "l0-setup set [--input key=value] instanceName",
            "title": "Usage"
        },
        {
            "location": "/reference/setup-cli/#options",
            "text": "--input key=val  - Specify an input using  key=val  format",
            "title": "Options"
        },
        {
            "location": "/reference/setup-cli/#example-usage",
            "text": "l0-setup set --input username=admin --input password=pass123 mylayer0",
            "title": "Example Usage"
        },
        {
            "location": "/reference/terraform_introduction/",
            "text": "Introduction to Terraform\n#\n\n\nWhat does Terraform do?\n#\n\n\nTerraform is a powerful orchestration tool for creating, updating, deleting, and otherwise managing infrastructure in an easy-to-understand, declarative manner.\nTerraform's \ndocumentation\n is very good, but at a glance:\n\n\nBe Declarative -\n\nSpecify desired infrastructure results in Terraform (\n*.tf\n) files, and let Terraform do the heavy work of figuring out how to make that specification a reality.\n\n\nScry the Future -\n\nUse \nterraform plan\n to see a list of everything that Terraform \nwould\n do without actually making those changes.\n\n\nVersion Infrastructure -\n\nCheck Terraform files into a VCS to track changes to and manage versions of your infrastructure.\n\n\nWhy Terraform?\n#\n\n\nWhy did we latch onto Terraform instead of something like CloudFormation?\n\n\nCloud-Agnostic -\n\nUnlike CloudFormation, Terraform is able to incorporate different \nresource providers\n to manage infrastructure across multiple cloud services (not just AWS).\n\n\nCustom Providers -\n\nTerraform can be extended to manage tools that don't come natively through use of custom providers.\nWe wrote a \nLayer0 provider\n so that Terraform can manage Layer0 resources in addition to tools and resources and infrastructure beyond Layer0's scope.\n\n\nTerraform has some \nthings to say\n on the matter as well.\n\n\nAdvantages Versus Layer0 CLI?\n#\n\n\nWhy should you move from using (or scripting) the Layer0 CLI directly?\n\n\nReduce Fat-Fingering Mistakes -\n\nCreating Terraform files (and using \nterraform plan\n) allows you to review your deployment and catch errors.\nExecuting Layer0 CLI commands one-by-one is tiresome, non-transportable, and a process ripe for typos.\n\n\nGo Beyond Layer0 -\n\nRetain the benefits of leveraging Layer0's concepts and resources using our \nprovider\n, but also gain the ability to orchestrate resources and tools beyond the CLI's scope.\n\n\nHow do I get Terraform?\n#\n\n\nCheck out Terraform's \ndocumentation\n on the subject.",
            "title": "Terraform"
        },
        {
            "location": "/reference/terraform_introduction/#introduction-to-terraform",
            "text": "",
            "title": "Introduction to Terraform"
        },
        {
            "location": "/reference/terraform_introduction/#what-does-terraform-do",
            "text": "Terraform is a powerful orchestration tool for creating, updating, deleting, and otherwise managing infrastructure in an easy-to-understand, declarative manner.\nTerraform's  documentation  is very good, but at a glance:  Be Declarative - \nSpecify desired infrastructure results in Terraform ( *.tf ) files, and let Terraform do the heavy work of figuring out how to make that specification a reality.  Scry the Future - \nUse  terraform plan  to see a list of everything that Terraform  would  do without actually making those changes.  Version Infrastructure - \nCheck Terraform files into a VCS to track changes to and manage versions of your infrastructure.",
            "title": "What does Terraform do?"
        },
        {
            "location": "/reference/terraform_introduction/#why-terraform",
            "text": "Why did we latch onto Terraform instead of something like CloudFormation?  Cloud-Agnostic - \nUnlike CloudFormation, Terraform is able to incorporate different  resource providers  to manage infrastructure across multiple cloud services (not just AWS).  Custom Providers - \nTerraform can be extended to manage tools that don't come natively through use of custom providers.\nWe wrote a  Layer0 provider  so that Terraform can manage Layer0 resources in addition to tools and resources and infrastructure beyond Layer0's scope.  Terraform has some  things to say  on the matter as well.",
            "title": "Why Terraform?"
        },
        {
            "location": "/reference/terraform_introduction/#advantages-versus-layer0-cli",
            "text": "Why should you move from using (or scripting) the Layer0 CLI directly?  Reduce Fat-Fingering Mistakes - \nCreating Terraform files (and using  terraform plan ) allows you to review your deployment and catch errors.\nExecuting Layer0 CLI commands one-by-one is tiresome, non-transportable, and a process ripe for typos.  Go Beyond Layer0 - \nRetain the benefits of leveraging Layer0's concepts and resources using our  provider , but also gain the ability to orchestrate resources and tools beyond the CLI's scope.",
            "title": "Advantages Versus Layer0 CLI?"
        },
        {
            "location": "/reference/terraform_introduction/#how-do-i-get-terraform",
            "text": "Check out Terraform's  documentation  on the subject.",
            "title": "How do I get Terraform?"
        },
        {
            "location": "/reference/terraform-plugin/",
            "text": "Layer0 Terraform Provider Reference\n#\n\n\nTerraform is an open-source tool for provisioning and managing infrastructure.\nIf you are new to Terraform, we recommend checking out their \ndocumentation\n.\n\n\nLayer0 has built a custom \nprovider\n for Layer0.\nThis provider allows users to create, manage, and update Layer0 entities using Terraform.\n\n\nPrerequisites\n#\n\n\n\n\nTerraform v0.11+\n (\ndownload\n), accessible in your system path.\n\n\n\n\nInstall\n#\n\n\nDownload a Layer0 v0.8.4+ \nrelease\n.\nThe Terraform plugin binary is located in the release zip file as \nterraform-provider-layer0\n.\nCopy this \nterraform-provider-layer0\n binary into the same directory as your Terraform binary - and you're done!\n\n\nFor further information, see Terraform's documentation on installing a Terraform plugin \nhere\n.\n\n\nGetting Started\n#\n\n\n\n\nCheckout the \nTerraform\n section of the Guestbook walkthrough \nhere\n.\n\n\nWe've added some tips and links to helpful resources in the \nBest Practices\n section below.\n\n\n\n\n\n\nProvider\n#\n\n\nThe Layer0 provider is used to interact with a Layer0 API.\nThe provider needs to be configured with the proper credentials before it can be used.\n\n\nExample Usage\n#\n\n\n# Add 'endpoint' and 'token' variables\nvariable \"endpoint\" {}\n\nvariable \"token\" {}\n\n# Configure the layer0 provider\nprovider \"layer0\" {\n  endpoint        = \"\n${\nvar\n.\nendpoint\n}\n\"\n  token           = \"\n${\nvar\n.\ntoken\n}\n\"\n  skip_ssl_verify = true\n}\n\n\n\n\n\nArgument Reference\n#\n\n\nThe following arguments are supported:\n\n\n\n\nNote\n\n\nThe \nendpoint\n and \ntoken\n variables for your layer0 api can be found using the \nl0-setup endpoint\n command\n\n\n\n\n\n\nendpoint\n - (Required) The endpoint of the layer0 api\n\n\ntoken\n - (Required) The authentication token for the layer0 api\n\n\nskip_ssl_verify\n - (Optional) If true, ssl certificate mismatch warnings will be ignored\n\n\n\n\n\n\nAPI Data Source\n#\n\n\nThe API data source is used to extract useful read-only variables from the Layer0 API.\n\n\nExample Usage\n#\n\n\n# Configure the api data source\ndata \"layer0_api\" \"config\" {}\n\n# Output the layer0 vpc id\noutput \"vpc id\" {\n  val = \"\n${\ndata\n.\nlayer0_api\n.\nconfig\n.\nvpc_id\n}\n\"\n}\n\n\n\n\n\nAttribute Reference\n#\n\n\nThe following attributes are exported:\n\n\n\n\nprefix\n - The prefix of the layer0 instance\n\n\nvpc_id\n - The vpc id of the layer0 instance\n\n\npublic_subnets\n - A list containing the 2 public subnet ids in the layer0 vpc\n\n\nprivate_subnets\n - A list containing the 2 private subnet ids in the layer0 vpc\n\n\n\n\n\n\nDeploy Data Source\n#\n\n\nThe Deploy data source is used to extract Layer0 Deploy attributes.\n\n\nExample Usage\n#\n\n\n# Configure the deploy data source\ndata \"layer0_deploy\" \"dpl\" {\n  name    = \"my-deploy\"\n  version = \"1\"\n}\n\n# Output the layer0 deploy id\noutput \"deploy_id\" {\n  val = \"\n${\ndata\n.\nlayer0_deploy\n.\ndpl\n.\nid\n}\n\"\n}\n\n\n\n\n\nArgument Reference\n#\n\n\nThe following arguments are supported:\n\n\n\n\nname\n - (Required) The name of the deploy\n\n\nversion\n - (Required) The version of the deploy\n\n\n\n\nAttribute Reference\n#\n\n\nThe following attributes are exported:\n\n\n\n\nname\n - The name of the deploy\n\n\nversion\n - The version of the deploy\n\n\nid\n - The id of the deploy\n\n\n\n\n\n\nEnvironment Data Source\n#\n\n\nThe Environment data source is used to extract Layer0 Environment attributes.\n\n\nExample Usage\n#\n\n\n# Configure the environment data source\ndata \"layer0_environment\" \"env\" {\n  name = \"my-environment\"\n}\n\n# Output the layer0 environment id\noutput \"environment_id\" {\n  val = \"\n${\ndata\n.\nlayer0_environment\n.\nenv\n.\nid\n}\n\"\n}\n\n\n\n\n\nArgument Reference\n#\n\n\nThe following arguments are supported:\n\n\n\n\nname\n - (Required) The name of the environment\n\n\n\n\nAttribute Reference\n#\n\n\nThe following attributes are exported:\n\n\n\n\nid\n - The id of the environment\n\n\nname\n - The name of the environment\n\n\nsize\n - The size of the instances in the environment\n\n\nmin_count\n - The current number instances in the environment\n\n\nos\n - The operating system used for the environment\n\n\nami\n - The AMI ID used for the environment\n\n\n\n\n\n\nLoad Balancer Data Source\n#\n\n\nThe Load Balancer data source is used to extract Layer0 Load Balancer attributes.\n\n\nExample Usage\n#\n\n\n# Configure the load balancer source\ndata \"layer0_load_balancer\" \"lb\" {\n  name           = \"my-loadbalancer\"\n  environment_id = \"\n${\ndata\n.\nlayer0_environment\n.\nenv\n.\nenvironment_id\n}\n\"\n}\n\n# Output the layer0 load balancer id\noutput \"load_balancer_id\" {\n  val = \"\n${\ndata\n.\nlayer0_load_balancer\n.\nlb\n.\nid\n}\n\"\n}\n\n\n\n\n\nArgument Reference\n#\n\n\nThe following arguments are supported:\n\n\n\n\nname\n - (required) The name of the load balancer\n\n\nenvironment_id\n - (required) The id of the environment the load balancer exists in\n\n\n\n\nAttribute Reference\n#\n\n\nThe following attributes are exported:\n\n\n\n\nid\n - The id of the load balancer\n\n\nname\n - The name of the load balancer\n\n\nenvironment_id\n - The id of the environment the load balancer exists in\n\n\nenvironment_name\n - The name of the environment the load balancer exists in\n\n\nprivate\n - Whether or not the load balancer is private\n\n\nurl\n - The URL of the load balancer\n\n\n\n\n\n\nService Data Source\n#\n\n\nThe Service data source is used to extract Layer0 Service attributes.\n\n\nExample Usage\n#\n\n\n# Configure the service data source\ndata \"layer0_service\" \"svc\" {\n  name           = \"my-service\"\n  environment_id = \"\n${\ndata\n.\nlayer0_environment\n.\nenv\n.\nenvironment_id\n}\n\"\n}\n\n# Output the layer0 service id\noutput \"service_id\" {\n  val = \"\n${\ndata\n.\nlayer0_service\n.\nsvc\n.\nid\n}\n\"\n}\n\n\n\n\n\nArgument Reference\n#\n\n\nThe following arguments are supported:\n\n\n\n\nname\n - (required) The name of the service\n\n\nenvironment_id\n - (required) The id of the environment the service exists in\n\n\n\n\nAttribute Reference\n#\n\n\nThe following attributes are exported:\n\n\n\n\nid\n - The id of the service\n\n\nname\n - The name of the service\n\n\nenvironment_id\n - The id of the environment the service exists in\n\n\nenvironment_name\n - The name of the environment the service exists in\n\n\nscale\n - The current desired scale of the service\n\n\n\n\n\n\nDeploy Resource\n#\n\n\nProvides a Layer0 Deploy.\n\n\nPerforming variable substitution inside of your deploy's json file (typically named \nDockerrun.aws.json\n) can be done through Terraform's \ntemplate_file\n.\nFor a working example, please see the sample \nGuestbook\n application\n\n\nExample Usage\n#\n\n\n# Configure the deploy template\ndata \"template_file\" \"guestbook\" {\n  template = \"\n${\nfile\n(\n\"Dockerrun.aws.json\"\n)\n}\n\"\n  vars {\n    docker_image_tag = \"latest\"\n  }\n}\n\n# Create a deploy using the rendered template\nresource \"layer0_deploy\" \"guestbook\" {\n  name    = \"guestbook\"\n  content = \"\n${\ndata\n.\ntemplate_file\n.\nguestbook\n.\nrendered\n}\n\"\n}\n\n\n\n\n\nArgument Reference\n#\n\n\nThe following arguments are supported:\n\n\n\n\nname\n - (Required) The name of the deploy\n\n\ncontent\n - (Required) The content of the deploy\n\n\n\n\nAttribute Reference\n#\n\n\nThe following attributes are exported:\n\n\n\n\nid\n - The id of the deploy\n\n\nname\n - The name of the deploy\n\n\nversion\n - The version number of the deploy\n\n\n\n\n\n\nEnvironment Resource\n#\n\n\nProvides a Layer0 Environment\n\n\nExample Usage\n#\n\n\n# Create a new environment\nresource \"layer0_environment\" \"demo\" {\n  name      = \"demo\"\n  size      = \"m3.medium\"\n  min_count = 0\n  user_data = \"echo hello, world\"\n  os        = \"linux\"\n  ami       = \"ami123\"\n}\n\n\n\n\n\nArgument Reference\n#\n\n\nThe following arguments are supported:\n\n\n\n\nname\n - (Required) The name of the environment\n\n\nsize\n - (Optional, Default: \"m3.medium\") The size of the instances in the environment.\nAvailable instance sizes can be found \nhere\n\n\nmin_count\n - (Optional, Default: 0) The minimum number of instances allowed in the environment\n\n\nuser-data\n - (Optional) The user data template to use for the environment's autoscaling group.\nSee the \ncli reference\n for the default template.\n\n\nos\n - (Optional, Default: \"linux\") Specifies the type of operating system used in the environment.\nOptions are \"linux\" or \"windows\".\n\n\nami\n - (Optional) A custom AMI ID to use in the environment. \nIf not specified, Layer0 will use its default AMI ID for the specified operating system.\n\n\n\n\nAttribute Reference\n#\n\n\nThe following attributes are exported:\n\n\n\n\nid\n - The id of the environment\n\n\nname\n - The name of the environment\n\n\nsize\n - The size of the instances in the environment\n\n\ncluster_count\n - The current number instances in the environment\n\n\nsecurity_group_id\n - The ID of the environment's security group\n\n\nos\n - The operating system used for the environment\n\n\nami\n - The AMI ID used for the environment\n\n\n\n\n\n\nLoad Balancer Resource\n#\n\n\nProvides a Layer0 Load Balancer\n\n\nExample Usage\n#\n\n\n# Create a new load balancer\nresource \"layer0_load_balancer\" \"guestbook\" {\n  name        = \"guestbook\"\n  environment = \"demo123\"\n  private     = false\n\n  port {\n    host_port      = 80\n    container_port = 80\n    protocol       = \"http\"\n  }\n\n  port {\n    host_port      = 443\n    container_port = 443\n    protocol       = \"https\"\n    certificate    = \"cert\"\n  }\n\n  health_check {\n    target              = \"tcp:80\"\n    interval            = 30\n    timeout             = 5\n    healthy_threshold   = 2\n    unhealthy_threshold = 2\n  }\n}\n\n\n\n\n\nArgument Reference\n#\n\n\nThe following arguments are supported:\n\n\n\n\nname\n - (Required) The name of the load balancer\n\n\nenvironment\n - (Required) The id of the environment to place the load balancer inside of\n\n\nprivate\n - (Optional) If true, the load balancer will not be exposed to the public internet\n\n\nport\n - (Optional, Default: 80:80/tcp) A list of port blocks. Ports documented below\n\n\nhealth_check\n - (Optional, Default: \n{\"TCP:80\" 30 5 2 2}\n) A health_check block. Health check documented below\n\n\n\n\nPorts (\nport\n) support the following:\n\n\n\n\nhost_port\n - (Required) The port on the load balancer to listen on\n\n\ncontainer_port\n - (Required) The port on the docker container to route to\n\n\nprotocol\n - (Required) The protocol to listen on. Valid values are \nHTTP, HTTPS, TCP, or SSL\n\n\ncertificate\n - (Optional) The name of an SSL certificate. Only required if the \nHTTP\n or \nSSL\n protocol is used.\n\n\n\n\nHealthcheck (\nhealth_check\n) supports the following:\n\n\n\n\ntarget\n - (Required) The target of the check. Valid pattern is \nPROTOCOL:PORT/PATH\n, where \nPROTOCOL\n values are:\n\n\nHTTP\n, \nHTTPS\n - \nPORT\n and \nPATH\n are required\n\n\nTCP\n, \nSSL\n - \nPORT\n is required, \nPATH\n is not supported\n\n\n\n\n\n\ninterval\n - (Required) The interval between checks.\n\n\ntimeout\n - (Required) The length of time before the check times out.\n\n\nhealthy_threshold\n - (Required) The number of checks before the instance is declared healthy.\n\n\nunhealthy_threshold\n - (Required) The number of checks before the instance is declared unhealthy.\n\n\n\n\nAttribute Reference\n#\n\n\nThe following attributes are exported:\n\n\n\n\nid\n - The id of the load balancer\n\n\nname\n - The name of the load balancer\n\n\nenvironment\n - The id of the environment the load balancer exists in\n\n\nprivate\n - Whether or not the load balancer is private\n\n\nurl\n - The URL of the load balancer\n\n\n\n\n\n\nService Resource\n#\n\n\nProvides a Layer0 Service\n\n\nExample Usage\n#\n\n\n# Create a new service\nresource \"layer0_service\" \"guestbook\" {\n  name          = \"guestbook\"\n  environment   = \"environment123\"\n  deploy        = \"deploy123\"\n  load_balancer = \"loadbalancer123\"\n  scale         = 3\n}\n\n\n\n\n\nArgument Reference\n#\n\n\nThe following arguments are supported:\n\n\n\n\nname\n - (Required) The name of the service\n\n\nenvironment\n - (Required) The id of the environment to place the service inside of\n\n\ndeploy\n - (Required) The id of the deploy for the service to run\n\n\nload_balancer\n (Optional) The id of the load balancer to place the service behind\n\n\nscale\n (Optional, Default: 1) The number of copies of the service to run\n\n\n\n\nAttribute Reference\n#\n\n\nThe following attributes are exported:\n\n\n\n\nid\n - The id of the service\n\n\nname\n - The name of the service\n\n\nenvironment\n - The id of the environment the service exists in\n\n\ndeploy\n - The id of the deploy the service is running\n\n\nload_balancer\n - The id of the load balancer the service is behind (if \nload_balancer\n was set)\n\n\nscale\n - The current desired scale of the service\n\n\n\n\n\n\nBest Practices\n#\n\n\n\n\nAlways run \nTerraform plan\n before \nterraform apply\n.\nThis will show you what action(s) Terraform plans to make before actually executing them.\n\n\nUse \nvariables\n to reference secrets.\nSecrets can be placed in a file named \nterraform.tfvars\n, or by setting \nTF_VAR_*\n environment variables.\nMore information can be found \nhere\n.\n\n\nUse Terraform's \nremote\n command to backup and sync your \nterraform.tfstate\n file across different members in your organization.\nTerraform has documentation for using S3 as a backend \nhere\n.\n\n\nTerraform \nmodules\n allow you to define and consume reusable components.\n\n\nExample configurations can be found \nhere",
            "title": "Layer0 Terraform Plugin"
        },
        {
            "location": "/reference/terraform-plugin/#layer0-terraform-provider-reference",
            "text": "Terraform is an open-source tool for provisioning and managing infrastructure.\nIf you are new to Terraform, we recommend checking out their  documentation .  Layer0 has built a custom  provider  for Layer0.\nThis provider allows users to create, manage, and update Layer0 entities using Terraform.",
            "title": "Layer0 Terraform Provider Reference"
        },
        {
            "location": "/reference/terraform-plugin/#prerequisites",
            "text": "Terraform v0.11+  ( download ), accessible in your system path.",
            "title": "Prerequisites"
        },
        {
            "location": "/reference/terraform-plugin/#install",
            "text": "Download a Layer0 v0.8.4+  release .\nThe Terraform plugin binary is located in the release zip file as  terraform-provider-layer0 .\nCopy this  terraform-provider-layer0  binary into the same directory as your Terraform binary - and you're done!  For further information, see Terraform's documentation on installing a Terraform plugin  here .",
            "title": "Install"
        },
        {
            "location": "/reference/terraform-plugin/#getting-started",
            "text": "Checkout the  Terraform  section of the Guestbook walkthrough  here .  We've added some tips and links to helpful resources in the  Best Practices  section below.",
            "title": "Getting Started"
        },
        {
            "location": "/reference/terraform-plugin/#provider",
            "text": "The Layer0 provider is used to interact with a Layer0 API.\nThe provider needs to be configured with the proper credentials before it can be used.",
            "title": "Provider"
        },
        {
            "location": "/reference/terraform-plugin/#example-usage",
            "text": "# Add 'endpoint' and 'token' variables\nvariable \"endpoint\" {}\n\nvariable \"token\" {}\n\n# Configure the layer0 provider\nprovider \"layer0\" {\n  endpoint        = \" ${ var . endpoint } \"\n  token           = \" ${ var . token } \"\n  skip_ssl_verify = true\n}",
            "title": "Example Usage"
        },
        {
            "location": "/reference/terraform-plugin/#argument-reference",
            "text": "The following arguments are supported:   Note  The  endpoint  and  token  variables for your layer0 api can be found using the  l0-setup endpoint  command    endpoint  - (Required) The endpoint of the layer0 api  token  - (Required) The authentication token for the layer0 api  skip_ssl_verify  - (Optional) If true, ssl certificate mismatch warnings will be ignored",
            "title": "Argument Reference"
        },
        {
            "location": "/reference/terraform-plugin/#api-data-source",
            "text": "The API data source is used to extract useful read-only variables from the Layer0 API.",
            "title": "API Data Source"
        },
        {
            "location": "/reference/terraform-plugin/#example-usage_1",
            "text": "# Configure the api data source\ndata \"layer0_api\" \"config\" {}\n\n# Output the layer0 vpc id\noutput \"vpc id\" {\n  val = \" ${ data . layer0_api . config . vpc_id } \"\n}",
            "title": "Example Usage"
        },
        {
            "location": "/reference/terraform-plugin/#attribute-reference",
            "text": "The following attributes are exported:   prefix  - The prefix of the layer0 instance  vpc_id  - The vpc id of the layer0 instance  public_subnets  - A list containing the 2 public subnet ids in the layer0 vpc  private_subnets  - A list containing the 2 private subnet ids in the layer0 vpc",
            "title": "Attribute Reference"
        },
        {
            "location": "/reference/terraform-plugin/#deploy-data-source",
            "text": "The Deploy data source is used to extract Layer0 Deploy attributes.",
            "title": "Deploy Data Source"
        },
        {
            "location": "/reference/terraform-plugin/#example-usage_2",
            "text": "# Configure the deploy data source\ndata \"layer0_deploy\" \"dpl\" {\n  name    = \"my-deploy\"\n  version = \"1\"\n}\n\n# Output the layer0 deploy id\noutput \"deploy_id\" {\n  val = \" ${ data . layer0_deploy . dpl . id } \"\n}",
            "title": "Example Usage"
        },
        {
            "location": "/reference/terraform-plugin/#argument-reference_1",
            "text": "The following arguments are supported:   name  - (Required) The name of the deploy  version  - (Required) The version of the deploy",
            "title": "Argument Reference"
        },
        {
            "location": "/reference/terraform-plugin/#attribute-reference_1",
            "text": "The following attributes are exported:   name  - The name of the deploy  version  - The version of the deploy  id  - The id of the deploy",
            "title": "Attribute Reference"
        },
        {
            "location": "/reference/terraform-plugin/#environment-data-source",
            "text": "The Environment data source is used to extract Layer0 Environment attributes.",
            "title": "Environment Data Source"
        },
        {
            "location": "/reference/terraform-plugin/#example-usage_3",
            "text": "# Configure the environment data source\ndata \"layer0_environment\" \"env\" {\n  name = \"my-environment\"\n}\n\n# Output the layer0 environment id\noutput \"environment_id\" {\n  val = \" ${ data . layer0_environment . env . id } \"\n}",
            "title": "Example Usage"
        },
        {
            "location": "/reference/terraform-plugin/#argument-reference_2",
            "text": "The following arguments are supported:   name  - (Required) The name of the environment",
            "title": "Argument Reference"
        },
        {
            "location": "/reference/terraform-plugin/#attribute-reference_2",
            "text": "The following attributes are exported:   id  - The id of the environment  name  - The name of the environment  size  - The size of the instances in the environment  min_count  - The current number instances in the environment  os  - The operating system used for the environment  ami  - The AMI ID used for the environment",
            "title": "Attribute Reference"
        },
        {
            "location": "/reference/terraform-plugin/#load-balancer-data-source",
            "text": "The Load Balancer data source is used to extract Layer0 Load Balancer attributes.",
            "title": "Load Balancer Data Source"
        },
        {
            "location": "/reference/terraform-plugin/#example-usage_4",
            "text": "# Configure the load balancer source\ndata \"layer0_load_balancer\" \"lb\" {\n  name           = \"my-loadbalancer\"\n  environment_id = \" ${ data . layer0_environment . env . environment_id } \"\n}\n\n# Output the layer0 load balancer id\noutput \"load_balancer_id\" {\n  val = \" ${ data . layer0_load_balancer . lb . id } \"\n}",
            "title": "Example Usage"
        },
        {
            "location": "/reference/terraform-plugin/#argument-reference_3",
            "text": "The following arguments are supported:   name  - (required) The name of the load balancer  environment_id  - (required) The id of the environment the load balancer exists in",
            "title": "Argument Reference"
        },
        {
            "location": "/reference/terraform-plugin/#attribute-reference_3",
            "text": "The following attributes are exported:   id  - The id of the load balancer  name  - The name of the load balancer  environment_id  - The id of the environment the load balancer exists in  environment_name  - The name of the environment the load balancer exists in  private  - Whether or not the load balancer is private  url  - The URL of the load balancer",
            "title": "Attribute Reference"
        },
        {
            "location": "/reference/terraform-plugin/#service-data-source",
            "text": "The Service data source is used to extract Layer0 Service attributes.",
            "title": "Service Data Source"
        },
        {
            "location": "/reference/terraform-plugin/#example-usage_5",
            "text": "# Configure the service data source\ndata \"layer0_service\" \"svc\" {\n  name           = \"my-service\"\n  environment_id = \" ${ data . layer0_environment . env . environment_id } \"\n}\n\n# Output the layer0 service id\noutput \"service_id\" {\n  val = \" ${ data . layer0_service . svc . id } \"\n}",
            "title": "Example Usage"
        },
        {
            "location": "/reference/terraform-plugin/#argument-reference_4",
            "text": "The following arguments are supported:   name  - (required) The name of the service  environment_id  - (required) The id of the environment the service exists in",
            "title": "Argument Reference"
        },
        {
            "location": "/reference/terraform-plugin/#attribute-reference_4",
            "text": "The following attributes are exported:   id  - The id of the service  name  - The name of the service  environment_id  - The id of the environment the service exists in  environment_name  - The name of the environment the service exists in  scale  - The current desired scale of the service",
            "title": "Attribute Reference"
        },
        {
            "location": "/reference/terraform-plugin/#deploy-resource",
            "text": "Provides a Layer0 Deploy.  Performing variable substitution inside of your deploy's json file (typically named  Dockerrun.aws.json ) can be done through Terraform's  template_file .\nFor a working example, please see the sample  Guestbook  application",
            "title": "Deploy Resource"
        },
        {
            "location": "/reference/terraform-plugin/#example-usage_6",
            "text": "# Configure the deploy template\ndata \"template_file\" \"guestbook\" {\n  template = \" ${ file ( \"Dockerrun.aws.json\" ) } \"\n  vars {\n    docker_image_tag = \"latest\"\n  }\n}\n\n# Create a deploy using the rendered template\nresource \"layer0_deploy\" \"guestbook\" {\n  name    = \"guestbook\"\n  content = \" ${ data . template_file . guestbook . rendered } \"\n}",
            "title": "Example Usage"
        },
        {
            "location": "/reference/terraform-plugin/#argument-reference_5",
            "text": "The following arguments are supported:   name  - (Required) The name of the deploy  content  - (Required) The content of the deploy",
            "title": "Argument Reference"
        },
        {
            "location": "/reference/terraform-plugin/#attribute-reference_5",
            "text": "The following attributes are exported:   id  - The id of the deploy  name  - The name of the deploy  version  - The version number of the deploy",
            "title": "Attribute Reference"
        },
        {
            "location": "/reference/terraform-plugin/#environment-resource",
            "text": "Provides a Layer0 Environment",
            "title": "Environment Resource"
        },
        {
            "location": "/reference/terraform-plugin/#example-usage_7",
            "text": "# Create a new environment\nresource \"layer0_environment\" \"demo\" {\n  name      = \"demo\"\n  size      = \"m3.medium\"\n  min_count = 0\n  user_data = \"echo hello, world\"\n  os        = \"linux\"\n  ami       = \"ami123\"\n}",
            "title": "Example Usage"
        },
        {
            "location": "/reference/terraform-plugin/#argument-reference_6",
            "text": "The following arguments are supported:   name  - (Required) The name of the environment  size  - (Optional, Default: \"m3.medium\") The size of the instances in the environment.\nAvailable instance sizes can be found  here  min_count  - (Optional, Default: 0) The minimum number of instances allowed in the environment  user-data  - (Optional) The user data template to use for the environment's autoscaling group.\nSee the  cli reference  for the default template.  os  - (Optional, Default: \"linux\") Specifies the type of operating system used in the environment.\nOptions are \"linux\" or \"windows\".  ami  - (Optional) A custom AMI ID to use in the environment. \nIf not specified, Layer0 will use its default AMI ID for the specified operating system.",
            "title": "Argument Reference"
        },
        {
            "location": "/reference/terraform-plugin/#attribute-reference_6",
            "text": "The following attributes are exported:   id  - The id of the environment  name  - The name of the environment  size  - The size of the instances in the environment  cluster_count  - The current number instances in the environment  security_group_id  - The ID of the environment's security group  os  - The operating system used for the environment  ami  - The AMI ID used for the environment",
            "title": "Attribute Reference"
        },
        {
            "location": "/reference/terraform-plugin/#load-balancer-resource",
            "text": "Provides a Layer0 Load Balancer",
            "title": "Load Balancer Resource"
        },
        {
            "location": "/reference/terraform-plugin/#example-usage_8",
            "text": "# Create a new load balancer\nresource \"layer0_load_balancer\" \"guestbook\" {\n  name        = \"guestbook\"\n  environment = \"demo123\"\n  private     = false\n\n  port {\n    host_port      = 80\n    container_port = 80\n    protocol       = \"http\"\n  }\n\n  port {\n    host_port      = 443\n    container_port = 443\n    protocol       = \"https\"\n    certificate    = \"cert\"\n  }\n\n  health_check {\n    target              = \"tcp:80\"\n    interval            = 30\n    timeout             = 5\n    healthy_threshold   = 2\n    unhealthy_threshold = 2\n  }\n}",
            "title": "Example Usage"
        },
        {
            "location": "/reference/terraform-plugin/#argument-reference_7",
            "text": "The following arguments are supported:   name  - (Required) The name of the load balancer  environment  - (Required) The id of the environment to place the load balancer inside of  private  - (Optional) If true, the load balancer will not be exposed to the public internet  port  - (Optional, Default: 80:80/tcp) A list of port blocks. Ports documented below  health_check  - (Optional, Default:  {\"TCP:80\" 30 5 2 2} ) A health_check block. Health check documented below   Ports ( port ) support the following:   host_port  - (Required) The port on the load balancer to listen on  container_port  - (Required) The port on the docker container to route to  protocol  - (Required) The protocol to listen on. Valid values are  HTTP, HTTPS, TCP, or SSL  certificate  - (Optional) The name of an SSL certificate. Only required if the  HTTP  or  SSL  protocol is used.   Healthcheck ( health_check ) supports the following:   target  - (Required) The target of the check. Valid pattern is  PROTOCOL:PORT/PATH , where  PROTOCOL  values are:  HTTP ,  HTTPS  -  PORT  and  PATH  are required  TCP ,  SSL  -  PORT  is required,  PATH  is not supported    interval  - (Required) The interval between checks.  timeout  - (Required) The length of time before the check times out.  healthy_threshold  - (Required) The number of checks before the instance is declared healthy.  unhealthy_threshold  - (Required) The number of checks before the instance is declared unhealthy.",
            "title": "Argument Reference"
        },
        {
            "location": "/reference/terraform-plugin/#attribute-reference_7",
            "text": "The following attributes are exported:   id  - The id of the load balancer  name  - The name of the load balancer  environment  - The id of the environment the load balancer exists in  private  - Whether or not the load balancer is private  url  - The URL of the load balancer",
            "title": "Attribute Reference"
        },
        {
            "location": "/reference/terraform-plugin/#service-resource",
            "text": "Provides a Layer0 Service",
            "title": "Service Resource"
        },
        {
            "location": "/reference/terraform-plugin/#example-usage_9",
            "text": "# Create a new service\nresource \"layer0_service\" \"guestbook\" {\n  name          = \"guestbook\"\n  environment   = \"environment123\"\n  deploy        = \"deploy123\"\n  load_balancer = \"loadbalancer123\"\n  scale         = 3\n}",
            "title": "Example Usage"
        },
        {
            "location": "/reference/terraform-plugin/#argument-reference_8",
            "text": "The following arguments are supported:   name  - (Required) The name of the service  environment  - (Required) The id of the environment to place the service inside of  deploy  - (Required) The id of the deploy for the service to run  load_balancer  (Optional) The id of the load balancer to place the service behind  scale  (Optional, Default: 1) The number of copies of the service to run",
            "title": "Argument Reference"
        },
        {
            "location": "/reference/terraform-plugin/#attribute-reference_8",
            "text": "The following attributes are exported:   id  - The id of the service  name  - The name of the service  environment  - The id of the environment the service exists in  deploy  - The id of the deploy the service is running  load_balancer  - The id of the load balancer the service is behind (if  load_balancer  was set)  scale  - The current desired scale of the service",
            "title": "Attribute Reference"
        },
        {
            "location": "/reference/terraform-plugin/#best-practices",
            "text": "Always run  Terraform plan  before  terraform apply .\nThis will show you what action(s) Terraform plans to make before actually executing them.  Use  variables  to reference secrets.\nSecrets can be placed in a file named  terraform.tfvars , or by setting  TF_VAR_*  environment variables.\nMore information can be found  here .  Use Terraform's  remote  command to backup and sync your  terraform.tfstate  file across different members in your organization.\nTerraform has documentation for using S3 as a backend  here .  Terraform  modules  allow you to define and consume reusable components.  Example configurations can be found  here",
            "title": "Best Practices"
        },
        {
            "location": "/reference/updateservice/",
            "text": "Updating a Layer0 service\n#\n\n\nThere are three methods of updating an existing Layer0 service. The first method is to update the existing Deploy to refer to a new Docker task definition. The second method is to create a new Service that uses the same Loadbalancer. The third method is to create both a new Loadbalancer and a new Service.\n\n\nThere are advantages and disadvantages to each of these methods. The following sections discuss the advantages and disadvantages of using each method, and include procedures for implementing each method.\n\n\nMethod 1: Refer to a new task definition\n#\n\n\nThis method of updating a Layer0 application is the easiest to implement, because you do not need to rescale the Service or modify the Loadbalancer. This method is completely transparent to all other components of the application, and using this method does not involve any downtime.\n\n\nThe disadvantage of using this method is that you cannot perform A/B testing of the old and new services, and you cannot control which traffic goes to the old service and which goes to the new one.\n\n\nTo replace a Deploy to refer to a new task definition:\n\n\nAt the command line, type the following to create a new Deploy:\n\n\nl0 deploy create taskDefPath deployName\n\n\n\n\n\ntaskDefPath\n is the path to the ECS Task Definition. Note that if \ndeployName\n already exists, this step will create a new version of that Deploy.\n\n\nUse \nl0 service update\n to update the existing service:\n\n\nl0 service update serviceName deployName[:deployVersion]\n\n\n\n\n\nBy default, the service name you specify in this command will refer to the latest version of \ndeployName\n. You can optionally specify a specific version of the deploy, as shown above.\n\n\nMethod 2: Create a new Deploy and Service using the same Loadbalancer\n#\n\n\nThis method of updating a Layer0 application is also rather easy to implement. Like the method described in the previous section, this method is completely transparent to all other services and components of the application. This method also you allows you to re-scale the service if necessary, using the \nl0 service scale\n command. Finally, this method allows for indirect A/B testing of the application; you can change the scale of the application, and observe the success and failure rates.\n\n\nThe disadvantage of using this method is that you cannot control the routing of traffic between the old and new versions of the application.\n\n\nTo create a new Deploy and Service:\n\n\nAt the command line, type the following to create a new deploy or a new version of a deploy:\n\n\nl0 deploy create taskDefPath deployName\n\n\n\n\n\ntaskDefPath\n is the path to the ECS Task Definition. Note that if \ndeployName\n already exists, this step will create a new version of that Deploy.\n\n\nUse \nl0 service create\n to create a new service that uses \ndeployName\n behind an existing load balancer named \nloadBalancerName\n\n\nl0 service create --loadbalancer [environmentName:]loadBalancerName environmentName serviceName deployName[:deployVersion]\n\n\n\n\n\nBy default, the service name you specify in this command will refer to the latest version of \ndeployName\n. You can optionally specify a specific version of the deploy, as shown above. You can also optionally specify the name of the environment, \nenvironmentName\n where the load balancer exists. \n\n\nCheck to make sure that the new service is working as expected. If it is, and you do not want to keep the old service, delete the old service: \n\n\nl0 service delete service\n\n\n\n\n\nMethod 3: Create a new Deploy, Loadbalancer and Service\n#\n\n\nThe final method of updating a Layer0 service is to create an entirely new Deploy, Load Balancer and Service. This method gives you complete control over both the new and the old Service, and allows you to perform true A/B testing by routing traffic to individual Services.\n\n\nThe disadvantage of using this method is that you need to implement a method of routing traffic between the new and the old Load Balancer.\n\n\nTo create a new Deploy, Load Balancer and Service:\n\n\nType the following command to create a new Deploy:\n\n\nl0 deploy create taskDefPath deployName\n\n\n\n\n\ntaskDefPath\n is the path to the ECS Task Definition. Note that if \ndeployName\n already exists, this step will create a new version of that Deploy.\n\n\nUse \nl0 loadbalancer create\n to create a new Load Balancer:\n\n\nl0 loadbalancer create --port port environmentName loadBalancerName deployName\n\n\n\n\n\n\n\nport\n is the port configuration for the listener of the Load Balancer. Valid pattern is \nhostPort:containerPort/protocol\n. Multiple ports can be specified using \n--port port1 --port port2 ...\n.\n\n\nhostPort\n - The port that the load balancer will listen for traffic on.\n\n\ncontainerPort\n - The port that the load balancer will forward traffic to.\n\n\nprotocol\n - The protocol to use when forwarding traffic (acceptable values: TCP, SSL, HTTP, and HTTPS).\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nThe value of \nloadbalancerName\n in the above command must be unique to the Environment.\n\n\n\n\nUse \nl0 service create\n to create a new Service using the Load Balancer you just created: \n\n\nl0 service create --loadbalancer loadBalancerName environmentName serviceName deployName\n\n\n\n\n\n\n\nNote\n\n\nThe value of \nserviceName\n in the above command  must be unique to the Environment.\n\n\n\n\nImplement a method of routing traffic between the old and new Services, such as \nHAProxy\n or \nConsul\n.",
            "title": "Updating a Service"
        },
        {
            "location": "/reference/updateservice/#updating-a-layer0-service",
            "text": "There are three methods of updating an existing Layer0 service. The first method is to update the existing Deploy to refer to a new Docker task definition. The second method is to create a new Service that uses the same Loadbalancer. The third method is to create both a new Loadbalancer and a new Service.  There are advantages and disadvantages to each of these methods. The following sections discuss the advantages and disadvantages of using each method, and include procedures for implementing each method.",
            "title": "Updating a Layer0 service"
        },
        {
            "location": "/reference/updateservice/#method-1-refer-to-a-new-task-definition",
            "text": "This method of updating a Layer0 application is the easiest to implement, because you do not need to rescale the Service or modify the Loadbalancer. This method is completely transparent to all other components of the application, and using this method does not involve any downtime.  The disadvantage of using this method is that you cannot perform A/B testing of the old and new services, and you cannot control which traffic goes to the old service and which goes to the new one.  To replace a Deploy to refer to a new task definition:  At the command line, type the following to create a new Deploy:  l0 deploy create taskDefPath deployName  taskDefPath  is the path to the ECS Task Definition. Note that if  deployName  already exists, this step will create a new version of that Deploy.  Use  l0 service update  to update the existing service:  l0 service update serviceName deployName[:deployVersion]  By default, the service name you specify in this command will refer to the latest version of  deployName . You can optionally specify a specific version of the deploy, as shown above.",
            "title": "Method 1: Refer to a new task definition"
        },
        {
            "location": "/reference/updateservice/#method-2-create-a-new-deploy-and-service-using-the-same-loadbalancer",
            "text": "This method of updating a Layer0 application is also rather easy to implement. Like the method described in the previous section, this method is completely transparent to all other services and components of the application. This method also you allows you to re-scale the service if necessary, using the  l0 service scale  command. Finally, this method allows for indirect A/B testing of the application; you can change the scale of the application, and observe the success and failure rates.  The disadvantage of using this method is that you cannot control the routing of traffic between the old and new versions of the application.  To create a new Deploy and Service:  At the command line, type the following to create a new deploy or a new version of a deploy:  l0 deploy create taskDefPath deployName  taskDefPath  is the path to the ECS Task Definition. Note that if  deployName  already exists, this step will create a new version of that Deploy.  Use  l0 service create  to create a new service that uses  deployName  behind an existing load balancer named  loadBalancerName  l0 service create --loadbalancer [environmentName:]loadBalancerName environmentName serviceName deployName[:deployVersion]  By default, the service name you specify in this command will refer to the latest version of  deployName . You can optionally specify a specific version of the deploy, as shown above. You can also optionally specify the name of the environment,  environmentName  where the load balancer exists.   Check to make sure that the new service is working as expected. If it is, and you do not want to keep the old service, delete the old service:   l0 service delete service",
            "title": "Method 2: Create a new Deploy and Service using the same Loadbalancer"
        },
        {
            "location": "/reference/updateservice/#method-3-create-a-new-deploy-loadbalancer-and-service",
            "text": "The final method of updating a Layer0 service is to create an entirely new Deploy, Load Balancer and Service. This method gives you complete control over both the new and the old Service, and allows you to perform true A/B testing by routing traffic to individual Services.  The disadvantage of using this method is that you need to implement a method of routing traffic between the new and the old Load Balancer.  To create a new Deploy, Load Balancer and Service:  Type the following command to create a new Deploy:  l0 deploy create taskDefPath deployName  taskDefPath  is the path to the ECS Task Definition. Note that if  deployName  already exists, this step will create a new version of that Deploy.  Use  l0 loadbalancer create  to create a new Load Balancer:  l0 loadbalancer create --port port environmentName loadBalancerName deployName   port  is the port configuration for the listener of the Load Balancer. Valid pattern is  hostPort:containerPort/protocol . Multiple ports can be specified using  --port port1 --port port2 ... .  hostPort  - The port that the load balancer will listen for traffic on.  containerPort  - The port that the load balancer will forward traffic to.  protocol  - The protocol to use when forwarding traffic (acceptable values: TCP, SSL, HTTP, and HTTPS).      Note  The value of  loadbalancerName  in the above command must be unique to the Environment.   Use  l0 service create  to create a new Service using the Load Balancer you just created:   l0 service create --loadbalancer loadBalancerName environmentName serviceName deployName   Note  The value of  serviceName  in the above command  must be unique to the Environment.   Implement a method of routing traffic between the old and new Services, such as  HAProxy  or  Consul .",
            "title": "Method 3: Create a new Deploy, Loadbalancer and Service"
        },
        {
            "location": "/reference/consul/",
            "text": "Consul reference\n#\n\n\nConsul\n is an open-source tool for discovering and configuring services in your network architecture. Specifically, Consul provides the following features:\n\n\n\n\nDiscovery of services\n\n\nMonitoring of the health of services\n\n\nKey/value storage with a simple HTTP API\n\n\n\n\nConsul Agent\n#\n\n\nThe \nConsul Agent\n exposes a DNS API for easy consumption of data generated by \nRegistrator\n. The Consul Agent can run either in server or client mode.\n\n\nWhen run as a Layer0 Service, the Consul Agent runs in server mode. To ensure the integrity of your data, the service in which you run consul should be scaled to size 3 or greater. A group of several consul deployments is known as a \"\ncluster\n.\"\n\n\nOther Layer0 Services that use Consul will run the Consul Agent in client mode, alongside their application containers.\nThe client is a very lightweight process that registers services, runs health checks, and forwards queries to servers.\n\n\nRegistrator\n#\n\n\nRegistrator\n is a tool that automatically registers and deregisters services into a Consul Cluster by inspecting Docker containers as they come online.\nContainer registration is based off of environment variables on the container.\nLayer0 Services that use Consul will run Registrator alongside their application containers.\n\n\nService Configuration\n#\n\n\nLayer0 Services that use Consul will need to add the \nRegistrator\n and \nConsul Agent\n definitions to the\n\ncontainerDefinitions\n section of your Deploys. You must also add the \nDocker Socket\n definition to the \nvolumes\n section of your Deploys.\n\n\n\n\nRegistrator Container Definition\n#\n\n\n{\n    \"name\": \"registrator\",\n    \"image\": \"gliderlabs/registrator:master\",\n    \"essential\": true,\n    \"links\": [\"consul-agent\"],\n    \"entrypoint\": [\"/bin/sh\", \"-c\"],\n    \"command\": [\"/bin/registrator -retry-attempts=-1 -retry-interval=30000 -ip $(wget http://169.254.169.254/latest/meta-data/local-ipv4 -q -O -) consul://consul-agent:8500\"],\n    \"memory\": 128,\n    \"mountPoints\": [\n        {\n            \"sourceVolume\": \"dockersocket\",\n            \"containerPath\": \"/tmp/docker.sock\"\n        }\n    ]\n},\n\n\n\n\n\n\n\nConsul Agent Container Definition\n#\n\n\n\n\nWarning\n\n\nYou must replace \n<url>\n with your Layer0 Consul Load Balancer's URL.\n\n\n\n\n{\n    \"name\": \"consul-agent\",\n    \"image\": \"progrium/consul\",\n    \"essential\": true,\n    \"entrypoint\": [\"/bin/bash\", \"-c\"],\n    \"command\": [\"/bin/start -advertise $(wget http://169.254.169.254/latest/meta-data/local-ipv4 -q -O -) -retry-join $EXTERNAL_URL -recursor $UPSTREAM_DNS -retry-interval 30s\"],\n    \"memory\": 128,\n    \"portMappings\": [\n        {\n            \"hostPort\": 8500,\n            \"containerPort\": 8500\n        },\n        {\n            \"hostPort\": 53,\n            \"containerPort\": 53,\n            \"protocol\": \"udp\"\n        }\n    ],\n    \"environment\": [\n        {\n            \"name\": \"EXTERNAL_URL\",\n            \"value\": \"<url>\"\n    },\n    {\n            \"name\": \"UPSTREAM_DNS\",\n            \"value\": \"10.100.0.2\"\n        }\n    ]\n},\n\n\n\n\n\nEnvironment Variables\n#\n\n\n\n\nEXTERNAL_URL\n - URL of the consul cluster\n\n\nUPSTREAM_DNS\n - The DNS server consul-agent queries for DNS entries that it cannot resolve internally (e.g. google.com)\n\n\nThe default value for \nUPSTREAM_DNS\n assumes you're using the default Layer0 configuration, making your internal DNS endpoint 10.100.0.2.  If you are a using a non standard configuration (e.g. installing Layer0 in an existing VPC with a CIDR other than \n10.100.0.0/16\n) please modify this variable accordingly.\n\n\n\n\n\n\n\n\n\n\nDocker Socket Volume Definition\n#\n\n\n\"volumes\": [\n    {\n        \"name\": \"dockersocket\",\n        \"host\": {\n                \"sourcePath\": \"/var/run/docker.sock\"\n        }\n    }\n],",
            "title": "Consul"
        },
        {
            "location": "/reference/consul/#consul-reference",
            "text": "Consul  is an open-source tool for discovering and configuring services in your network architecture. Specifically, Consul provides the following features:   Discovery of services  Monitoring of the health of services  Key/value storage with a simple HTTP API",
            "title": "Consul reference"
        },
        {
            "location": "/reference/consul/#consul-agent",
            "text": "The  Consul Agent  exposes a DNS API for easy consumption of data generated by  Registrator . The Consul Agent can run either in server or client mode.  When run as a Layer0 Service, the Consul Agent runs in server mode. To ensure the integrity of your data, the service in which you run consul should be scaled to size 3 or greater. A group of several consul deployments is known as a \" cluster .\"  Other Layer0 Services that use Consul will run the Consul Agent in client mode, alongside their application containers.\nThe client is a very lightweight process that registers services, runs health checks, and forwards queries to servers.",
            "title": "Consul Agent"
        },
        {
            "location": "/reference/consul/#registrator",
            "text": "Registrator  is a tool that automatically registers and deregisters services into a Consul Cluster by inspecting Docker containers as they come online.\nContainer registration is based off of environment variables on the container.\nLayer0 Services that use Consul will run Registrator alongside their application containers.",
            "title": "Registrator"
        },
        {
            "location": "/reference/consul/#service-configuration",
            "text": "Layer0 Services that use Consul will need to add the  Registrator  and  Consul Agent  definitions to the containerDefinitions  section of your Deploys. You must also add the  Docker Socket  definition to the  volumes  section of your Deploys.",
            "title": "Service Configuration"
        },
        {
            "location": "/reference/consul/#registrator-container-definition",
            "text": "{\n    \"name\": \"registrator\",\n    \"image\": \"gliderlabs/registrator:master\",\n    \"essential\": true,\n    \"links\": [\"consul-agent\"],\n    \"entrypoint\": [\"/bin/sh\", \"-c\"],\n    \"command\": [\"/bin/registrator -retry-attempts=-1 -retry-interval=30000 -ip $(wget http://169.254.169.254/latest/meta-data/local-ipv4 -q -O -) consul://consul-agent:8500\"],\n    \"memory\": 128,\n    \"mountPoints\": [\n        {\n            \"sourceVolume\": \"dockersocket\",\n            \"containerPath\": \"/tmp/docker.sock\"\n        }\n    ]\n},",
            "title": "Registrator Container Definition"
        },
        {
            "location": "/reference/consul/#consul-agent-container-definition",
            "text": "Warning  You must replace  <url>  with your Layer0 Consul Load Balancer's URL.   {\n    \"name\": \"consul-agent\",\n    \"image\": \"progrium/consul\",\n    \"essential\": true,\n    \"entrypoint\": [\"/bin/bash\", \"-c\"],\n    \"command\": [\"/bin/start -advertise $(wget http://169.254.169.254/latest/meta-data/local-ipv4 -q -O -) -retry-join $EXTERNAL_URL -recursor $UPSTREAM_DNS -retry-interval 30s\"],\n    \"memory\": 128,\n    \"portMappings\": [\n        {\n            \"hostPort\": 8500,\n            \"containerPort\": 8500\n        },\n        {\n            \"hostPort\": 53,\n            \"containerPort\": 53,\n            \"protocol\": \"udp\"\n        }\n    ],\n    \"environment\": [\n        {\n            \"name\": \"EXTERNAL_URL\",\n            \"value\": \"<url>\"\n    },\n    {\n            \"name\": \"UPSTREAM_DNS\",\n            \"value\": \"10.100.0.2\"\n        }\n    ]\n},",
            "title": "Consul Agent Container Definition"
        },
        {
            "location": "/reference/consul/#environment-variables",
            "text": "EXTERNAL_URL  - URL of the consul cluster  UPSTREAM_DNS  - The DNS server consul-agent queries for DNS entries that it cannot resolve internally (e.g. google.com)  The default value for  UPSTREAM_DNS  assumes you're using the default Layer0 configuration, making your internal DNS endpoint 10.100.0.2.  If you are a using a non standard configuration (e.g. installing Layer0 in an existing VPC with a CIDR other than  10.100.0.0/16 ) please modify this variable accordingly.",
            "title": "Environment Variables"
        },
        {
            "location": "/reference/consul/#docker-socket-volume-definition",
            "text": "\"volumes\": [\n    {\n        \"name\": \"dockersocket\",\n        \"host\": {\n                \"sourcePath\": \"/var/run/docker.sock\"\n        }\n    }\n],",
            "title": "Docker Socket Volume Definition"
        },
        {
            "location": "/reference/task_definition/",
            "text": "Task Definitions\n#\n\n\nThis guide gives some overview into the composition of a task definition.\nFor more comprehensive documentation, we recommend taking a look at the official AWS docs:\n\n\n\n\nCreating a Task Definition\n\n\nTask Definition Parameters\n\n\n\n\nSample\n#\n\n\nThe following snippet contains the task definition for the \nGuestbook\n application\n\n\n{\n    \"AWSEBDockerrunVersion\": 2,\n    \"containerDefinitions\": [\n        {\n            \"name\": \"guestbook\",\n            \"image\": \"quintilesims/guestbook\",\n            \"essential\": true,\n            \"memory\": 128,\n            \"portMappings\": [\n                {\n                    \"hostPort\": 80,\n                    \"containerPort\": 80\n                }\n            ],\n        }\n    ]\n}\n\n\n\n\n\n\n\nName\n The name of the container\n\n\n\n\n\n\nWarning\n\n\nIf you wish to update your task definition, the container names \nmust\n remain the same.\nIf any container names are changed or removed in an updated task definition,\nECS will not know how the existing container(s) should be mapped over and you will not be able to deploy the updated task definition.\nIf you encounter a scenario where you must change or remove a container's name in a task definition, we recommend re-creating the Layer0 Deploy and Service.\n\n\n\n\n\n\nImage\n The Docker image used to build the container. The image format is \nurl/image:tag\n\n\nThe \nurl\n specifies which Docker Repo to pull the image from\n    If a non-Docker-Hub \nurl\n is not specified, \nDocker Hub\n is used (as is the case here)\n\n\nThe \nimage\n specifies the name of the image to grab (in this case, the \nguestbook\n image from the \nquintilesims\n Docker Hub group)\n\n\nThe \ntag\n specifies which version of image to grab\nIf \ntag\n is not specified, \n:latest\n is used\n\n\n\n\n\n\nEssential\n If set to \ntrue\n, all other containers in the task definition will be stopped if that container fails or stops for any reason.\nOtherwise, the container's failure will not affect the rest of the containers in the task definition.\n\n\nMemory\n The number of MiB of memory to reserve for the container.\nIf your container attempts to exceed the memory allocated here, the container is killed\n\n\nPortMappings\n A list of hostPort, containerPort mappings for the container\n\n\nHostPort\n The port number on the host instance reserved for your container.\nIf your Layer0 Service is behind a Layer0 Load Balancer, this should map to an \ninstancePort\n on the Layer0 Load Balancer.\n\n\nContainerPort\n The port number the container should receive traffic on.\nAny traffic received from the instance's \nhostPort\n will be forwarded to the container on this port",
            "title": "Task Definitions"
        },
        {
            "location": "/reference/task_definition/#task-definitions",
            "text": "This guide gives some overview into the composition of a task definition.\nFor more comprehensive documentation, we recommend taking a look at the official AWS docs:   Creating a Task Definition  Task Definition Parameters",
            "title": "Task Definitions"
        },
        {
            "location": "/reference/task_definition/#sample",
            "text": "The following snippet contains the task definition for the  Guestbook  application  {\n    \"AWSEBDockerrunVersion\": 2,\n    \"containerDefinitions\": [\n        {\n            \"name\": \"guestbook\",\n            \"image\": \"quintilesims/guestbook\",\n            \"essential\": true,\n            \"memory\": 128,\n            \"portMappings\": [\n                {\n                    \"hostPort\": 80,\n                    \"containerPort\": 80\n                }\n            ],\n        }\n    ]\n}   Name  The name of the container    Warning  If you wish to update your task definition, the container names  must  remain the same.\nIf any container names are changed or removed in an updated task definition,\nECS will not know how the existing container(s) should be mapped over and you will not be able to deploy the updated task definition.\nIf you encounter a scenario where you must change or remove a container's name in a task definition, we recommend re-creating the Layer0 Deploy and Service.    Image  The Docker image used to build the container. The image format is  url/image:tag  The  url  specifies which Docker Repo to pull the image from\n    If a non-Docker-Hub  url  is not specified,  Docker Hub  is used (as is the case here)  The  image  specifies the name of the image to grab (in this case, the  guestbook  image from the  quintilesims  Docker Hub group)  The  tag  specifies which version of image to grab\nIf  tag  is not specified,  :latest  is used    Essential  If set to  true , all other containers in the task definition will be stopped if that container fails or stops for any reason.\nOtherwise, the container's failure will not affect the rest of the containers in the task definition.  Memory  The number of MiB of memory to reserve for the container.\nIf your container attempts to exceed the memory allocated here, the container is killed  PortMappings  A list of hostPort, containerPort mappings for the container  HostPort  The port number on the host instance reserved for your container.\nIf your Layer0 Service is behind a Layer0 Load Balancer, this should map to an  instancePort  on the Layer0 Load Balancer.  ContainerPort  The port number the container should receive traffic on.\nAny traffic received from the instance's  hostPort  will be forwarded to the container on this port",
            "title": "Sample"
        },
        {
            "location": "/reference/architecture/",
            "text": "Layer0 Architecture\n#\n\n\nLayer0 is built on top of the following primary technologies:\n\n\n\n\nApplication Container: \nDocker\n\n\nCloud Provider: \nAmazon Web Services\n\n\nContainer Management: \nAmazon EC2 Container Service (ECS)\n\n\nLoad Balancing: \nAmazon Elastic Load Balancing\n\n\nInfrastructure Configuration: Hashicorp \nTerraform\n\n\nIdentity Management: \nAuth0",
            "title": "Architecture"
        },
        {
            "location": "/reference/architecture/#layer0-architecture",
            "text": "Layer0 is built on top of the following primary technologies:   Application Container:  Docker  Cloud Provider:  Amazon Web Services  Container Management:  Amazon EC2 Container Service (ECS)  Load Balancing:  Amazon Elastic Load Balancing  Infrastructure Configuration: Hashicorp  Terraform  Identity Management:  Auth0",
            "title": "Layer0 Architecture"
        },
        {
            "location": "/reference/ecr/",
            "text": "EC2 Container Registry\n#\n\n\nECR is an Amazon implementation of a docker registry.  It acts as a private registry in your AWS account, which can be accessed from any docker client, and Layer0.  Consider using ECR if you have stability issues with hosted docker registries, and do not wish to share your images publicly on \ndockerhub\n.\n\n\nSetup\n#\n\n\nWhen interacting with ECR, you will first need to create a repository and a login to interact from your development machine.\n\n\nRepository\n#\n\n\nEach repository needs to be created by an AWS api call.\n\n\n  > aws ecr create-repository --repository-name myteam/myproject\n\n\n\n\n\nLogin\n#\n\n\nTo authenticate with the ECR service, Amazon provides the \nget-login\n command, which generates an authentication token, and returns a docker command to set it up\n\n\n  > aws ecr get-login\n  # this command will return the following: (password is typically hundreds of characters)\n  docker login -u AWS -p password -e none https://aws_account_id.dkr.ecr.us-east-1.amazonaws.com\n\n\n\n\n\nExecute the provided docker command to store the login credentials\n\n\nAfterward creating the repository and local login credentials you may interact with images (and tags) under this path from a local docker client.\n\n\n  docker pull \n${\necr\n-\nurl\n}\n/myteam/myproject\n  docker push \n${\necr\n-\nurl\n}\n/myteam/myproject:custom-tag-1\n\n\n\n\n\nDeploy Example\n#\n\n\nHere we'll walk through using ECR when deploying to Layer0,  Using a very basic wait container.\n\n\nMake docker image\n#\n\n\nYour docker image can be built locally or pulled from dockerhub.  For this example, we made a service that waits and then exits (useful for triggering regular restarts).\n\n\nFROM busybox\n\nENV SLEEP_TIME=60\n\nCMD sleep $SLEEP_TIME\n\n\n\n\n\nThen build the file, with the tag \nxfra/wait\n\n\n > docker build -f Dockerfile.wait -t xfra/wait .\n\n\n\n\n\nUpload to ECR\n#\n\n\nAfter preparing a login and registry, tag the image with the remote url, and use \ndocker push\n\n\n  docker tag xfra/wait 111222333444.dkr.ecr.us-east-1.amazonaws.com/xfra-wait\n  docker push 111222333444.dkr.ecr.us-east-1.amazonaws.com/xfra-wait\n\n\n\n\n\n\n\nNote: your account id in this url will be different.\n\n\n\n\nCreate a deploy\n#\n\n\nTo run this image in Layer0, we create a dockerrun file, describing the instance and any additional variables\n\n\n{\n  \"containerDefinitions\": [\n    {\n      \"name\": \"timeout\",\n      \"image\": \"111222333444.dkr.ecr.us-east-1.amazonaws.com/xfra-wait:latest\",\n      \"essential\": true,\n      \"memory\": 10,\n      \"environment\": [\n        { \"name\": \"SLEEP_TIME\", \"value\": \"43200\" }\n      ]\n    }\n  ]\n}\n\n\n\n\n\nAnd create that in Layer0\n\n\n  l0 deploy create timeout.dockerrun.aws.json timeout\n\n\n\n\n\nDeploy\n#\n\n\nFinally, run that deploy as a service or a task. (the service will restart every 12 hours)\n\n\n  l0 service create demo timeoutsvc timeout:latest\n\n\n\n\n\nReferences\n#\n\n\n\n\nECR User Guide\n\n\ncreate-repository\n\n\nget-login",
            "title": "ECR"
        },
        {
            "location": "/reference/ecr/#ec2-container-registry",
            "text": "ECR is an Amazon implementation of a docker registry.  It acts as a private registry in your AWS account, which can be accessed from any docker client, and Layer0.  Consider using ECR if you have stability issues with hosted docker registries, and do not wish to share your images publicly on  dockerhub .",
            "title": "EC2 Container Registry"
        },
        {
            "location": "/reference/ecr/#setup",
            "text": "When interacting with ECR, you will first need to create a repository and a login to interact from your development machine.",
            "title": "Setup"
        },
        {
            "location": "/reference/ecr/#repository",
            "text": "Each repository needs to be created by an AWS api call.    > aws ecr create-repository --repository-name myteam/myproject",
            "title": "Repository"
        },
        {
            "location": "/reference/ecr/#login",
            "text": "To authenticate with the ECR service, Amazon provides the  get-login  command, which generates an authentication token, and returns a docker command to set it up    > aws ecr get-login\n  # this command will return the following: (password is typically hundreds of characters)\n  docker login -u AWS -p password -e none https://aws_account_id.dkr.ecr.us-east-1.amazonaws.com  Execute the provided docker command to store the login credentials  Afterward creating the repository and local login credentials you may interact with images (and tags) under this path from a local docker client.    docker pull  ${ ecr - url } /myteam/myproject\n  docker push  ${ ecr - url } /myteam/myproject:custom-tag-1",
            "title": "Login"
        },
        {
            "location": "/reference/ecr/#deploy-example",
            "text": "Here we'll walk through using ECR when deploying to Layer0,  Using a very basic wait container.",
            "title": "Deploy Example"
        },
        {
            "location": "/reference/ecr/#make-docker-image",
            "text": "Your docker image can be built locally or pulled from dockerhub.  For this example, we made a service that waits and then exits (useful for triggering regular restarts).  FROM busybox\n\nENV SLEEP_TIME=60\n\nCMD sleep $SLEEP_TIME  Then build the file, with the tag  xfra/wait   > docker build -f Dockerfile.wait -t xfra/wait .",
            "title": "Make docker image"
        },
        {
            "location": "/reference/ecr/#upload-to-ecr",
            "text": "After preparing a login and registry, tag the image with the remote url, and use  docker push    docker tag xfra/wait 111222333444.dkr.ecr.us-east-1.amazonaws.com/xfra-wait\n  docker push 111222333444.dkr.ecr.us-east-1.amazonaws.com/xfra-wait   Note: your account id in this url will be different.",
            "title": "Upload to ECR"
        },
        {
            "location": "/reference/ecr/#create-a-deploy",
            "text": "To run this image in Layer0, we create a dockerrun file, describing the instance and any additional variables  {\n  \"containerDefinitions\": [\n    {\n      \"name\": \"timeout\",\n      \"image\": \"111222333444.dkr.ecr.us-east-1.amazonaws.com/xfra-wait:latest\",\n      \"essential\": true,\n      \"memory\": 10,\n      \"environment\": [\n        { \"name\": \"SLEEP_TIME\", \"value\": \"43200\" }\n      ]\n    }\n  ]\n}  And create that in Layer0    l0 deploy create timeout.dockerrun.aws.json timeout",
            "title": "Create a deploy"
        },
        {
            "location": "/reference/ecr/#deploy",
            "text": "Finally, run that deploy as a service or a task. (the service will restart every 12 hours)    l0 service create demo timeoutsvc timeout:latest",
            "title": "Deploy"
        },
        {
            "location": "/reference/ecr/#references",
            "text": "ECR User Guide  create-repository  get-login",
            "title": "References"
        },
        {
            "location": "/troubleshooting/commonissues/",
            "text": "Common issues and their solutions\n#\n\n\n\"Connection refused\" error when executing Layer0 commands\n#\n\n\nWhen executing commands using the Layer0 CLI, you may see the following error message: \n\n\nGet http://localhost:9090/command/: dial tcp 127.0.0.1:9090: connection refused\n\n\nWhere \ncommand\n is the Layer0 command you are trying to execute.\n\n\nThis error indicates that your Layer0 environment variables have not been set for the current session. See the \n\"Connect to a Layer0 Instance\" section\n of the Layer0 installation guide for instructions for setting up your environment variables.\n\n\n\n\n\"Invalid Dockerrun.aws.json\" error when creating a deploy\n#\n\n\nByte Order Marks (BOM) in Dockerrun file\n#\n\n\nIf your Dockerrun.aws.json file contains a Byte Order Marker, you may receive an \"Invalid Dockerrun.aws.json\" error when creating a deploy. If you create or edit the Dockerrun file using Visual Studio, and you have not modified the file encoding settings in Visual Studio, you are likely to encounter this error.\n\n\nTo remove the BOM:\n\n\n\n\n\n\nAt the command line, type the following to remove the BOM:\n\n\n\n\n(Linux/OS X) \n\n\n\n\ntail -c +4 DockerrunFile > DockerrunFileNew\n\n\nReplace \nDockerrunFile\n with the path to your Dockerrun file, and \nDockerrunFileNew\n with a new name for the Dockerrun file without the BOM.\n\n\n\n\n\n\nAlternatively, you can use the \ndos2unix file converter\n to remove the BOM from your Dockerrun files. Dos2unix is available for Windows, Linux and Mac OS.\n\n\nTo remove the BOM using dos2unix:\n\n\n\n\nAt the command line, type the following:\n\n\n\n\ndos2unix --remove-bom -n DockerrunFile DockerrunFileNew\n\n\n\n\n\nReplace DockerrunFile with the path to your Dockerrun file, and DockerrunFileNew with a new name for the Dockerrun file without the BOM.\n\n\n\n\n\"AWS Error: the key pair '\n' does not exist (code 'ValidationError')\" with l0-setup\n#\n\n\nThis occurs when you pass an invalid EC2 keypair to l0-setup. To fix this, follow the instructions for \ncreating an EC2 Key Pair\n.\n\n\n\n\nAfter you've created a new EC2 Key Pair, use \nl0-setup init\n to reconfigure your instance:\n\n\n\n\nl0-setup init --aws-ssh-key-pair keypair",
            "title": "Common Issues"
        },
        {
            "location": "/troubleshooting/commonissues/#common-issues-and-their-solutions",
            "text": "",
            "title": "Common issues and their solutions"
        },
        {
            "location": "/troubleshooting/commonissues/#connection-refused-error-when-executing-layer0-commands",
            "text": "When executing commands using the Layer0 CLI, you may see the following error message:   Get http://localhost:9090/command/: dial tcp 127.0.0.1:9090: connection refused  Where  command  is the Layer0 command you are trying to execute.  This error indicates that your Layer0 environment variables have not been set for the current session. See the  \"Connect to a Layer0 Instance\" section  of the Layer0 installation guide for instructions for setting up your environment variables.",
            "title": "\"Connection refused\" error when executing Layer0 commands"
        },
        {
            "location": "/troubleshooting/commonissues/#invalid-dockerrunawsjson-error-when-creating-a-deploy",
            "text": "",
            "title": "\"Invalid Dockerrun.aws.json\" error when creating a deploy"
        },
        {
            "location": "/troubleshooting/commonissues/#byte-order-marks-bom-in-dockerrun-file",
            "text": "If your Dockerrun.aws.json file contains a Byte Order Marker, you may receive an \"Invalid Dockerrun.aws.json\" error when creating a deploy. If you create or edit the Dockerrun file using Visual Studio, and you have not modified the file encoding settings in Visual Studio, you are likely to encounter this error.  To remove the BOM:    At the command line, type the following to remove the BOM:   (Linux/OS X)    tail -c +4 DockerrunFile > DockerrunFileNew  Replace  DockerrunFile  with the path to your Dockerrun file, and  DockerrunFileNew  with a new name for the Dockerrun file without the BOM.    Alternatively, you can use the  dos2unix file converter  to remove the BOM from your Dockerrun files. Dos2unix is available for Windows, Linux and Mac OS.  To remove the BOM using dos2unix:   At the command line, type the following:   dos2unix --remove-bom -n DockerrunFile DockerrunFileNew  Replace DockerrunFile with the path to your Dockerrun file, and DockerrunFileNew with a new name for the Dockerrun file without the BOM.",
            "title": "Byte Order Marks (BOM) in Dockerrun file"
        },
        {
            "location": "/troubleshooting/commonissues/#aws-error-the-key-pair-does-not-exist-code-validationerror-with-l0-setup",
            "text": "This occurs when you pass an invalid EC2 keypair to l0-setup. To fix this, follow the instructions for  creating an EC2 Key Pair .   After you've created a new EC2 Key Pair, use  l0-setup init  to reconfigure your instance:   l0-setup init --aws-ssh-key-pair keypair",
            "title": "\"AWS Error: the key pair '' does not exist (code 'ValidationError')\" with l0-setup"
        },
        {
            "location": "/troubleshooting/ssh/",
            "text": "Secure Shell (SSH)\n#\n\n\nYou can use Secure Shell (SSH) to access your Layer0 environment(s).\n\n\nBy default, Layer0 Setup asks for an EC2 key pair when creating a new Layer0. This key pair is associated with all machines that host your Layer0 Services. This means you can use SSH to log into the underlying Docker host to perform tasks such as troubleshooting failing containers or viewing logs. For information about creating an EC2 key pair, see \nInstall and Configure Layer0\n.\n\n\n\n\nWarning\n\n\nThis section is recommended for development debugging only.\nIt is \nnot\n recommended for production environments.\n\n\n\n\nTo SSH into a Service\n#\n\n\n\n\nIn a console window, add port 2222:22/tcp to your Service's load balancer:\n\n\n\n\nl0 loadbalancer addport <name> 2222:22/tcp\n\n\n\n\n\n\n  \nSSH into your Service by supplying the load balancer url and key pair file name.\n\n\n\n\n\nssh -i <key pair path and file name> ec2-user@<load balancer url> -p 2222\n\n\n\n\n\n\n  \nIf required, Use Docker to access a specific container with Bash.\n\n\n\n\n\ndocker exec -it <container id> /bin/bash\n\n\n\n\n\nRemarks\n#\n\n\nYou can get the load balancer url from the Load Balancers section of your Layer0 AWS console.\n\n\nUse the \nl0 loadbalancer dropport\n subcommand to remove a port configuration from an existing Layer0 load balancer.\n\n\nYou \ncannot\n change the key pair after a Layer0 has been created. If you lose your key pair or need to generate a new one, you will need to create a new Layer0.\n\n\nIf your Service is behind a private load balancer, or none at all, you can either re-create your Service behind a public load balancer, use an existing public load balancer as a \"jump\" point, or create a new Layer0 Service behind a public load balancer to serve as a \"jump\" point.",
            "title": "Secure Shell (SSH)"
        },
        {
            "location": "/troubleshooting/ssh/#secure-shell-ssh",
            "text": "You can use Secure Shell (SSH) to access your Layer0 environment(s).  By default, Layer0 Setup asks for an EC2 key pair when creating a new Layer0. This key pair is associated with all machines that host your Layer0 Services. This means you can use SSH to log into the underlying Docker host to perform tasks such as troubleshooting failing containers or viewing logs. For information about creating an EC2 key pair, see  Install and Configure Layer0 .   Warning  This section is recommended for development debugging only.\nIt is  not  recommended for production environments.",
            "title": "Secure Shell (SSH)"
        },
        {
            "location": "/troubleshooting/ssh/#to-ssh-into-a-service",
            "text": "In a console window, add port 2222:22/tcp to your Service's load balancer:   l0 loadbalancer addport <name> 2222:22/tcp  \n   SSH into your Service by supplying the load balancer url and key pair file name.   ssh -i <key pair path and file name> ec2-user@<load balancer url> -p 2222  \n   If required, Use Docker to access a specific container with Bash.   docker exec -it <container id> /bin/bash",
            "title": "To SSH into a Service"
        },
        {
            "location": "/troubleshooting/ssh/#remarks",
            "text": "You can get the load balancer url from the Load Balancers section of your Layer0 AWS console.  Use the  l0 loadbalancer dropport  subcommand to remove a port configuration from an existing Layer0 load balancer.  You  cannot  change the key pair after a Layer0 has been created. If you lose your key pair or need to generate a new one, you will need to create a new Layer0.  If your Service is behind a private load balancer, or none at all, you can either re-create your Service behind a public load balancer, use an existing public load balancer as a \"jump\" point, or create a new Layer0 Service behind a public load balancer to serve as a \"jump\" point.",
            "title": "Remarks"
        }
    ]
}