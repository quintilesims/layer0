{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Build, Manage, and Deploy Your Application # Meet Layer0 # Layer0 is a framework that helps you deploy web applications to the cloud with minimal fuss. Using a simple command line interface (CLI), you can manage the entire life cycle of your application without having to focus on infrastructure. Ready to learn more about Layer0? See our introduction page to learn about some important concepts. When you're ready to get started, take a look at the installation page for information about setting up Layer0. Download # Download v0.10.9 macOS Linux Windows Contact Us # If you have questions about Layer0, email the development team at carbon@us.imshealth.com .","title":"Home"},{"location":"#build-manage-and-deploy-your-application","text":"","title":"Build, Manage, and Deploy Your Application"},{"location":"#meet-layer0","text":"Layer0 is a framework that helps you deploy web applications to the cloud with minimal fuss. Using a simple command line interface (CLI), you can manage the entire life cycle of your application without having to focus on infrastructure. Ready to learn more about Layer0? See our introduction page to learn about some important concepts. When you're ready to get started, take a look at the installation page for information about setting up Layer0.","title":"Meet Layer0"},{"location":"#download","text":"Download v0.10.9 macOS Linux Windows","title":"Download"},{"location":"#contact-us","text":"If you have questions about Layer0, email the development team at carbon@us.imshealth.com .","title":"Contact Us"},{"location":"intro/","text":"Layer0 Introduction # In recent years, the process of deploying applications has seen incredible innovation. However, this innovation has taken a somewhat simple task and made it into something quite complicated . Cloud providers, load balancing, virtual servers, IP subnets, and a continuing list of technological considerations are not only required to be understood, but their creation and management must be automated for a modern application to be successful at scale. The burden of understanding a complicated and ever-growing infrastructure is a large aspect of what Layer0 is trying to fix. We've already done the leg work for huge swathes of your backend infrastructure, and we've made it easy to tear down and start over again, too. Meanwhile, you can develop locally using Docker and be assured that your application will properly translate to the cloud when you're ready to deploy. Layer0 requires a solid understanding of Docker to get the most out of it. We highly recommend starting with Docker's Understanding the Architecture to learn more about using Docker locally and in the cloud. We also recommend the Twelve-Factor App primer, which is a critical resource for understanding how to build a microservice. Layer0 Concepts # The following concepts are core Layer0 abstractions for the technologies and features we use behind the scenes . These terms will be used throughout our guides, so having a general understanding of them is helpful. Certificates # SSL certificates obtained from a valid Certificate Authority (CA) . You can use these certificates to secure your HTTPS services by applying them to your Layer0 load balancers. Deploys # ECS Task Definitions . These configuration files detail how to deploy your application. We have several sample applications available that show what these files look like --- they're called Dockerrun.aws.json within each sample app. Tasks # Manual one-off commands that don't necessarily make sense to keep running, or to restart when they finish. These run using Amazon's RunTask action (more info here ), and are \"ideally suited for processes such as batch jobs that perform work and then stop.\" Load Balancers # Powerful tools that give you the basic building blocks for high-availability, scaling, and HTTPS. We currently use Amazon's Elastic Load Balancing , and it pays to understand the basics of this service when working with Layer0. Services # Your running Layer0 applications. We also use the term service for tools such as Consul, for which we provide a pre-built sample implementation using Layer0. Environments # Logical groupings of services. Typically, you would make a single environment for each tier of your application, such as dev , staging , and prod .","title":"Introduction"},{"location":"intro/#layer0-introduction","text":"In recent years, the process of deploying applications has seen incredible innovation. However, this innovation has taken a somewhat simple task and made it into something quite complicated . Cloud providers, load balancing, virtual servers, IP subnets, and a continuing list of technological considerations are not only required to be understood, but their creation and management must be automated for a modern application to be successful at scale. The burden of understanding a complicated and ever-growing infrastructure is a large aspect of what Layer0 is trying to fix. We've already done the leg work for huge swathes of your backend infrastructure, and we've made it easy to tear down and start over again, too. Meanwhile, you can develop locally using Docker and be assured that your application will properly translate to the cloud when you're ready to deploy. Layer0 requires a solid understanding of Docker to get the most out of it. We highly recommend starting with Docker's Understanding the Architecture to learn more about using Docker locally and in the cloud. We also recommend the Twelve-Factor App primer, which is a critical resource for understanding how to build a microservice.","title":"Layer0 Introduction"},{"location":"intro/#layer0-concepts","text":"The following concepts are core Layer0 abstractions for the technologies and features we use behind the scenes . These terms will be used throughout our guides, so having a general understanding of them is helpful.","title":"Layer0 Concepts"},{"location":"intro/#certificates","text":"SSL certificates obtained from a valid Certificate Authority (CA) . You can use these certificates to secure your HTTPS services by applying them to your Layer0 load balancers.","title":"Certificates"},{"location":"intro/#deploys","text":"ECS Task Definitions . These configuration files detail how to deploy your application. We have several sample applications available that show what these files look like --- they're called Dockerrun.aws.json within each sample app.","title":"Deploys"},{"location":"intro/#tasks","text":"Manual one-off commands that don't necessarily make sense to keep running, or to restart when they finish. These run using Amazon's RunTask action (more info here ), and are \"ideally suited for processes such as batch jobs that perform work and then stop.\"","title":"Tasks"},{"location":"intro/#load-balancers","text":"Powerful tools that give you the basic building blocks for high-availability, scaling, and HTTPS. We currently use Amazon's Elastic Load Balancing , and it pays to understand the basics of this service when working with Layer0.","title":"Load Balancers"},{"location":"intro/#services","text":"Your running Layer0 applications. We also use the term service for tools such as Consul, for which we provide a pre-built sample implementation using Layer0.","title":"Services"},{"location":"intro/#environments","text":"Logical groupings of services. Typically, you would make a single environment for each tier of your application, such as dev , staging , and prod .","title":"Environments"},{"location":"releases/","text":"v0.10.x Breaking Change The v0.10.x family contains a breaking change for the task workflow. For best results, please consider versions 0.10.4 and newer to be incompatible with versions 0.10.3 and older. See this issue for more. Version macOS Linux Windows v0.10.9 macOS Linux Windows v0.10.8 macOS Linux Windows v0.10.7 macOS Linux Windows v0.10.6 macOS Linux Windows v0.10.5 macOS Linux Windows v0.10.4 macOS Linux Windows v0.10.3 macOS Linux Windows v0.10.2 macOS Linux Windows v0.10.1 macOS Linux Windows v0.10.0 macOS Linux Windows v0.9.0 macOS Linux Windows v0.8.4 macOS Linux Windows","title":"Releases"},{"location":"guides/one_off_task/","text":"Deployment guide: Guestbook one-off task # In this example, you will learn how to use layer0 to run a one-off task. A task is used to run a single instance of your Task Definition and is typically a short running job that will be stopped once finished. Before you start # In order to complete the procedures in this section, you must install and configure Layer0 v0.8.4 or later. If you have not already configured Layer0, see the installation guide . If you are running an older version of Layer0, see the upgrade instructions . Part 1: Prepare the task definition # Download the Guestbook One-off Task Definition and save it to your computer as Dockerrun.aws.json . Part 2: Create a deploy # Next, you will create a new deploy for the task using the deploy create command. At the command prompt, run the following command: l0 deploy create Dockerrun.aws.json one-off-task-dpl You will see the following output: DEPLOY ID DEPLOY NAME VERSION one-off-task-dpl.1 one-off-task-dpl 1 Part 3: Create the task # At this point, you can use the task create command to run a copy of the task. To run the task, use the following command: l0 task create demo-env echo-tsk one-off-task-dpl:latest --wait You will see the following output: TASK ID TASK NAME ENVIRONMENT DEPLOY SCALE one-off851c9 echo-tsk demo-env one-off-task-dpl:1 0/1 (1) The SCALE column shows the running, desired and pending counts. A value of 0/1 (1) indicates that running = 0, desired = 1 and (1) for 1 pending task that is about to transition to running state. After your task has finished running, note that the desired count will remain 1 and pending value will no longer be shown, so the value will be 0/1 for a finished task. Part 4: Check the status of the task # To view the logs for this task, and evaluate its progress, you can use the task logs command: l0 task logs echo-tsk You will see the following output: alpine ------ Task finished! You can also use the following command for more information in the task. l0 -o json task get echo-tsk Outputs: [ { \"copies\": [ { \"details\": [], \"reason\": \"Waiting for cluster capacity to run\", \"task_copy_id\": \"\" } ], \"deploy_id\": \"one-off-task-dpl.2\", \"deploy_name\": \"one-off-task-dpl\", \"deploy_version\": \"2\", \"desired_count\": 1, \"environment_id\": \"demoenv669e4\", \"environment_name\": \"demo-env\", \"pending_count\": 1, \"running_count\": 0, \"task_id\": \"echotsk1facd\", \"task_name\": \"echo-tsk\" } ] After the task has finished, running l0 -o json task get echo-tsk will show a pending_count of 0. Outputs: ... \"copies\": [ { \"details\": [ { \"container_name\": \"alpine\", \"exit_code\": 0, \"last_status\": \"STOPPED\", \"reason\": \"\" } ], \"reason\": \"Essential container in task exited\", \"task_copy_id\": \"arn:aws:ecs:us-west-2:856306994068:task/0e723c3e-9cd1-4914-8393-b59abd40eb89\" } ], ... \"pending_count\": 0, \"running_count\": 0, ...","title":"One-off Task"},{"location":"guides/one_off_task/#deployment-guide-guestbook-one-off-task","text":"In this example, you will learn how to use layer0 to run a one-off task. A task is used to run a single instance of your Task Definition and is typically a short running job that will be stopped once finished.","title":"Deployment guide: Guestbook one-off task"},{"location":"guides/one_off_task/#before-you-start","text":"In order to complete the procedures in this section, you must install and configure Layer0 v0.8.4 or later. If you have not already configured Layer0, see the installation guide . If you are running an older version of Layer0, see the upgrade instructions .","title":"Before you start"},{"location":"guides/one_off_task/#part-1-prepare-the-task-definition","text":"Download the Guestbook One-off Task Definition and save it to your computer as Dockerrun.aws.json .","title":"Part 1: Prepare the task definition"},{"location":"guides/one_off_task/#part-2-create-a-deploy","text":"Next, you will create a new deploy for the task using the deploy create command. At the command prompt, run the following command: l0 deploy create Dockerrun.aws.json one-off-task-dpl You will see the following output: DEPLOY ID DEPLOY NAME VERSION one-off-task-dpl.1 one-off-task-dpl 1","title":"Part 2: Create a deploy"},{"location":"guides/one_off_task/#part-3-create-the-task","text":"At this point, you can use the task create command to run a copy of the task. To run the task, use the following command: l0 task create demo-env echo-tsk one-off-task-dpl:latest --wait You will see the following output: TASK ID TASK NAME ENVIRONMENT DEPLOY SCALE one-off851c9 echo-tsk demo-env one-off-task-dpl:1 0/1 (1) The SCALE column shows the running, desired and pending counts. A value of 0/1 (1) indicates that running = 0, desired = 1 and (1) for 1 pending task that is about to transition to running state. After your task has finished running, note that the desired count will remain 1 and pending value will no longer be shown, so the value will be 0/1 for a finished task.","title":"Part 3: Create the task"},{"location":"guides/one_off_task/#part-4-check-the-status-of-the-task","text":"To view the logs for this task, and evaluate its progress, you can use the task logs command: l0 task logs echo-tsk You will see the following output: alpine ------ Task finished! You can also use the following command for more information in the task. l0 -o json task get echo-tsk Outputs: [ { \"copies\": [ { \"details\": [], \"reason\": \"Waiting for cluster capacity to run\", \"task_copy_id\": \"\" } ], \"deploy_id\": \"one-off-task-dpl.2\", \"deploy_name\": \"one-off-task-dpl\", \"deploy_version\": \"2\", \"desired_count\": 1, \"environment_id\": \"demoenv669e4\", \"environment_name\": \"demo-env\", \"pending_count\": 1, \"running_count\": 0, \"task_id\": \"echotsk1facd\", \"task_name\": \"echo-tsk\" } ] After the task has finished, running l0 -o json task get echo-tsk will show a pending_count of 0. Outputs: ... \"copies\": [ { \"details\": [ { \"container_name\": \"alpine\", \"exit_code\": 0, \"last_status\": \"STOPPED\", \"reason\": \"\" } ], \"reason\": \"Essential container in task exited\", \"task_copy_id\": \"arn:aws:ecs:us-west-2:856306994068:task/0e723c3e-9cd1-4914-8393-b59abd40eb89\" } ], ... \"pending_count\": 0, \"running_count\": 0, ...","title":"Part 4: Check the status of the task"},{"location":"guides/walkthrough/deployment-1/","text":"Deployment 1: A Simple Guestbook App # In this section you'll learn how different Layer0 commands work together to deploy applications to the cloud. The example application in this section is a guestbook -- a web application that acts as a simple message board. You can choose to complete this section using either the Layer0 CLI or Terraform . Deploy with Layer0 CLI # If you're following along, you'll want to be working in the walkthrough/deployment-1/ directory of your clone of the guides repo. Files used in this deployment: Filename Purpose Guestbook.Dockerrun.aws.json Template for running the Guestbook application Part 1: Create the Environment # The first step in deploying an application with Layer0 is to create an environment. An environment is a dedicated space in which one or more services can reside. Here, we'll create a new environment named demo-env . At the command prompt, execute the following: l0 environment create demo-env We should see output like the following: ENVIRONMENT ID ENVIRONMENT NAME OS CLUSTER COUNT INSTANCE SIZE LINKS demoenvd4e17 demo-env linux 0 m3.medium We can inspect our environments in a couple of different ways: l0 environment list will give us a brief summary of all environments: ENVIRONMENT ID ENVIRONMENT NAME OS api api linux demoenvd4e17 demo-env linux l0 environment get demo-env will show us more information about the demo-env environment we just created: ENVIRONMENT ID ENVIRONMENT NAME OS CLUSTER COUNT INSTANCE SIZE LINKS demoenvd4e17 demo-env linux 0 m3.medium l0 environment get \\* illustrates wildcard matching (you could also have used demo* in the above command), and it will return detailed information for each environment, not just one - it's like a detailed list : ENVIRONMENT ID ENVIRONMENT NAME OS CLUSTER COUNT INSTANCE SIZE LINKS api api linux 2 t2.small demoenvd4e17 demo-env linux 0 m3.medium Part 2: Create the Load Balancer # In order to expose a web application to the public internet, we need to create a load balancer. A load balancer listens for web traffic at a specific address and directs that traffic to a Layer0 service. A load balancer also has a notion of a health check - a way to assess whether or not the service is healthy and running properly. By default, Layer0 configures the health check of a load balancer based upon a simple TCP ping to port 80 every thirty seconds. Also by default, this ping will timeout after five seconds of no response from the service, and two consecutive successes or failures are required for the service to be considered healthy or unhealthy. Here, we'll create a new load balancer named guestbook-lb inside of our environment named demo-env . The load balancer will listen on port 80, and forward that traffic along to port 80 in the Docker container using the HTTP protocol. Since the port configuration is already aligned with the default health check, we don't need to specify any health check configuration when we create this load balancer. At the command prompt, execute the following: l0 loadbalancer create --port 80:80/http demo-env guestbook-lb We should see output like the following: LOADBALANCER ID LOADBALANCER NAME ENVIRONMENT SERVICE PORTS PUBLIC URL IDLE TIMEOUT guestboc8d07 guestbook-lb demo-env 80:80/HTTP true 60 The following is a summary of the arguments passed in the above command: loadbalancer create : creates a new load balancer --port 80:80/HTTP : instructs the load balancer to forward requests from port 80 on the load balancer to port 80 in the EC2 instance using the HTTP protocol demo-env : the name of the environment in which you are creating the load balancer guestbook-lb : a name for the load balancer itself You can inspect load balancers in the same way that you inspected environments in Part 1. Try running the following commands to get an idea of the information available to you: l0 loadbalancer list l0 loadbalancer get guestbook-lb l0 loadbalancer get gues* l0 loadbalancer get \\* Note Notice that the load balancer list and get outputs list an ENVIRONMENT field - if you ever have load balancers (or other Layer0 entities) with the same name but in different environments, you can target a specific load balancer by qualifying it with its environment name: `l0 loadbalancer get demo-env:guestbook-lb` Part 3: Deploy the ECS Task Definition # The deploy command is used to specify the ECS task definition that outlines a web application. A deploy, once created, can be applied to multiple services - even across different environments! Here, we'll create a new deploy called guestbook-dpl that refers to the Guestbook.Dockerrun.aws.json file found in the guides reposiory. At the command prompt, execute the following: l0 deploy create Guestbook.Dockerrun.aws.json guestbook-dpl We should see output like the following: DEPLOY ID DEPLOY NAME VERSION guestbook-dpl.1 guestbook-dpl 1 The following is a summary of the arguments passed in the above command: deploy create : creates a new deployment and allows you to specify an ECS task definition Guestbook.Dockerrun.aws.json : the file name of the ECS task definition (use the full path of the file if it is not in your current working directory) guestbook-dpl : a name for the deploy, which you will use later when you create the service Deploy Versioning The DEPLOY NAME and VERSION are combined to create a unique identifier for a deploy. If you create additional deploys named guestbook-dpl , they will be assigned different version numbers. You can always specify the latest version when targeting a deploy by using <deploy name>:latest -- for example, guestbook-dpl:latest . Deploys support the same methods of inspection as environments and load balancers: l0 deploy list l0 deploy get guestbook* l0 deploy get guestbook:1 l0 deploy get guestbook:latest l0 deploy get \\* Part 4: Create the Service # The final stage of the deployment process involves using the service command to create a new service and associate it with the environment, load balancer, and deploy that we created in the previous sections. The service will execute the Docker containers which have been described in the deploy. Here, we'll create a new service called guestbook-svc . At the command prompt, execute the following: l0 service create --loadbalancer demo-env:guestbook-lb demo-env guestbook-svc guestbook-dpl:latest We should see output like the following: SERVICE ID SERVICE NAME ENVIRONMENT LOADBALANCER DEPLOYMENTS SCALE guestbo9364b guestbook-svc demo-env guestbook-lb guestbook-dpl:1* 0/1 The following is a summary of the arguments passed in the above command: service create : creates a new service --loadbalancer demo-env:guestbook-lb : the fully-qualified name of the load balancer; in this case, the load balancer named guestbook-lb in the environment named demo-env . - (It is not strictly necessary to use the fully qualified name of the load balancer, unless another load balancer with exactly the same name exists in a different environment.) demo-env : the name of the environment you created in Part 1 guestbook-svc : a name for the service you are creating guestbook-dpl : the name of the deploy that you created in Part 3 Layer0 services can be queried using the same get and list commands that we've come to expect by now. Check the Status of the Service # After a service has been created, it may take several minutes for that service to completely finish deploying. A service's status may be checked by using the service get command. Let's take a peek at our guestbook-svc service. At the command prompt, execute the following: l0 service get demo-env:guestbook-svc If we're quick enough, we'll be able to see the first stage of the process (this is what was output after running the service create command up in Part 4). We should see an asterisk (*) next to the name of the guestbook-dpl:1 deploy, which indicates that the service is in a transitional state: SERVICE ID SERVICE NAME ENVIRONMENT LOADBALANCER DEPLOYMENTS SCALE guestbo9364b guestbook-svc demo-env guestbook-lb guestbook-dpl:1* 0/1 In the next phase of deployment, if we execute the service get command again, we will see (1) in the Scale column; this indicates that 1 copy of the service is transitioning to an active state: SERVICE ID SERVICE NAME ENVIRONMENT LOADBALANCER DEPLOYMENTS SCALE guestbo9364b guestbook-svc demo-env guestbook-lb guestbook-dpl:1* 0/1 (1) In the final phase of deployment, we will see 1/1 in the Scale column; this indicates that the service is running 1 copy: SERVICE ID SERVICE NAME ENVIRONMENT LOADBALANCER DEPLOYMENTS SCALE guestbo9364b guestbook-svc demo-env guestbook-lb guestbook-dpl:1 1/1 Get the Application's URL # Once the service has been completely deployed, we can obtain the URL for the application and launch it in a browser. At the command prompt, execute the following: l0 loadbalancer get demo-env:guestbook-lb We should see output like the following: LOADBALANCER ID LOADBALANCER NAME ENVIRONMENT SERVICE PORTS PUBLIC URL IDLE TIMEOUT guestboc8d07 guestbook-lb demo-env guestbook-svc 80:80/HTTP true <url> 60 Copy the value shown in the URL column and paste it into a web browser. The guestbook application will appear (once the service has completely finished deploying). Logs # Output from a Service's docker containers may be acquired by running the following command: l0 service logs <SERVICE> guestbook --------- 2018/11/05 19:11:46 Using memory backend 2018/11/05 19:11:46 Listening on :80 Cleanup CLI # If you're finished with the example and don't want to continue with this walkthrough, you can instruct Layer0 to delete the environment and terminate the application. l0 environment delete demo-env However, if you intend to continue through Deployment 2 , you will want to keep the resources you made in this section. Deploy with Terraform # Instead of using the Layer0 CLI directly, you can instead use our Terraform provider, and deploy using Terraform ( learn more ) . You can use Terraform with Layer0 and AWS to create \"fire-and-forget\" deployments for your applications. If you're following along, you'll want to be working in the walkthrough/deployment-1/ directory of your clone of the guides repo. We use these files to set up a Layer0 environment with Terraform: Filename Purpose main.tf Provisions resources; populates resources in template files outputs.tf Values that Terraform will yield during deployment terraform.tfstate Tracks status of deployment (created and managed by Terraform) terraform.tfvars Variables specific to the environment and application(s) variables.tf Values that Terraform will use during deployment *.tf : A Brief Aside # Let's take a moment to discuss the .tf files. The names of these files (and even the fact that they are separated out into multiple files at all) are completely arbitrary and exist soley for human-readability. Terraform understands all .tf files in a directory all together. In variables.tf , you'll see \"endpoint\" and \"token\" variables. In outputs.tf , you'll see that Terraform should spit out the url of the guestbook's load balancer once deployment has finished. In main.tf , you'll see the bulk of the deployment process. If you've followed along with the Layer0 CLI deployment above, it should be fairly easy to see how blocks in this file map to steps in the CLI process. When we began the CLI deployment, our first step was to create an environment: l0 environment create demo-env This command is recreated in main.tf like so: # walkthrough/deployment-1/main.tf resource \"layer0_environment\" \"demo-env\" { name = \"demo-env\" } We've bundled up the heart of the Guestbook deployment (load balancer, deploy, service, etc.) into a Terraform module . To use it, we declare a module block and pass in the source of the module as well as any configuration or variables that the module needs. # walkthrough/deployment-1/main.tf module \"guestbook\" { source = \"github.com/quintilesims/guides//guestbook/module\" environment_id = \" ${ layer0_environment . demo . id } \" } You can see that we pass in the ID of the environment we create. All variables declared in this block are passed to the module, so the next file we should look at is variables.tf inside of the module to get an idea of what the module is expecting. There are a lot of variables here, but only one of them doesn't have a default value. # guestbook/module/variables.tf variable \"environment_id\" { description = \"id of the layer0 environment in which to create resources\" } You'll notice that this is the variable that we're passing in. For this particular deployment of the Guestbook, all of the default options are fine. We could override any of them if we wanted to, just by specifying a new value for them back in deployment-1/main.tf . Now that we've seen the variables that the module will have, let's take a look at part of module/main.tf and see how some of them might be used: # guestbook/module/main.tf resource \"layer0_load_balancer\" \"guestbook-lb\" { name = \" ${ var . load_balancer_name } \" environment = \" ${ var . environment_id } \" port { host_port = 80 container_port = 80 protocol = \"http\" } } ... You can follow this link to learn more about Layer0 resources in Terraform. Part 1: Terraform Init # This deployment has provider dependencies so an init call must be made. (Terraform v0.11~ requries init) At the command prompt, execute the following command: terraform init We should see output like the following: Initializing modules... - module.guestbook Getting source \"github.com/quintilesims/guides//guestbook/module\" Initializing provider plugins... - Checking for available provider plugins on https://releases.hashicorp.com... - Downloading plugin for provider \"template\" (1.0.0)... The following providers do not have any version constraints in configuration, so the latest version was installed. To prevent automatic upgrades to new major versions that may contain breaking changes, it is recommended to add version = \"...\" constraints to the corresponding provider blocks in configuration, with the constraint strings suggested below. * provider.template: version = \"~> 1.0\" Terraform has been successfully initialized! You may now begin working with Terraform. Try running \"terraform plan\" to see any changes that are required for your infrastructure. All Terraform commands should now work. If you ever set or change modules or backend configuration for Terraform, rerun this command to reinitialize your working directory. If you forget, other commands will detect it and remind you to do so if necessary. Part 2: Terraform Plan # Before we actually create/update/delete any resources, it's a good idea to find out what Terraform intends to do. Run terraform plan . Terraform will prompt you for configuration values that it does not have: var.endpoint Enter a value: var.token Enter a value: You can find these values by running l0-setup endpoint <your layer0 prefix> . Note There are a few ways to configure Terraform so that you don't have to keep entering these values every time you run a Terraform command (editing the terraform.tfvars file, or exporting environment variables like TF_VAR_endpoint and TF_VAR_token , for example). See the Terraform Docs for more. The plan command should give us output like the following: Refreshing Terraform state in-memory prior to plan... The refreshed state will be used to calculate this plan, but will not be persisted to local or remote state storage. data.template_file.guestbook: Refreshing state... ------------------------------------------------------------------------ An execution plan has been generated and is shown below. Resource actions are indicated with the following symbols: + create Terraform will perform the following actions: + layer0_environment.demo id: <computed> ami: <computed> current_scale: <computed> instance_type: \"t2.small\" name: \"demo\" os: \"linux\" scale: \"0\" security_group_id: <computed> + module.guestbook.layer0_deploy.guestbook id: <computed> content: \"{\\n \\\"AWSEBDockerrunVersion\\\": 2,\\n \\\"containerDefinitions\\\": [\\n {\\n \\\"name\\\": \\\"guestbook\\\",\\n \\\"image\\\": \\\"quintilesims/guestbook\\\",\\n \\\"essential\\\": true,\\n \\\"memory\\\": 128,\\n \\\"environment\\\": [\\n {\\n \\\"name\\\": \\\"GUESTBOOK_BACKEND_TYPE\\\",\\n \\\"value\\\": \\\"memory\\\"\\n },\\n {\\n \\\"name\\\": \\\"GUESTBOOK_BACKEND_CONFIG\\\",\\n \\\"value\\\": \\\"\\\"\\n },\\n {\\n \\\"name\\\": \\\"AWS_ACCESS_KEY_ID\\\",\\n \\\"value\\\": \\\"\\\"\\n },\\n {\\n \\\"name\\\": \\\"AWS_SECRET_ACCESS_KEY\\\",\\n \\\"value\\\": \\\"\\\"\\n },\\n {\\n \\\"name\\\": \\\"AWS_REGION\\\",\\n \\\"value\\\": \\\"us-west-2\\\"\\n }\\n ],\\n \\\"portMappings\\\": [\\n {\\n \\\"hostPort\\\": 80,\\n \\\"containerPort\\\": 80\\n }\\n ]\\n }\\n ]\\n}\\n\" name: \"guestbook\" version: <computed> + module.guestbook.layer0_load_balancer.guestbook id: <computed> environment: \" ${ var . environment_id } \" health_check.#: <computed> idle_timeout: \"60\" name: \"guestbook\" port.#: \"1\" port.2027667003.certificate_arn: \"\" port.2027667003.container_port: \"80\" port.2027667003.host_port: \"80\" port.2027667003.protocol: \"http\" private: <computed> type: \"application\" url: <computed> + module.guestbook.layer0_service.guestbook id: <computed> deploy: \" ${ var . deploy_id == \\ \" \\\" ? layer0_deploy.guestbook.id : var.deploy_id } \" environment: \" ${ var . environment_id } \" load_balancer: \" ${ layer0_load_balancer . guestbook . id } \" name: \"guestbook\" scale: \"1\" stateful: \"false\" Plan: 4 to add, 0 to change, 0 to destroy. This shows you that Terraform intends to create a deploy, an environment, a load balancer, and a service, all through Layer0. If you've gone through this deployment using the Layer0 CLI , you may notice that these resources appear out of order - that's fine. Terraform presents these resources in alphabetical order, but underneath, it knows the correct order in which to create them. Once we're satisfied that Terraform will do what we want it to do, we can move on to actually making these things exist! Part 3: Terraform Apply # Run terraform apply to begin the process. We should see output like the following: layer0_environment.demo: Refreshing state... ... ... ... layer0_service.guestbook: Creation complete Apply complete! Resources: 7 added, 0 changed, 0 destroyed. The state of your infrastructure has been saved to the path below. This state is required to modify and destroy your infrastructure, so keep it safe. To inspect the complete state use the `terraform show` command. State path: terraform.tfstate Outputs: guestbook_url = <http endpoint for the sample application> Note It may take a few minutes for the guestbook service to launch and the load balancer to become available. During that time you may get HTTP 503 errors when making HTTP requests against the load balancer URL. What's Happening # Terraform provisions the AWS resources through Layer0, configures environment variables for the application, and deploys the application into a Layer0 environment. Terraform also writes the state of your deployment to the terraform.tfstate file (creating a new one if it's not already there). Cleanup Terraform # When you're finished with the example, you can instruct Terraform to destroy the Layer0 environment, and terminate the application. Execute the following command (in the same directory): terraform destroy It's also now safe to remove the .terraform/ directory and the *.tfstate* files.","title":"Walkthrough: Deployment 1"},{"location":"guides/walkthrough/deployment-1/#deployment-1-a-simple-guestbook-app","text":"In this section you'll learn how different Layer0 commands work together to deploy applications to the cloud. The example application in this section is a guestbook -- a web application that acts as a simple message board. You can choose to complete this section using either the Layer0 CLI or Terraform .","title":"Deployment 1: A Simple Guestbook App"},{"location":"guides/walkthrough/deployment-1/#deploy-with-layer0-cli","text":"If you're following along, you'll want to be working in the walkthrough/deployment-1/ directory of your clone of the guides repo. Files used in this deployment: Filename Purpose Guestbook.Dockerrun.aws.json Template for running the Guestbook application","title":"Deploy with Layer0 CLI"},{"location":"guides/walkthrough/deployment-1/#part-1-create-the-environment","text":"The first step in deploying an application with Layer0 is to create an environment. An environment is a dedicated space in which one or more services can reside. Here, we'll create a new environment named demo-env . At the command prompt, execute the following: l0 environment create demo-env We should see output like the following: ENVIRONMENT ID ENVIRONMENT NAME OS CLUSTER COUNT INSTANCE SIZE LINKS demoenvd4e17 demo-env linux 0 m3.medium We can inspect our environments in a couple of different ways: l0 environment list will give us a brief summary of all environments: ENVIRONMENT ID ENVIRONMENT NAME OS api api linux demoenvd4e17 demo-env linux l0 environment get demo-env will show us more information about the demo-env environment we just created: ENVIRONMENT ID ENVIRONMENT NAME OS CLUSTER COUNT INSTANCE SIZE LINKS demoenvd4e17 demo-env linux 0 m3.medium l0 environment get \\* illustrates wildcard matching (you could also have used demo* in the above command), and it will return detailed information for each environment, not just one - it's like a detailed list : ENVIRONMENT ID ENVIRONMENT NAME OS CLUSTER COUNT INSTANCE SIZE LINKS api api linux 2 t2.small demoenvd4e17 demo-env linux 0 m3.medium","title":"Part 1: Create the Environment"},{"location":"guides/walkthrough/deployment-1/#part-2-create-the-load-balancer","text":"In order to expose a web application to the public internet, we need to create a load balancer. A load balancer listens for web traffic at a specific address and directs that traffic to a Layer0 service. A load balancer also has a notion of a health check - a way to assess whether or not the service is healthy and running properly. By default, Layer0 configures the health check of a load balancer based upon a simple TCP ping to port 80 every thirty seconds. Also by default, this ping will timeout after five seconds of no response from the service, and two consecutive successes or failures are required for the service to be considered healthy or unhealthy. Here, we'll create a new load balancer named guestbook-lb inside of our environment named demo-env . The load balancer will listen on port 80, and forward that traffic along to port 80 in the Docker container using the HTTP protocol. Since the port configuration is already aligned with the default health check, we don't need to specify any health check configuration when we create this load balancer. At the command prompt, execute the following: l0 loadbalancer create --port 80:80/http demo-env guestbook-lb We should see output like the following: LOADBALANCER ID LOADBALANCER NAME ENVIRONMENT SERVICE PORTS PUBLIC URL IDLE TIMEOUT guestboc8d07 guestbook-lb demo-env 80:80/HTTP true 60 The following is a summary of the arguments passed in the above command: loadbalancer create : creates a new load balancer --port 80:80/HTTP : instructs the load balancer to forward requests from port 80 on the load balancer to port 80 in the EC2 instance using the HTTP protocol demo-env : the name of the environment in which you are creating the load balancer guestbook-lb : a name for the load balancer itself You can inspect load balancers in the same way that you inspected environments in Part 1. Try running the following commands to get an idea of the information available to you: l0 loadbalancer list l0 loadbalancer get guestbook-lb l0 loadbalancer get gues* l0 loadbalancer get \\* Note Notice that the load balancer list and get outputs list an ENVIRONMENT field - if you ever have load balancers (or other Layer0 entities) with the same name but in different environments, you can target a specific load balancer by qualifying it with its environment name: `l0 loadbalancer get demo-env:guestbook-lb`","title":"Part 2: Create the Load Balancer"},{"location":"guides/walkthrough/deployment-1/#part-3-deploy-the-ecs-task-definition","text":"The deploy command is used to specify the ECS task definition that outlines a web application. A deploy, once created, can be applied to multiple services - even across different environments! Here, we'll create a new deploy called guestbook-dpl that refers to the Guestbook.Dockerrun.aws.json file found in the guides reposiory. At the command prompt, execute the following: l0 deploy create Guestbook.Dockerrun.aws.json guestbook-dpl We should see output like the following: DEPLOY ID DEPLOY NAME VERSION guestbook-dpl.1 guestbook-dpl 1 The following is a summary of the arguments passed in the above command: deploy create : creates a new deployment and allows you to specify an ECS task definition Guestbook.Dockerrun.aws.json : the file name of the ECS task definition (use the full path of the file if it is not in your current working directory) guestbook-dpl : a name for the deploy, which you will use later when you create the service Deploy Versioning The DEPLOY NAME and VERSION are combined to create a unique identifier for a deploy. If you create additional deploys named guestbook-dpl , they will be assigned different version numbers. You can always specify the latest version when targeting a deploy by using <deploy name>:latest -- for example, guestbook-dpl:latest . Deploys support the same methods of inspection as environments and load balancers: l0 deploy list l0 deploy get guestbook* l0 deploy get guestbook:1 l0 deploy get guestbook:latest l0 deploy get \\*","title":"Part 3: Deploy the ECS Task Definition"},{"location":"guides/walkthrough/deployment-1/#part-4-create-the-service","text":"The final stage of the deployment process involves using the service command to create a new service and associate it with the environment, load balancer, and deploy that we created in the previous sections. The service will execute the Docker containers which have been described in the deploy. Here, we'll create a new service called guestbook-svc . At the command prompt, execute the following: l0 service create --loadbalancer demo-env:guestbook-lb demo-env guestbook-svc guestbook-dpl:latest We should see output like the following: SERVICE ID SERVICE NAME ENVIRONMENT LOADBALANCER DEPLOYMENTS SCALE guestbo9364b guestbook-svc demo-env guestbook-lb guestbook-dpl:1* 0/1 The following is a summary of the arguments passed in the above command: service create : creates a new service --loadbalancer demo-env:guestbook-lb : the fully-qualified name of the load balancer; in this case, the load balancer named guestbook-lb in the environment named demo-env . - (It is not strictly necessary to use the fully qualified name of the load balancer, unless another load balancer with exactly the same name exists in a different environment.) demo-env : the name of the environment you created in Part 1 guestbook-svc : a name for the service you are creating guestbook-dpl : the name of the deploy that you created in Part 3 Layer0 services can be queried using the same get and list commands that we've come to expect by now.","title":"Part 4: Create the Service"},{"location":"guides/walkthrough/deployment-1/#check-the-status-of-the-service","text":"After a service has been created, it may take several minutes for that service to completely finish deploying. A service's status may be checked by using the service get command. Let's take a peek at our guestbook-svc service. At the command prompt, execute the following: l0 service get demo-env:guestbook-svc If we're quick enough, we'll be able to see the first stage of the process (this is what was output after running the service create command up in Part 4). We should see an asterisk (*) next to the name of the guestbook-dpl:1 deploy, which indicates that the service is in a transitional state: SERVICE ID SERVICE NAME ENVIRONMENT LOADBALANCER DEPLOYMENTS SCALE guestbo9364b guestbook-svc demo-env guestbook-lb guestbook-dpl:1* 0/1 In the next phase of deployment, if we execute the service get command again, we will see (1) in the Scale column; this indicates that 1 copy of the service is transitioning to an active state: SERVICE ID SERVICE NAME ENVIRONMENT LOADBALANCER DEPLOYMENTS SCALE guestbo9364b guestbook-svc demo-env guestbook-lb guestbook-dpl:1* 0/1 (1) In the final phase of deployment, we will see 1/1 in the Scale column; this indicates that the service is running 1 copy: SERVICE ID SERVICE NAME ENVIRONMENT LOADBALANCER DEPLOYMENTS SCALE guestbo9364b guestbook-svc demo-env guestbook-lb guestbook-dpl:1 1/1","title":"Check the Status of the Service"},{"location":"guides/walkthrough/deployment-1/#get-the-applications-url","text":"Once the service has been completely deployed, we can obtain the URL for the application and launch it in a browser. At the command prompt, execute the following: l0 loadbalancer get demo-env:guestbook-lb We should see output like the following: LOADBALANCER ID LOADBALANCER NAME ENVIRONMENT SERVICE PORTS PUBLIC URL IDLE TIMEOUT guestboc8d07 guestbook-lb demo-env guestbook-svc 80:80/HTTP true <url> 60 Copy the value shown in the URL column and paste it into a web browser. The guestbook application will appear (once the service has completely finished deploying).","title":"Get the Application's URL"},{"location":"guides/walkthrough/deployment-1/#logs","text":"Output from a Service's docker containers may be acquired by running the following command: l0 service logs <SERVICE> guestbook --------- 2018/11/05 19:11:46 Using memory backend 2018/11/05 19:11:46 Listening on :80","title":"Logs"},{"location":"guides/walkthrough/deployment-1/#cleanup-cli","text":"If you're finished with the example and don't want to continue with this walkthrough, you can instruct Layer0 to delete the environment and terminate the application. l0 environment delete demo-env However, if you intend to continue through Deployment 2 , you will want to keep the resources you made in this section.","title":"Cleanup CLI"},{"location":"guides/walkthrough/deployment-1/#deploy-with-terraform","text":"Instead of using the Layer0 CLI directly, you can instead use our Terraform provider, and deploy using Terraform ( learn more ) . You can use Terraform with Layer0 and AWS to create \"fire-and-forget\" deployments for your applications. If you're following along, you'll want to be working in the walkthrough/deployment-1/ directory of your clone of the guides repo. We use these files to set up a Layer0 environment with Terraform: Filename Purpose main.tf Provisions resources; populates resources in template files outputs.tf Values that Terraform will yield during deployment terraform.tfstate Tracks status of deployment (created and managed by Terraform) terraform.tfvars Variables specific to the environment and application(s) variables.tf Values that Terraform will use during deployment","title":"Deploy with Terraform"},{"location":"guides/walkthrough/deployment-1/#tf-a-brief-aside","text":"Let's take a moment to discuss the .tf files. The names of these files (and even the fact that they are separated out into multiple files at all) are completely arbitrary and exist soley for human-readability. Terraform understands all .tf files in a directory all together. In variables.tf , you'll see \"endpoint\" and \"token\" variables. In outputs.tf , you'll see that Terraform should spit out the url of the guestbook's load balancer once deployment has finished. In main.tf , you'll see the bulk of the deployment process. If you've followed along with the Layer0 CLI deployment above, it should be fairly easy to see how blocks in this file map to steps in the CLI process. When we began the CLI deployment, our first step was to create an environment: l0 environment create demo-env This command is recreated in main.tf like so: # walkthrough/deployment-1/main.tf resource \"layer0_environment\" \"demo-env\" { name = \"demo-env\" } We've bundled up the heart of the Guestbook deployment (load balancer, deploy, service, etc.) into a Terraform module . To use it, we declare a module block and pass in the source of the module as well as any configuration or variables that the module needs. # walkthrough/deployment-1/main.tf module \"guestbook\" { source = \"github.com/quintilesims/guides//guestbook/module\" environment_id = \" ${ layer0_environment . demo . id } \" } You can see that we pass in the ID of the environment we create. All variables declared in this block are passed to the module, so the next file we should look at is variables.tf inside of the module to get an idea of what the module is expecting. There are a lot of variables here, but only one of them doesn't have a default value. # guestbook/module/variables.tf variable \"environment_id\" { description = \"id of the layer0 environment in which to create resources\" } You'll notice that this is the variable that we're passing in. For this particular deployment of the Guestbook, all of the default options are fine. We could override any of them if we wanted to, just by specifying a new value for them back in deployment-1/main.tf . Now that we've seen the variables that the module will have, let's take a look at part of module/main.tf and see how some of them might be used: # guestbook/module/main.tf resource \"layer0_load_balancer\" \"guestbook-lb\" { name = \" ${ var . load_balancer_name } \" environment = \" ${ var . environment_id } \" port { host_port = 80 container_port = 80 protocol = \"http\" } } ... You can follow this link to learn more about Layer0 resources in Terraform.","title":"*.tf: A Brief Aside"},{"location":"guides/walkthrough/deployment-1/#part-1-terraform-init","text":"This deployment has provider dependencies so an init call must be made. (Terraform v0.11~ requries init) At the command prompt, execute the following command: terraform init We should see output like the following: Initializing modules... - module.guestbook Getting source \"github.com/quintilesims/guides//guestbook/module\" Initializing provider plugins... - Checking for available provider plugins on https://releases.hashicorp.com... - Downloading plugin for provider \"template\" (1.0.0)... The following providers do not have any version constraints in configuration, so the latest version was installed. To prevent automatic upgrades to new major versions that may contain breaking changes, it is recommended to add version = \"...\" constraints to the corresponding provider blocks in configuration, with the constraint strings suggested below. * provider.template: version = \"~> 1.0\" Terraform has been successfully initialized! You may now begin working with Terraform. Try running \"terraform plan\" to see any changes that are required for your infrastructure. All Terraform commands should now work. If you ever set or change modules or backend configuration for Terraform, rerun this command to reinitialize your working directory. If you forget, other commands will detect it and remind you to do so if necessary.","title":"Part 1: Terraform Init"},{"location":"guides/walkthrough/deployment-1/#part-2-terraform-plan","text":"Before we actually create/update/delete any resources, it's a good idea to find out what Terraform intends to do. Run terraform plan . Terraform will prompt you for configuration values that it does not have: var.endpoint Enter a value: var.token Enter a value: You can find these values by running l0-setup endpoint <your layer0 prefix> . Note There are a few ways to configure Terraform so that you don't have to keep entering these values every time you run a Terraform command (editing the terraform.tfvars file, or exporting environment variables like TF_VAR_endpoint and TF_VAR_token , for example). See the Terraform Docs for more. The plan command should give us output like the following: Refreshing Terraform state in-memory prior to plan... The refreshed state will be used to calculate this plan, but will not be persisted to local or remote state storage. data.template_file.guestbook: Refreshing state... ------------------------------------------------------------------------ An execution plan has been generated and is shown below. Resource actions are indicated with the following symbols: + create Terraform will perform the following actions: + layer0_environment.demo id: <computed> ami: <computed> current_scale: <computed> instance_type: \"t2.small\" name: \"demo\" os: \"linux\" scale: \"0\" security_group_id: <computed> + module.guestbook.layer0_deploy.guestbook id: <computed> content: \"{\\n \\\"AWSEBDockerrunVersion\\\": 2,\\n \\\"containerDefinitions\\\": [\\n {\\n \\\"name\\\": \\\"guestbook\\\",\\n \\\"image\\\": \\\"quintilesims/guestbook\\\",\\n \\\"essential\\\": true,\\n \\\"memory\\\": 128,\\n \\\"environment\\\": [\\n {\\n \\\"name\\\": \\\"GUESTBOOK_BACKEND_TYPE\\\",\\n \\\"value\\\": \\\"memory\\\"\\n },\\n {\\n \\\"name\\\": \\\"GUESTBOOK_BACKEND_CONFIG\\\",\\n \\\"value\\\": \\\"\\\"\\n },\\n {\\n \\\"name\\\": \\\"AWS_ACCESS_KEY_ID\\\",\\n \\\"value\\\": \\\"\\\"\\n },\\n {\\n \\\"name\\\": \\\"AWS_SECRET_ACCESS_KEY\\\",\\n \\\"value\\\": \\\"\\\"\\n },\\n {\\n \\\"name\\\": \\\"AWS_REGION\\\",\\n \\\"value\\\": \\\"us-west-2\\\"\\n }\\n ],\\n \\\"portMappings\\\": [\\n {\\n \\\"hostPort\\\": 80,\\n \\\"containerPort\\\": 80\\n }\\n ]\\n }\\n ]\\n}\\n\" name: \"guestbook\" version: <computed> + module.guestbook.layer0_load_balancer.guestbook id: <computed> environment: \" ${ var . environment_id } \" health_check.#: <computed> idle_timeout: \"60\" name: \"guestbook\" port.#: \"1\" port.2027667003.certificate_arn: \"\" port.2027667003.container_port: \"80\" port.2027667003.host_port: \"80\" port.2027667003.protocol: \"http\" private: <computed> type: \"application\" url: <computed> + module.guestbook.layer0_service.guestbook id: <computed> deploy: \" ${ var . deploy_id == \\ \" \\\" ? layer0_deploy.guestbook.id : var.deploy_id } \" environment: \" ${ var . environment_id } \" load_balancer: \" ${ layer0_load_balancer . guestbook . id } \" name: \"guestbook\" scale: \"1\" stateful: \"false\" Plan: 4 to add, 0 to change, 0 to destroy. This shows you that Terraform intends to create a deploy, an environment, a load balancer, and a service, all through Layer0. If you've gone through this deployment using the Layer0 CLI , you may notice that these resources appear out of order - that's fine. Terraform presents these resources in alphabetical order, but underneath, it knows the correct order in which to create them. Once we're satisfied that Terraform will do what we want it to do, we can move on to actually making these things exist!","title":"Part 2: Terraform Plan"},{"location":"guides/walkthrough/deployment-1/#part-3-terraform-apply","text":"Run terraform apply to begin the process. We should see output like the following: layer0_environment.demo: Refreshing state... ... ... ... layer0_service.guestbook: Creation complete Apply complete! Resources: 7 added, 0 changed, 0 destroyed. The state of your infrastructure has been saved to the path below. This state is required to modify and destroy your infrastructure, so keep it safe. To inspect the complete state use the `terraform show` command. State path: terraform.tfstate Outputs: guestbook_url = <http endpoint for the sample application> Note It may take a few minutes for the guestbook service to launch and the load balancer to become available. During that time you may get HTTP 503 errors when making HTTP requests against the load balancer URL.","title":"Part 3: Terraform Apply"},{"location":"guides/walkthrough/deployment-1/#whats-happening","text":"Terraform provisions the AWS resources through Layer0, configures environment variables for the application, and deploys the application into a Layer0 environment. Terraform also writes the state of your deployment to the terraform.tfstate file (creating a new one if it's not already there).","title":"What's Happening"},{"location":"guides/walkthrough/deployment-1/#cleanup-terraform","text":"When you're finished with the example, you can instruct Terraform to destroy the Layer0 environment, and terminate the application. Execute the following command (in the same directory): terraform destroy It's also now safe to remove the .terraform/ directory and the *.tfstate* files.","title":"Cleanup Terraform"},{"location":"guides/walkthrough/deployment-2/","text":"Deployment 2: Guestbook + Redis # In this section, we're going to add some complexity to the previous deployment. Deployment 1 saw us create a simple guestbook application which kept its data in memory. But what if that ever came down, either by intention or accident? It would be easy enough to redeploy it, but all of the entered data would be lost. What if we wanted to scale the application to run more than one copy? For this deployment, we're going to separate the data store from the guestbook application by creating a second Layer0 service which will house a Redis database server and linking it to the first. You can choose to complete this section using either the Layer0 CLI or Terraform . Deploy with Layer0 CLI # For this example, we'll be working in the walkthrough/deployment-2/ directory of the guides repo. We assume that you've completed the Layer0 CLI section of Deployment 1. Files used in this deployment: Filename Purpose Guestbook.Dockerrun.aws.json Template for running the Guestbook application Redis.Dockerrun.aws.json Template for running a Redis server Part 1: Create the Redis Load Balancer # Both the Guestbook service and the Redis service will live in the same Layer0 environment, so we don't need to create one like we did in the first deployment. We'll start by making a load balancer behind which the Redis service will be deployed. The Redis.Dockerrun.aws.json task definition file we'll use is very simple - it just spins up a Redis server with the default configuration, which means that it will be serving on port 6379. Our load balancer needs to be able to forward TCP traffic to and from this port. And since we don't want the Redis server to be exposed to the public internet, we'll put it behind a private load balancer; private load balancers only accept traffic that originates from within their own environment. We'll also need to specify a non-default health check target, since the load balancer won't expose port 80. At the command prompt, execute the following: l0 loadbalancer create --port 6379:6379/tcp --private --healthcheck-target tcp:6379 demo-env redis-lb We should see output like the following: LOADBALANCER ID LOADBALANCER NAME ENVIRONMENT SERVICE PORTS PUBLIC URL IDLE TIMEOUT redislb16ae6 redis-lb demo-env 6378:6379:TCP false 60 The following is a summary of the arguments passed in the above command: loadbalancer create : creates a new load balancer --port 6379:6379/TCP : instructs the load balancer to forward requests from port 6379 on the load balancer to port 6379 in the EC2 instance using the TCP protocol --private : instructs the load balancer to ignore external traffic --healthcheck-target tcp:6379 : instructs the load balancer to check the health of the service via TCP pings to port 6379 demo-env : the name of the environment in which the load balancer is being created redis-lb : a name for the load balancer itself Part 2: Deploy the ECS Task Definition # Here, we just need to create the deploy using the Redis.Dockerrun.aws.json task definition file. At the command prompt, execute the following: l0 deploy create Redis.Dockerrun.aws.json redis-dpl We should see output like the following: DEPLOY ID DEPLOY NAME VERSION redis-dpl.1 redis-dpl 1 The following is a summary of the arguments passed in the above command: deploy create : creates a new Layer0 Deploy and allows you to specify an ECS task definition Redis.Dockerrun.aws.json : the file name of the ECS task definition (use the full path of the file if it is not in your current working directory) redis-dpl : a name for the deploy, which we will use later when we create the service Part 3: Create the Redis Service # Here, we just need to pull the previous resources together into a service. At the command prompt, execute the following: l0 service create --wait --loadbalancer demo-env:redis-lb demo-env redis-svc redis-dpl:latest We should see output like the following: SERVICE ID SERVICE NAME ENVIRONMENT LOADBALANCER DEPLOYMENTS SCALE redislb16ae6 redis-svc demo-env redis-lb redis-dpl:1 0/1 The following is a summary of the arguments passed in the above commands: service create : creates a new Layer0 Service --wait : instructs the CLI to keep hold of the shell until the service has been successfully deployed --loadbalancer demo-env:redis-lb : the fully-qualified name of the load balancer; in this case, the load balancer named redis-lb in the environment named demo-env (Again, it's not strictly necessary to use the fully-qualified name of the load balancer as long as there isn't another load balancer with the same name in a different environment) demo-env : the name of the environment in which the service is to reside redis-svc : a name for the service we're creating redis-dpl:latest : the name of the deploy the service will put into action (We use : to specify which deploy we want - :latest will always give us the most recently-created one.) Part 4: Check the Status of the Redis Service # As in the first deployment, we can keep an eye on our service by using the service get command: l0 service get redis-svc Once the service has finished scaling, try looking at the service's logs to see the output that the Redis server creates: l0 service logs redis-svc Among some warnings and information not important to this exercise and a fun bit of ASCII art, you should see something like the following: ... # words and ASCII art 1:M 05 Apr 23:29:47.333 * The server is now ready to accept connections on port 6379 Now we just need to teach the Guestbook application how to talk with our Redis service. Part 5: Update the Guestbook Deploy # You should see in walkthrough/deployment-2/ another Guestbook.Dockerrun.aws.json file. This file is very similar to but not the same as the one in deployment-1/ - if you open it up, you can see the following additions: ... \"environment\" : [ { \"name\" : \"GUESTBOOK_BACKEND_TYPE\" , \"value\" : \"redis\" }, { \"name\" : \"GUESTBOOK_BACKEND_CONFIG\" , \"value\" : \"<redis host and port here>\" } ] , ... The \"GUESTBOOK_BACKEND_CONFIG\" variable is what will point the Guestbook application towards the Redis server. The <redis host and port here> section needs to be replaced and populated in the following format: \"value\" : \"ADDRESS_OF_REDIS_SERVER:PORT_THE_SERVER_IS_SERVING_ON\" We already know that Redis is serving on port 6379, so let's go find the server's address. Remember, it lives behind a load balancer that we made, so run the following command: l0 loadbalancer get redis-lb We should see output like the following: LOADBALANCER ID LOADBALANCER NAME ENVIRONMENT SERVICE PORTS PUBLIC URL IDLE TIMEOUT redislb16ae6 redis-lb demo-env redis-svc 6379:6379/TCP false internal-l0-<yadda-yadda>.elb.amazonaws.com 60 Copy that URL value, replace <redis host and port here> with the URL value in Guestbook.Dockerrun.aws.json , append :6379 to it, and save the file. It should look something like the following: ... \"environment\" : [ { \"name\" : \"GUESTBOOK_BACKEND_CONFIG\" , \"value\" : \"internal-l0-<yadda-yadda>.elb.amazonaws.com:6379\" } ] , ... Now, we can create an updated deploy: l0 deploy create Guestbook.Dockerrun.aws.json guestbook-dpl We should see output like the following: DEPLOY ID DEPLOY NAME VERSION guestbook-dpl.2 guestbook-dpl 2 Part 6: Update the Guestbook Service # Almost all the pieces are in place! Now we just need to apply the new Guestbook deploy to the running Guestbook service: l0 service update guestbook-svc guestbook-dpl:latest As the Guestbook service moves through the phases of its update process, we should see outputs like the following (if we keep an eye on the service with l0 service get guestbook-svc , that is): SERVICE ID SERVICE NAME ENVIRONMENT LOADBALANCER DEPLOYMENTS SCALE guestbo5fadd guestbook-svc demo-env guestbook-lb guestbook-dpl:2* 1/1 guestbook-dpl:1 above: guestbook-dpl:2 is in a transitional state SERVICE ID SERVICE NAME ENVIRONMENT LOADBALANCER DEPLOYMENTS SCALE guestbo5fadd guestbook-svc demo-env guestbook-lb guestbook-dpl:2 2/1 guestbook-dpl:1 above: both versions of the deployment are running at scale SERVICE ID SERVICE NAME ENVIRONMENT LOADBALANCER DEPLOYMENTS SCALE guestbo5fadd guestbook-svc demo-env guestbook-lb guestbook-dpl:2 1/1 guestbook-dpl:1* above: guestbook-dpl:1 is in a transitional state SERVICE ID SERVICE NAME ENVIRONMENT LOADBALANCER DEPLOYMENTS SCALE guestbo5fadd guestbook-svc demo-env guestbook-lb guestbook-dpl:2 1/1 above: guestbook-dpl:1 has been removed, and only guestbook-dpl:2 remains Part 7: Prove It # You should now be able to point your browser at the URL for the Guestbook load balancer (run l0 loadbalancer get guestbook-lb to find it) and see what looks like the same Guestbook application you deployed in the first section of the walkthrough. Go ahead and add a few entries, make sure it's functioning properly. We'll wait. Now, let's prove that we've actually separated the data from the application by deleting and redeploying the Guestbook application: l0 service delete --wait guestbook-svc (We'll leave the deploy intact so we can spin up a new service easily, and we'll leave the environment untouched because it also contained the Redis server. We'll also pass the --wait flag so that we don't need to keep checking on the status of the job to know when it's complete.) Once those resources have been deleted, we can recreate them! Create another service, using the guestbook-dpl deploy we kept around: l0 service create --loadbalancer demo-env:guestbook-lb demo-env guestbook-svc guestbook-dpl:latest Wait for everything to spin up, and hit that new load balancer's url ( l0 loadbalancer get guestbook-lb ) with your browser. Your data should still be there! Cleanup CLI # When you're finished with the example, you can instruct Layer0 to delete the environment and terminate the application. l0 environment delete demo-env Deploy with Terraform # As before, we can complete this deployment using Terraform and the Layer0 provider instead of the Layer0 CLI. As before, we will assume that you've cloned the guides repo and are working in the walkthrough/deployment-2/ directory. We'll use these files to manage our deployment with Terraform: Filename Purpose main.tf Provisions resources; populates variables in template files outputs.tf Values that Terraform will yield during deployment terraform.tfstate Tracks status of deployment (created and managed by Terraform) terraform.tfvars Variables specific to the environment and application(s) variables.tf Values that Terraform will use during deployment *.tf : A Brief Aside: Revisited # Not much is changed from Deployment 1 . In main.tf , we pull in a new, second module that will deploy Redis for us. We maintain this module as well; you can inspect the repo if you'd like. In main.tf where we pull in the Guestbook module, you'll see that we're supplying more values than we did last time, because we need some additional configuration to let the Guestbook application use a Redis backend instead of its default in-memory storage. Part 1: Terraform Get # Run terraform get to pull down the source materials Terraform will use for deployment. This will create a local .terraform/ directory. Part 2: Terraform Init # This deployment has provider dependencies so an init call must be made. (Terraform v0.11~ requires init) At the command prompt, execute the following command: terraform init We should see output like the following: Initializing modules... - module.redis Getting source \"github.com/quintilesims/redis//terraform\" - module.guestbook Getting source \"github.com/quintilesims/guides//guestbook/module\" Initializing provider plugins... - Checking for available provider plugins on https://releases.hashicorp.com... - Downloading plugin for provider \"template\" (1.0.0)... The following providers do not have any version constraints in configuration, so the latest version was installed. To prevent automatic upgrades to new major versions that may contain breaking changes, it is recommended to add version = \"...\" constraints to the corresponding provider blocks in configuration, with the constraint strings suggested below. * provider.template: version = \"~> 1.0\" Terraform has been successfully initialized! You may now begin working with Terraform. Try running \"terraform plan\" to see any changes that are required for your infrastructure. All Terraform commands should now work. If you ever set or change modules or backend configuration for Terraform, rerun this command to reinitialize your working directory. If you forget, other commands will detect it and remind you to do so if necessary. Part 3: Terraform Plan # It's always a good idea to find out what Terraform intends to do, so let's do that: terraform plan As before, we'll be prompted for any variables Terraform needs and doesn't have (see the note in Deployment 1 for configuring Terraform variables). We'll see output similar to the following: Refreshing Terraform state in-memory prior to plan... The refreshed state will be used to calculate this plan, but will not be persisted to local or remote state storage. data.template_file.redis: Refreshing state... The Terraform execution plan has been generated and is shown below. Resources are shown in alphabetical order for quick scanning. Green resources will be created (or destroyed and then created if an existing resource exists), yellow resources are being changed in-place, and red resources will be destroyed. Cyan entries are data sources to be read. Note: You didn't specify an \"-out\" parameter to save this plan, so when \"apply\" is called, Terraform can't guarantee this is what will execute. + layer0_environment.demo ami: \" <computed> \" cluster_count: \" <computed> \" links: \" <computed> \" name: \"demo\" os: \"linux\" security_group_id: \" <computed> \" size: \"m3.medium\" + module.redis.layer0_deploy.redis content: \"{\\n \\\"AWSEBDockerrunVersion\\\": 2,\\n \\\"containerDefinitions\\\": [\\n {\\n \\\"name\\\": \\\"redis\\\",\\n \\\"image\\\": \\\"redis:3.2-alpine\\\",\\n \\\"essential\\\": true,\\n \\\"memory\\\": 128,\\n \\\"portMappings\\\": [\\n {\\n \\\"hostPort\\\": 6379,\\n \\\"containerPort\\\": 6379\\n }\\n ]\\n }\\n ]\\n}\\n\\n\" name: \"redis\" + module.redis.layer0_load_balancer.redis environment: \" ${ var . environment_id } \" health_check.#: \" <computed> \" name: \"redis\" port.#: \"1\" port.1072619732.certificate: \"\" port.1072619732.container_port: \"6379\" port.1072619732.host_port: \"6379\" port.1072619732.protocol: \"tcp\" private: \"true\" url: \" <computed> \" + module.redis.layer0_service.redis deploy: \" ${ var . deploy_id == \\ \" \\\" ? layer0_deploy.redis.id : var.deploy_id } \" environment: \" ${ var . environment_id } \" load_balancer: \" ${ layer0_load_balancer . redis . id } \" name: \"redis\" scale: \"1\" wait: \"true\" < = module.guestbook.data.template_file.guestbook rendered: \" <computed> \" template: \"{\\n \\\"AWSEBDockerrunVersion\\\": 2,\\n \\\"containerDefinitions\\\": [\\n {\\n \\\"name\\\": \\\"guestbook\\\",\\n \\\"image\\\": \\\"quintilesims/guestbook\\\",\\n \\\"essential\\\": true,\\n \\\"memory\\\": 128,\\n \\\"environment\\\": [\\n {\\n \\\"name\\\": \\\"GUESTBOOK_BACKEND_TYPE\\\",\\n \\\"value\\\": \\\" ${ backend_type } \\\"\\n },\\n {\\n \\\"name\\\": \\\"GUESTBOOK_BACKEND_CONFIG\\\",\\n \\\"value\\\": \\\" ${ backend_config } \\\"\\n },\\n {\\n \\\"name\\\": \\\"AWS_ACCESS_KEY_ID\\\",\\n \\\"value\\\": \\\" ${ access_key } \\\"\\n },\\n {\\n \\\"name\\\": \\\"AWS_SECRET_ACCESS_KEY\\\",\\n \\\"value\\\": \\\" ${ secret_key } \\\"\\n },\\n {\\n \\\"name\\\": \\\"AWS_REGION\\\",\\n \\\"value\\\": \\\" ${ region } \\\"\\n }\\n ],\\n \\\"portMappings\\\": [\\n {\\n \\\"hostPort\\\": 80,\\n \\\"containerPort\\\": 80\\n }\\n ]\\n }\\n ]\\n}\\n\" vars.%: \" <computed> \" + module.guestbook.layer0_deploy.guestbook content: \" ${ data . template_file . guestbook . rendered } \" name: \"guestbook\" + module.guestbook.layer0_load_balancer.guestbook environment: \" ${ var . environment_id } \" health_check.#: \" <computed> \" name: \"guestbook\" port.#: \"1\" port.2027667003.certificate: \"\" port.2027667003.container_port: \"80\" port.2027667003.host_port: \"80\" port.2027667003.protocol: \"http\" url: \" <computed> \" + module.guestbook.layer0_service.guestbook deploy: \" ${ var . deploy_id == \\ \" \\\" ? layer0_deploy.guestbook.id : var.deploy_id } \" environment: \" ${ var . environment_id } \" load_balancer: \" ${ layer0_load_balancer . guestbook . id } \" name: \"guestbook\" scale: \"2\" wait: \"true\" Plan: 7 to add, 0 to change, 0 to destroy. We should see that Terraform intends to add 7 new resources, some of which are for the Guestbook deployment and some of which are for the Redis deployment. Part 4: Terraform Apply # Run terraform apply , and we should see output similar to the following: data.template_file.redis: Refreshing state... layer0_deploy.redis-dpl: Creating... ... ... ... layer0_service.guestbook-svc: Creation complete Apply complete! Resources: 7 added, 0 changed, 0 destroyed. The state of your infrastructure has been saved to the path below. This state is required to modify and destroy your infrastructure, so keep it safe. To inspect the complete state use the `terraform show` command. State path: terraform.tfstate Outputs: guestbook_url = <http endpoint for the sample application> Note It may take a few minutes for the guestbook service to launch and the load balancer to become available. During that time you may get HTTP 503 errors when making HTTP requests against the load balancer URL. What's Happening # Terraform provisions the AWS resources through Layer0, configures environment variables for the application, and deploys the application into a Layer0 environment. Terraform also writes the state of your deployment to the terraform.tfstate file (creating a new one if it's not already there). Cleanup Terraform # When you're finished with the example, you can instruct Terraform to destroy the Layer0 environment, and terminate the application. Execute the following command (in the same directory): terraform destroy It's also now safe to remove the .terraform/ directory and the *.tfstate* files.","title":"Walkthrough: Deployment 2"},{"location":"guides/walkthrough/deployment-2/#deployment-2-guestbook-redis","text":"In this section, we're going to add some complexity to the previous deployment. Deployment 1 saw us create a simple guestbook application which kept its data in memory. But what if that ever came down, either by intention or accident? It would be easy enough to redeploy it, but all of the entered data would be lost. What if we wanted to scale the application to run more than one copy? For this deployment, we're going to separate the data store from the guestbook application by creating a second Layer0 service which will house a Redis database server and linking it to the first. You can choose to complete this section using either the Layer0 CLI or Terraform .","title":"Deployment 2: Guestbook + Redis"},{"location":"guides/walkthrough/deployment-2/#deploy-with-layer0-cli","text":"For this example, we'll be working in the walkthrough/deployment-2/ directory of the guides repo. We assume that you've completed the Layer0 CLI section of Deployment 1. Files used in this deployment: Filename Purpose Guestbook.Dockerrun.aws.json Template for running the Guestbook application Redis.Dockerrun.aws.json Template for running a Redis server","title":"Deploy with Layer0 CLI"},{"location":"guides/walkthrough/deployment-2/#part-1-create-the-redis-load-balancer","text":"Both the Guestbook service and the Redis service will live in the same Layer0 environment, so we don't need to create one like we did in the first deployment. We'll start by making a load balancer behind which the Redis service will be deployed. The Redis.Dockerrun.aws.json task definition file we'll use is very simple - it just spins up a Redis server with the default configuration, which means that it will be serving on port 6379. Our load balancer needs to be able to forward TCP traffic to and from this port. And since we don't want the Redis server to be exposed to the public internet, we'll put it behind a private load balancer; private load balancers only accept traffic that originates from within their own environment. We'll also need to specify a non-default health check target, since the load balancer won't expose port 80. At the command prompt, execute the following: l0 loadbalancer create --port 6379:6379/tcp --private --healthcheck-target tcp:6379 demo-env redis-lb We should see output like the following: LOADBALANCER ID LOADBALANCER NAME ENVIRONMENT SERVICE PORTS PUBLIC URL IDLE TIMEOUT redislb16ae6 redis-lb demo-env 6378:6379:TCP false 60 The following is a summary of the arguments passed in the above command: loadbalancer create : creates a new load balancer --port 6379:6379/TCP : instructs the load balancer to forward requests from port 6379 on the load balancer to port 6379 in the EC2 instance using the TCP protocol --private : instructs the load balancer to ignore external traffic --healthcheck-target tcp:6379 : instructs the load balancer to check the health of the service via TCP pings to port 6379 demo-env : the name of the environment in which the load balancer is being created redis-lb : a name for the load balancer itself","title":"Part 1: Create the Redis Load Balancer"},{"location":"guides/walkthrough/deployment-2/#part-2-deploy-the-ecs-task-definition","text":"Here, we just need to create the deploy using the Redis.Dockerrun.aws.json task definition file. At the command prompt, execute the following: l0 deploy create Redis.Dockerrun.aws.json redis-dpl We should see output like the following: DEPLOY ID DEPLOY NAME VERSION redis-dpl.1 redis-dpl 1 The following is a summary of the arguments passed in the above command: deploy create : creates a new Layer0 Deploy and allows you to specify an ECS task definition Redis.Dockerrun.aws.json : the file name of the ECS task definition (use the full path of the file if it is not in your current working directory) redis-dpl : a name for the deploy, which we will use later when we create the service","title":"Part 2: Deploy the ECS Task Definition"},{"location":"guides/walkthrough/deployment-2/#part-3-create-the-redis-service","text":"Here, we just need to pull the previous resources together into a service. At the command prompt, execute the following: l0 service create --wait --loadbalancer demo-env:redis-lb demo-env redis-svc redis-dpl:latest We should see output like the following: SERVICE ID SERVICE NAME ENVIRONMENT LOADBALANCER DEPLOYMENTS SCALE redislb16ae6 redis-svc demo-env redis-lb redis-dpl:1 0/1 The following is a summary of the arguments passed in the above commands: service create : creates a new Layer0 Service --wait : instructs the CLI to keep hold of the shell until the service has been successfully deployed --loadbalancer demo-env:redis-lb : the fully-qualified name of the load balancer; in this case, the load balancer named redis-lb in the environment named demo-env (Again, it's not strictly necessary to use the fully-qualified name of the load balancer as long as there isn't another load balancer with the same name in a different environment) demo-env : the name of the environment in which the service is to reside redis-svc : a name for the service we're creating redis-dpl:latest : the name of the deploy the service will put into action (We use : to specify which deploy we want - :latest will always give us the most recently-created one.)","title":"Part 3: Create the Redis Service"},{"location":"guides/walkthrough/deployment-2/#part-4-check-the-status-of-the-redis-service","text":"As in the first deployment, we can keep an eye on our service by using the service get command: l0 service get redis-svc Once the service has finished scaling, try looking at the service's logs to see the output that the Redis server creates: l0 service logs redis-svc Among some warnings and information not important to this exercise and a fun bit of ASCII art, you should see something like the following: ... # words and ASCII art 1:M 05 Apr 23:29:47.333 * The server is now ready to accept connections on port 6379 Now we just need to teach the Guestbook application how to talk with our Redis service.","title":"Part 4: Check the Status of the Redis Service"},{"location":"guides/walkthrough/deployment-2/#part-5-update-the-guestbook-deploy","text":"You should see in walkthrough/deployment-2/ another Guestbook.Dockerrun.aws.json file. This file is very similar to but not the same as the one in deployment-1/ - if you open it up, you can see the following additions: ... \"environment\" : [ { \"name\" : \"GUESTBOOK_BACKEND_TYPE\" , \"value\" : \"redis\" }, { \"name\" : \"GUESTBOOK_BACKEND_CONFIG\" , \"value\" : \"<redis host and port here>\" } ] , ... The \"GUESTBOOK_BACKEND_CONFIG\" variable is what will point the Guestbook application towards the Redis server. The <redis host and port here> section needs to be replaced and populated in the following format: \"value\" : \"ADDRESS_OF_REDIS_SERVER:PORT_THE_SERVER_IS_SERVING_ON\" We already know that Redis is serving on port 6379, so let's go find the server's address. Remember, it lives behind a load balancer that we made, so run the following command: l0 loadbalancer get redis-lb We should see output like the following: LOADBALANCER ID LOADBALANCER NAME ENVIRONMENT SERVICE PORTS PUBLIC URL IDLE TIMEOUT redislb16ae6 redis-lb demo-env redis-svc 6379:6379/TCP false internal-l0-<yadda-yadda>.elb.amazonaws.com 60 Copy that URL value, replace <redis host and port here> with the URL value in Guestbook.Dockerrun.aws.json , append :6379 to it, and save the file. It should look something like the following: ... \"environment\" : [ { \"name\" : \"GUESTBOOK_BACKEND_CONFIG\" , \"value\" : \"internal-l0-<yadda-yadda>.elb.amazonaws.com:6379\" } ] , ... Now, we can create an updated deploy: l0 deploy create Guestbook.Dockerrun.aws.json guestbook-dpl We should see output like the following: DEPLOY ID DEPLOY NAME VERSION guestbook-dpl.2 guestbook-dpl 2","title":"Part 5: Update the Guestbook Deploy"},{"location":"guides/walkthrough/deployment-2/#part-6-update-the-guestbook-service","text":"Almost all the pieces are in place! Now we just need to apply the new Guestbook deploy to the running Guestbook service: l0 service update guestbook-svc guestbook-dpl:latest As the Guestbook service moves through the phases of its update process, we should see outputs like the following (if we keep an eye on the service with l0 service get guestbook-svc , that is): SERVICE ID SERVICE NAME ENVIRONMENT LOADBALANCER DEPLOYMENTS SCALE guestbo5fadd guestbook-svc demo-env guestbook-lb guestbook-dpl:2* 1/1 guestbook-dpl:1 above: guestbook-dpl:2 is in a transitional state SERVICE ID SERVICE NAME ENVIRONMENT LOADBALANCER DEPLOYMENTS SCALE guestbo5fadd guestbook-svc demo-env guestbook-lb guestbook-dpl:2 2/1 guestbook-dpl:1 above: both versions of the deployment are running at scale SERVICE ID SERVICE NAME ENVIRONMENT LOADBALANCER DEPLOYMENTS SCALE guestbo5fadd guestbook-svc demo-env guestbook-lb guestbook-dpl:2 1/1 guestbook-dpl:1* above: guestbook-dpl:1 is in a transitional state SERVICE ID SERVICE NAME ENVIRONMENT LOADBALANCER DEPLOYMENTS SCALE guestbo5fadd guestbook-svc demo-env guestbook-lb guestbook-dpl:2 1/1 above: guestbook-dpl:1 has been removed, and only guestbook-dpl:2 remains","title":"Part 6: Update the Guestbook Service"},{"location":"guides/walkthrough/deployment-2/#part-7-prove-it","text":"You should now be able to point your browser at the URL for the Guestbook load balancer (run l0 loadbalancer get guestbook-lb to find it) and see what looks like the same Guestbook application you deployed in the first section of the walkthrough. Go ahead and add a few entries, make sure it's functioning properly. We'll wait. Now, let's prove that we've actually separated the data from the application by deleting and redeploying the Guestbook application: l0 service delete --wait guestbook-svc (We'll leave the deploy intact so we can spin up a new service easily, and we'll leave the environment untouched because it also contained the Redis server. We'll also pass the --wait flag so that we don't need to keep checking on the status of the job to know when it's complete.) Once those resources have been deleted, we can recreate them! Create another service, using the guestbook-dpl deploy we kept around: l0 service create --loadbalancer demo-env:guestbook-lb demo-env guestbook-svc guestbook-dpl:latest Wait for everything to spin up, and hit that new load balancer's url ( l0 loadbalancer get guestbook-lb ) with your browser. Your data should still be there!","title":"Part 7: Prove It"},{"location":"guides/walkthrough/deployment-2/#cleanup-cli","text":"When you're finished with the example, you can instruct Layer0 to delete the environment and terminate the application. l0 environment delete demo-env","title":"Cleanup CLI"},{"location":"guides/walkthrough/deployment-2/#deploy-with-terraform","text":"As before, we can complete this deployment using Terraform and the Layer0 provider instead of the Layer0 CLI. As before, we will assume that you've cloned the guides repo and are working in the walkthrough/deployment-2/ directory. We'll use these files to manage our deployment with Terraform: Filename Purpose main.tf Provisions resources; populates variables in template files outputs.tf Values that Terraform will yield during deployment terraform.tfstate Tracks status of deployment (created and managed by Terraform) terraform.tfvars Variables specific to the environment and application(s) variables.tf Values that Terraform will use during deployment","title":"Deploy with Terraform"},{"location":"guides/walkthrough/deployment-2/#tf-a-brief-aside-revisited","text":"Not much is changed from Deployment 1 . In main.tf , we pull in a new, second module that will deploy Redis for us. We maintain this module as well; you can inspect the repo if you'd like. In main.tf where we pull in the Guestbook module, you'll see that we're supplying more values than we did last time, because we need some additional configuration to let the Guestbook application use a Redis backend instead of its default in-memory storage.","title":"*.tf: A Brief Aside: Revisited"},{"location":"guides/walkthrough/deployment-2/#part-1-terraform-get","text":"Run terraform get to pull down the source materials Terraform will use for deployment. This will create a local .terraform/ directory.","title":"Part 1: Terraform Get"},{"location":"guides/walkthrough/deployment-2/#part-2-terraform-init","text":"This deployment has provider dependencies so an init call must be made. (Terraform v0.11~ requires init) At the command prompt, execute the following command: terraform init We should see output like the following: Initializing modules... - module.redis Getting source \"github.com/quintilesims/redis//terraform\" - module.guestbook Getting source \"github.com/quintilesims/guides//guestbook/module\" Initializing provider plugins... - Checking for available provider plugins on https://releases.hashicorp.com... - Downloading plugin for provider \"template\" (1.0.0)... The following providers do not have any version constraints in configuration, so the latest version was installed. To prevent automatic upgrades to new major versions that may contain breaking changes, it is recommended to add version = \"...\" constraints to the corresponding provider blocks in configuration, with the constraint strings suggested below. * provider.template: version = \"~> 1.0\" Terraform has been successfully initialized! You may now begin working with Terraform. Try running \"terraform plan\" to see any changes that are required for your infrastructure. All Terraform commands should now work. If you ever set or change modules or backend configuration for Terraform, rerun this command to reinitialize your working directory. If you forget, other commands will detect it and remind you to do so if necessary.","title":"Part 2: Terraform Init"},{"location":"guides/walkthrough/deployment-2/#part-3-terraform-plan","text":"It's always a good idea to find out what Terraform intends to do, so let's do that: terraform plan As before, we'll be prompted for any variables Terraform needs and doesn't have (see the note in Deployment 1 for configuring Terraform variables). We'll see output similar to the following: Refreshing Terraform state in-memory prior to plan... The refreshed state will be used to calculate this plan, but will not be persisted to local or remote state storage. data.template_file.redis: Refreshing state... The Terraform execution plan has been generated and is shown below. Resources are shown in alphabetical order for quick scanning. Green resources will be created (or destroyed and then created if an existing resource exists), yellow resources are being changed in-place, and red resources will be destroyed. Cyan entries are data sources to be read. Note: You didn't specify an \"-out\" parameter to save this plan, so when \"apply\" is called, Terraform can't guarantee this is what will execute. + layer0_environment.demo ami: \" <computed> \" cluster_count: \" <computed> \" links: \" <computed> \" name: \"demo\" os: \"linux\" security_group_id: \" <computed> \" size: \"m3.medium\" + module.redis.layer0_deploy.redis content: \"{\\n \\\"AWSEBDockerrunVersion\\\": 2,\\n \\\"containerDefinitions\\\": [\\n {\\n \\\"name\\\": \\\"redis\\\",\\n \\\"image\\\": \\\"redis:3.2-alpine\\\",\\n \\\"essential\\\": true,\\n \\\"memory\\\": 128,\\n \\\"portMappings\\\": [\\n {\\n \\\"hostPort\\\": 6379,\\n \\\"containerPort\\\": 6379\\n }\\n ]\\n }\\n ]\\n}\\n\\n\" name: \"redis\" + module.redis.layer0_load_balancer.redis environment: \" ${ var . environment_id } \" health_check.#: \" <computed> \" name: \"redis\" port.#: \"1\" port.1072619732.certificate: \"\" port.1072619732.container_port: \"6379\" port.1072619732.host_port: \"6379\" port.1072619732.protocol: \"tcp\" private: \"true\" url: \" <computed> \" + module.redis.layer0_service.redis deploy: \" ${ var . deploy_id == \\ \" \\\" ? layer0_deploy.redis.id : var.deploy_id } \" environment: \" ${ var . environment_id } \" load_balancer: \" ${ layer0_load_balancer . redis . id } \" name: \"redis\" scale: \"1\" wait: \"true\" < = module.guestbook.data.template_file.guestbook rendered: \" <computed> \" template: \"{\\n \\\"AWSEBDockerrunVersion\\\": 2,\\n \\\"containerDefinitions\\\": [\\n {\\n \\\"name\\\": \\\"guestbook\\\",\\n \\\"image\\\": \\\"quintilesims/guestbook\\\",\\n \\\"essential\\\": true,\\n \\\"memory\\\": 128,\\n \\\"environment\\\": [\\n {\\n \\\"name\\\": \\\"GUESTBOOK_BACKEND_TYPE\\\",\\n \\\"value\\\": \\\" ${ backend_type } \\\"\\n },\\n {\\n \\\"name\\\": \\\"GUESTBOOK_BACKEND_CONFIG\\\",\\n \\\"value\\\": \\\" ${ backend_config } \\\"\\n },\\n {\\n \\\"name\\\": \\\"AWS_ACCESS_KEY_ID\\\",\\n \\\"value\\\": \\\" ${ access_key } \\\"\\n },\\n {\\n \\\"name\\\": \\\"AWS_SECRET_ACCESS_KEY\\\",\\n \\\"value\\\": \\\" ${ secret_key } \\\"\\n },\\n {\\n \\\"name\\\": \\\"AWS_REGION\\\",\\n \\\"value\\\": \\\" ${ region } \\\"\\n }\\n ],\\n \\\"portMappings\\\": [\\n {\\n \\\"hostPort\\\": 80,\\n \\\"containerPort\\\": 80\\n }\\n ]\\n }\\n ]\\n}\\n\" vars.%: \" <computed> \" + module.guestbook.layer0_deploy.guestbook content: \" ${ data . template_file . guestbook . rendered } \" name: \"guestbook\" + module.guestbook.layer0_load_balancer.guestbook environment: \" ${ var . environment_id } \" health_check.#: \" <computed> \" name: \"guestbook\" port.#: \"1\" port.2027667003.certificate: \"\" port.2027667003.container_port: \"80\" port.2027667003.host_port: \"80\" port.2027667003.protocol: \"http\" url: \" <computed> \" + module.guestbook.layer0_service.guestbook deploy: \" ${ var . deploy_id == \\ \" \\\" ? layer0_deploy.guestbook.id : var.deploy_id } \" environment: \" ${ var . environment_id } \" load_balancer: \" ${ layer0_load_balancer . guestbook . id } \" name: \"guestbook\" scale: \"2\" wait: \"true\" Plan: 7 to add, 0 to change, 0 to destroy. We should see that Terraform intends to add 7 new resources, some of which are for the Guestbook deployment and some of which are for the Redis deployment.","title":"Part 3: Terraform Plan"},{"location":"guides/walkthrough/deployment-2/#part-4-terraform-apply","text":"Run terraform apply , and we should see output similar to the following: data.template_file.redis: Refreshing state... layer0_deploy.redis-dpl: Creating... ... ... ... layer0_service.guestbook-svc: Creation complete Apply complete! Resources: 7 added, 0 changed, 0 destroyed. The state of your infrastructure has been saved to the path below. This state is required to modify and destroy your infrastructure, so keep it safe. To inspect the complete state use the `terraform show` command. State path: terraform.tfstate Outputs: guestbook_url = <http endpoint for the sample application> Note It may take a few minutes for the guestbook service to launch and the load balancer to become available. During that time you may get HTTP 503 errors when making HTTP requests against the load balancer URL.","title":"Part 4: Terraform Apply"},{"location":"guides/walkthrough/deployment-2/#whats-happening","text":"Terraform provisions the AWS resources through Layer0, configures environment variables for the application, and deploys the application into a Layer0 environment. Terraform also writes the state of your deployment to the terraform.tfstate file (creating a new one if it's not already there).","title":"What's Happening"},{"location":"guides/walkthrough/deployment-2/#cleanup-terraform","text":"When you're finished with the example, you can instruct Terraform to destroy the Layer0 environment, and terminate the application. Execute the following command (in the same directory): terraform destroy It's also now safe to remove the .terraform/ directory and the *.tfstate* files.","title":"Cleanup Terraform"},{"location":"guides/walkthrough/intro/","text":"An Iterative Walkthrough # This guide aims to take you through two increasingly-complex deployment examples using Layer0. Successive sections build upon the previous ones, and each deployment can be completed either through the Layer0 CLI directly, or through Terraform using our custom Layer0 Terraform Provider . We assume that you're using Layer0 v0.9.0 or later. If you have not already installed and configured Layer0, see the installation guide . If you are running an older version of Layer0, you may need to upgrade . If you intend to deploy services using the Layer0 Terraform Provider, you'll want to make sure that you've installed the provider correctly. Regardless of the deployment method you choose, we maintain a guides repository that you should clone/download. It contains all the files you will need to progress through this walkthrough. As you do so, we will assume that your working directory matches the part of the guide that you're following (for example, Deployment 1 of this guide will assume that your working directory is .../walkthrough/deployment-1/ ). Table of Contents : Deployment 1 : Deploying a web service (Guestbook) Deployment 2 : Deploying Guestbook and a data store service (Redis)","title":"Walkthrough: Introduction"},{"location":"guides/walkthrough/intro/#an-iterative-walkthrough","text":"This guide aims to take you through two increasingly-complex deployment examples using Layer0. Successive sections build upon the previous ones, and each deployment can be completed either through the Layer0 CLI directly, or through Terraform using our custom Layer0 Terraform Provider . We assume that you're using Layer0 v0.9.0 or later. If you have not already installed and configured Layer0, see the installation guide . If you are running an older version of Layer0, you may need to upgrade . If you intend to deploy services using the Layer0 Terraform Provider, you'll want to make sure that you've installed the provider correctly. Regardless of the deployment method you choose, we maintain a guides repository that you should clone/download. It contains all the files you will need to progress through this walkthrough. As you do so, we will assume that your working directory matches the part of the guide that you're following (for example, Deployment 1 of this guide will assume that your working directory is .../walkthrough/deployment-1/ ). Table of Contents : Deployment 1 : Deploying a web service (Guestbook) Deployment 2 : Deploying Guestbook and a data store service (Redis)","title":"An Iterative Walkthrough"},{"location":"reference/architecture/","text":"Layer0 Architecture # Layer0 is built on top of the following primary technologies: Application Container: Docker Cloud Provider: Amazon Web Services Container Management: Amazon EC2 Container Service (ECS) Load Balancing: Amazon Elastic Load Balancing Infrastructure Configuration: Hashicorp Terraform Identity Management: Auth0","title":"Architecture"},{"location":"reference/architecture/#layer0-architecture","text":"Layer0 is built on top of the following primary technologies: Application Container: Docker Cloud Provider: Amazon Web Services Container Management: Amazon EC2 Container Service (ECS) Load Balancing: Amazon Elastic Load Balancing Infrastructure Configuration: Hashicorp Terraform Identity Management: Auth0","title":"Layer0 Architecture"},{"location":"reference/cli/","text":"Layer0 CLI Reference # Global options # The l0 application is designed to be used with one of several commands: admin , deploy , environment , job , loadbalancer , service , and task . These commands are detailed in the sections below. There are, however, some global parameters that you may specify whenever using l0 . Usage # l0 [ global options ] command subcommand [ subcommand options ] params Global options # -o [text|json], --output [text|json] - Specify the format of Layer0 outputs. By default, Layer0 outputs unformatted text; by issuing the --output json option, you can force l0 to output JSON-formatted text. -t value, --timeout value - Specify the timeout for running l0 commands. Values can be in h, m, s, or ms. -d, --debug - Print debug statements -v, --version - Display the version number of the l0 application. Admin # The admin command is used to manage the Layer0 API server. This command is used with the following subcommands: debug , sql , and version . admin debug # Use the debug subcommand to view the running version of your Layer0 API server and CLI. Usage # l0 admin debug admin sql # Use the sql subcommand to initialize the Layer0 API database. Usage # l0 admin sql Additional information # The sql subcommand is automatically executed during the Layer0 installation process; we recommend that you do not use this subcommand unless specifically directed to do so. admin version # Use the version subcommand to display the current version of the Layer0 API. Usage # l0 admin version Deploy # Deploys are ECS Task Definitions. They are configuration files that detail how to deploy your application. The deploy command is used to manage Layer0 environments. This command is used with the following subcommands: create , delete , get , and list . deploy create # Use the create subcommand to upload a Docker task definition into Layer0. Usage # l0 deploy create taskDefPath deployName Required parameters # taskDefPath - The path to the Docker task definition that you want to upload. deployName - A name for the deploy. Additional information # If deployName exactly matches the name of an existing Layer0 deploy, then the version number of that deploy will increase by 1, and the task definition you specified will replace the task definition specified in the previous version. If you use Visual Studio to modify or create your Dockerrun file, you may see an \"Invalid Dockerrun.aws.json\" error. This error is caused by the default encoding used by Visual Studio. See the \"Common issues\" page for steps to resolve this issue. Deploys created through Layer0 are rendered with a logConfiguration section for each container. If a logConfiguration section already exists, no changes are made to the section. The additional section enables logs from each container to be sent to the the Layer0 log group. This is where logs are looked up during l0 <entity> logs commands. The added logConfiguration section uses the following template: \"logConfiguration\": { \"logDriver\": \"awslogs\", \"options\": { \"awslogs-group\": \"l0-<prefix>\", \"awslogs-region\": \"<region>\", \"awslogs-stream-prefix\": \"l0\" } } } deploy delete # Use the delete subcommand to delete a version of a Layer0 deploy. Usage # l0 deploy delete deployName Required parameters # deployName - The name of the Layer0 deploy you want to delete. deploy get # Use the get subcommand to view information about an existing Layer0 deploy. Usage # l0 deploy get deployName Required parameters # deployName - The name of the Layer0 deploy for which you want to view additional information. Additional information # The get subcommand supports wildcard matching: l0 deploy get dep* would return all deploys beginning with dep . deploy list # Use the list subcommand to view a list of deploys in your instance of Layer0. Usage # l0 deploy list Environment # Layer0 environments allow you to isolate services and load balancers for specific applications. The environment command is used to manage Layer0 environments. This command is used with the following subcommands: create , delete , get , list , and setmincount . environment create # Use the create subcommand to create a new Layer0 environment. Usage # l0 environment create [--size size | --min-count mincount | --user-data path | --os os | --ami amiID] environmentName Required parameters # environmentName - A name for the environment. Optional arguments # --size size - The instance size of the EC2 instances to create in your environment (default: m3.medium). --min-count mincount - The minimum number of EC2 instances allowed in the environment's autoscaling group (default: 0). --user-data path - The user data template file to use for the environment's autoscaling group. --os os - The operating system used in the environment. Options are \"linux\" or \"windows\" (default: linux). More information on windows environments is documented below. ami amiID - A custom EC2 AMI ID to use in the environment. If not specified, Layer0 will use its default AMI ID for the specified operating system. The user data template can be used to add custom configuration to your Layer0 environment. They are usually scripts that are executed at instance launch time to ensure an EC2 instance is in the correct state after the provisioning process finishes. Layer0 uses Go Templates to render user data. Currently, two variables are passed into the template: ECSEnvironmentID and S3Bucket . Danger Please review the ECS Tutorial to better understand how to write a user data template, and use at your own risk! Linux Environments : The default Layer0 user data template is: #!/bin/bash echo ECS_CLUSTER ={{ .ECSEnvironmentID }} >> /etc/ecs/ecs.config echo ECS_ENGINE_AUTH_TYPE = dockercfg >> /etc/ecs/ecs.config yum install -y aws-cli awslogs jq aws s3 cp s3:// {{ .S3Bucket }} /bootstrap/dockercfg dockercfg cfg = $( cat dockercfg ) echo ECS_ENGINE_AUTH_DATA = $cfg >> /etc/ecs/ecs.config docker pull amazon/amazon-ecs-agent:latest start ecs Windows Environments : The default Layer0 user data template is: < powershell > # Set agent env variables for the Machine context (durable) $clusterName = \"{{ .ECSEnvironmentID }}\" Write-Host Cluster name set as : $clusterName -foreground green [Environment] :: SetEnvironmentVariable ( \"ECS_CLUSTER\" , $clusterName , \"Machine\" ) [Environment] :: SetEnvironmentVariable ( \"ECS_ENABLE_TASK_IAM_ROLE\" , \"false\" , \"Machine\" ) $agentVersion = 'v1.5.2' $agentZipUri = \"https://s3.amazonaws.com/amazon-ecs-agent/ecs-agent-windows-$agentVersion.zip\" $agentZipMD5Uri = \"$agentZipUri.md5\" # Configure docker auth Read-S3Object -BucketName {{ . S3Bucket }} -Key bootstrap / dockercfg -File dockercfg . json $dockercfgContent = [IO.File] :: ReadAllText ( \"dockercfg.json\" ) [Environment] :: SetEnvironmentVariable ( \"ECS_ENGINE_AUTH_DATA\" , $dockercfgContent , \"Machine\" ) [Environment] :: SetEnvironmentVariable ( \"ECS_ENGINE_AUTH_TYPE\" , \"dockercfg\" , \"Machine\" ) ### --- Nothing user configurable after this point --- $ecsExeDir = \"$env:ProgramFiles\\Amazon\\ECS\" $zipFile = \"$env:TEMP\\ecs-agent.zip\" $md5File = \"$env:TEMP\\ecs-agent.zip.md5\" ### Get the files from S3 Invoke-RestMethod -OutFile $zipFile -Uri $agentZipUri Invoke-RestMethod -OutFile $md5File -Uri $agentZipMD5Uri ## MD5 Checksum $expectedMD5 = ( Get-Content $md5File ) $md5 = New-Object -TypeName System . Security . Cryptography . MD5CryptoServiceProvider $actualMD5 = [System.BitConverter] :: ToString ( $md5 . ComputeHash ( [System.IO.File] :: ReadAllBytes ( $zipFile ))). replace ( '-' , '' ) if ( $expectedMD5 -ne $actualMD5 ) { echo \"Download doesn't match hash.\" echo \"Expected: $expectedMD5 - Got: $actualMD5\" exit 1 } ## Put the executables in the executable directory. Expand-Archive -Path $zipFile -DestinationPath $ecsExeDir -Force ## Start the agent script in the background. $jobname = \"ECS-Agent-Init\" $script = \"cd '$ecsExeDir'; .\\amazon-ecs-agent.ps1\" $repeat = ( New-TimeSpan -Minutes 1 ) $jobpath = $env:LOCALAPPDATA + \"\\Microsoft\\Windows\\PowerShell\\ScheduledJobs\\$jobname\\ScheduledJobDefinition.xml\" if ($( Test-Path -Path $jobpath )) { echo \"Job definition already present\" exit 0 } $scriptblock = [scriptblock] :: Create ( \"$script\" ) $trigger = New-JobTrigger -At ( Get-Date ). Date -RepeatIndefinitely -RepetitionInterval $repeat -Once $options = New-ScheduledJobOption -RunElevated -ContinueIfGoingOnBattery -StartIfOnBattery Register-ScheduledJob -Name $jobname -ScriptBlock $scriptblock -Trigger $trigger -ScheduledJobOption $options -RunNow Add-JobTrigger -Name $jobname -Trigger ( New-JobTrigger -AtStartup -RandomDelay 00 : 1 : 00 ) </ powershell > < persist > true </ persist > Note: Windows Environments Windows environments have a few quirks and idiosyncracies to be aware of: - You can view the documented caveats with ECS here . - When creating Windows environments in Layer0, the root volume sizes for instances are 200GiB to accommodate the large size of the containers. - It can take as long as 45 minutes for a new windows container to come online. environment delete # Use the delete subcommand to delete an existing Layer0 environment. Usage # l0 environment delete [--wait] environmentName Required parameters # environmentName - The name of the Layer0 environment that you want to delete. Optional arguments # --wait - Wait until the deletion is complete before exiting. Additional information # This operation performs several tasks asynchronously. When run without the --wait option, this operation will most likely exit before all of these tasks are complete; when run with the --wait option, this operation will only exit once these tasks have completed. environment get # Use the get subcommand to display information about an existing Layer0 environment. Usage # l0 environment get environmentName Required parameters # environmentName - The name of the Layer0 environment for which you want to view additional information. Additional information # The get subcommand supports wildcard matching: l0 environment get test* would return all environments beginning with test . environment list # Use the list subcommand to display a list of environments in your instance of Layer0. Usage # l0 environment list environment setmincount # Use the setmincount subcommand to set the minimum number of EC2 instances allowed the environment's autoscaling group. Usage # l0 environment setmincount environmentName count Required parameters # environmentName - The name of the Layer0 environment that you want to adjust. count - The minimum number of instances allowed in the environment's autoscaling group. environment link # Use the link subcommand to link two environments together. When environments are linked, services inside the environments are allowed to communicate with each other as if they were in the same environment. This link is bidirectional. This command is idempotent; it will succeed even if the two specified environments are already linked. Usage # l0 environment link sourceEnvironmentName destEnvironmentName Required parameters # sourceEnvironmentName - The name of the source environment to link. destEnvironmentName - The name of the destination environment to link. environment unlink # Use the unlink subcommand to remove the link between two environments. This command is idempotent; it will succeed even if the link does not exist. Usage # l0 environment unlink sourceEnvironmentName destEnvironmentName Required parameters # sourceEnvironmentName - The name of the source environment to unlink. destEnvironmentName - The name of the destination environment to unlink. Job # A Job is a long-running unit of work performed on behalf of the Layer0 API. Jobs are executed as Layer0 tasks that run in the api environment. The job command is used with the following subcommands: logs , delete , get , and list . job logs # Use the logs subcommand to display the logs from a Layer0 job that is currently running. Usage # l0 job logs [--start MM/DD HH:MM | --end MM/DD HH:MM | --tail=N] jobName Required parameters # jobName - The name of the Layer0 job for which you want to view logs. Optional arguments # --start MM/DD HH:MM - The start of the time range to fetch logs. --end MM/DD HH:MM - The end of the time range to fetch logs. --tail=N - Display only the last N lines of the log. job delete # Use the delete subcommand to delete an existing job. Usage # l0 job delete jobName Required parameters # jobName - The name of the job that you want to delete. job get # Use the get subcommand to display information about an existing Layer0 job. Usage # l0 job get jobName Required parameters # jobName - The name of an existing Layer0 job to display. Additional information # The get subcommand supports wildcard matching: l0 job get 2a55* would return all jobs beginning with 2a55 . job list # Use the list subcommand to display information about all of the existing jobs in an instance of Layer0. Usage # l0 job list Load Balancer # A load balancer is a component of a Layer0 environment. Load balancers listen for traffic on certain ports, and then forward that traffic to Layer0 services . The loadbalancer command is used with the following subcommands: create , delete , addport , dropport , get , list , healthcheck , and idletimeout . loadbalancer create # Use the create subcommand to create a new load balancer. Usage # l0 loadbalancer create [--port port ... | --certificate nameOrARN | --private | --healthcheck-target target | --healthcheck-interval interval | --healthcheck-timeout timeout | --healthcheck-healthy-threshold healthyThreshold | --healthcheck-unhealthy-threshold unhealthyThreshold | --idle-timeout idleTimeout] environmentName loadBalancerName Required parameters # environmentName - The name of the existing Layer0 environment in which you want to create the load balancer. loadBalancerName - A name for the load balancer you are creating. Optional arguments # --port port ... - The port configuration for the listener of the load balancer. Valid pattern is hostPort:containerPort/protocol . Multiple ports can be specified using --port port1 --port port2 ... (default: 80/80:TCP ). hostPort - The port that the load balancer will listen for traffic on. containerPort - The port that the load balancer will forward traffic to. protocol - The protocol to use when forwarding traffic (acceptable values: TCP, SSL, HTTP, and HTTPS). --certificate nameOrARN - The name or arn of an existing Layer0 certificate. You must include this option if you are using an HTTPS port configuration. --private - When you use this option, the load balancer will only be accessible from within the Layer0 environment. --healthcheck-target target - The target of the check. Valid pattern is PROTOCOL:PORT/PATH (default: \"TCP:80\" ). If PROTOCOL is HTTP or HTTPS , both PORT and PATH are required. Example: HTTP:80/admin/healthcheck . If PROTOCOL is TCP or SSL , PORT is required and PATH is not used. Example: TCP:80 --healthcheck-interval interval - The interval between checks (default: 30 ). --healthcheck-timeout timeout - The length of time before the check times out (default: 5 ). --healthcheck-healthy-threshold healthyThreshold - The number of checks before the instance is declared healthy (default: 2 ). --healthcheck-unhealthy-threshold unhealthyThreshold - The number of checks before the instance is declared unhealthy (default: 2 ). --idle-timeout idleTimeout - The idle timeout in seconds. Ports and Health Checks When both the --port and the --healthcheck-target options are omitted, Layer0 configures the load balancer with some default values: 80:80/TCP for ports and TCP:80 for healthcheck target. These default values together create a load balancer configured with a simple but functioning health check, opening up a set of ports that allows traffic to the target of the healthcheck. ( --healthcheck-target TCP:80 tells the load balancer to ping its services at port 80 to determine their status, and --port 80:80/TCP configures a security group to allow traffic to pass between port 80 of the load balancer and port 80 of its services) When creating a load balancer with non-default configurations for either --port or --healthcheck-target , make sure that a valid --port and --healthcheck-target pairing is also created. loadbalancer delete # Use the delete subcommand to delete an existing load balancer. Usage # l0 loadbalancer delete [--wait] loadBalancerName Required parameters # loadBalancerName - The name of the load balancer that you want to delete. Optional arguments # --wait - Wait until the deletion is complete before exiting. Additional information # In order to delete a load balancer that is already attached to a service, you must first delete the service that uses the load balancer. This operation performs several tasks asynchronously. When run without the --wait option, this operation will most likely exit before all of these tasks are complete; when run with the --wait option, this operation will only exit once these tasks have completed . loadbalancer addport # Use the addport subcommand to add a new port configuration to an existing Layer0 load balancer. Usage # l0 loadbalancer addport [--certificate certificateName] loadBalancerName port Required parameters # loadBalancerName - The name of an existing Layer0 load balancer in which you want to add the port configuration. port - The port configuration for the listener of the load balancer. Valid pattern is hostPort:containerPort/protocol . hostPort - The port that the load balancer will listen for traffic on. containerPort - The port that the load balancer will forward traffic to. protocol - The protocol to use when forwarding traffic (acceptable values: TCP, SSL, HTTP, and HTTPS). Optional arguments # --certificate certificateName - The name of an existing Layer0 certificate. You must include this option if you are using an HTTPS port configuration. Additional information # The port configuration you specify must not already be in use by the load balancer you specify. loadbalancer dropport # Use the dropport subcommand to remove a port configuration from an existing Layer0 load balancer. Usage # l0 loadbalancer dropport loadBalancerName hostPort Required parameters # loadBalancerName - The name of an existing Layer0 load balancer from which you want to remove the port configuration. hostPort - The host port to remove from the load balancer. loadbalancer get # Use the get subcommand to display information about an existing Layer0 load balancer. Usage # l0 loadbalancer get [environmentName:]loadBalancerName Required parameters # [environmentName:]loadBalancerName - The name of an existing Layer0 load balancer. You can optionally provide the Layer0 environment ( environmentName ) associated with the Load Balancer Additional information # The get subcommand supports wildcard matching: l0 loadbalancer get entrypoint* would return all jobs beginning with entrypoint . loadbalancer list # Use the list subcommand to display information about all of the existing load balancers in an instance of Layer0. Usage # l0 loadbalancer list loadbalancer healthcheck # Use the healthcheck subcommand to display information about or update the configuration of a load balancer's health check. Usage # l0 loadbalancer healthcheck [--set-target target | --set-interval interval | --set-timeout timeout | --set-healthy-threshold healthyThreshold | --set-unhealthy-threshold unhealthyThreshold] loadbalancerName Required parameters # loadBalancerName - The name of the existing Layer0 load balancer you are modifying. Optional arguments # --set-target target - The target of the check. Valid pattern is PROTOCOL:PORT/PATH . If PROTOCOL is HTTP or HTTPS , both PORT and PATH are required. Example: HTTP:80/admin/healthcheck . If PROTOCOL is TCP or SSL , PORT is required and PATH is not used. Example: TCP:80 --set-interval interval - The interval between health checks. --set-timeout timeout - The length of time in seconds before the health check times out. --set-healthy-threshold healthyThreshold - The number of checks before the instance is declared healthy. --set-unhealthy-threshold unhealthyThreshold - The number of checks before the instance is declared unhealthy. Additional information # Calling the subcommand without flags will display the current configuration of the load balancer's health check. Setting any of the flags will update the corresponding field in the health check, and all omitted flags will leave the corresponding fields unchanged. loadbalancer idletimeout # Use the idletimeout subcommand to update the idle timeout of a load balancer. The load balancer manages an idle timeout that is triggered when no data is sent over a connection for the specified time period. If no data has been sent or received by the time that the idle timeout period elapses, the load balancer closes the connection. See the following documentation for more information: https://docs.aws.amazon.com/elasticloadbalancing/latest/application/application-load-balancers.html#connection-idle-timeout Usage # l0 loadbalancer idletimeout loadbalancerName idleTimeout Required parameters # loadBalancerName - The name of the existing Layer0 load balancer you are modifying. idleTimeout - The idle timeout in seconds. Service # A service is a component of a Layer0 environment. The purpose of a service is to execute a Docker image specified in a deploy . In order to create a service, you must first create an environment and a deploy ; in most cases, you should also create a load balancer before creating the service. The service command is used with the following subcommands: create , delete , get , update , list , logs , and scale . service create # Use the create subcommand to create a Layer0 service. Usage # l0 service create [--loadbalancer [environmentName:]loadBalancerName | --no-logs] environmentName serviceName deployName[:deployVersion] Required parameters # serviceName - A name for the service that you are creating. environmentName - The name of an existing Layer0 environment. deployName[:deployVersion] - The name of a Layer0 deploy that exists in the environment environmentName . You can optionally specify the version number of the Layer0 deploy that you want to deploy. If you do not specify a version number, the latest version of the deploy will be used. Optional arguments # --loadbalancer [environmentName:]loadBalancerName - Place the new service behind an existing load balancer loadBalancerName . You can optionally specify the Layer0 environment ( environmentName ) where the load balancer exists. --no-logs - Disable cloudwatch logging for the service service update # Use the update subcommand to apply an existing Layer0 Deploy to an existing Layer0 service. Usage # l0 service update [--no-logs] [environmentName:]serviceName deployName[:deployVersion] Required parameters # [environmentName:]serviceName - The name of an existing Layer0 service into which you want to apply the deploy. You can optionally specify the Layer0 environment ( environmentName ) of the service. deployName[:deployVersion] - The name of the Layer0 deploy that you want to apply to the service. You can optionally specify a specific version of the deploy ( deployVersion ). If you do not specify a version number, the latest version of the deploy will be applied. Optional arguments # --no-logs - Disable cloudwatch logging for the service Additional information # If your service uses a load balancer, when you update the task definition for the service, the container name and container port that were specified when the service was created must remain the same in the task definition. In other words, if your service has a load balancer, you cannot apply any deploy you want to that service. If you are varying the container name or exposed ports, you must create a new service instead. service delete # Use the delete subcommand to delete an existing Layer0 service. Usage # l0 service delete [--wait] [environmentName:]serviceName Required parameters # [environmentName:]serviceName - The name of the Layer0 service that you want to delete. You can optionally provide the Layer0 environment ( environmentName ) of the service. Optional arguments # --wait - Wait until the deletion is complete before exiting. Additional information # This operation performs several tasks asynchronously. When run without the --wait option, this operation will most likely exit before all of these tasks are complete; when run with the --wait option, this operation will only exit once these tasks have completed. service get # Use the get subcommand to display information about an existing Layer0 service. Usage # l0 service get [environmentName:]serviceName Required parameters # [environmentName:]serviceName - The name of an existing Layer0 service. You can optionally provide the Layer0 environment ( environmentName ) of the service. service list # Use the list subcommand to list all of the existing services in your Layer0 instance. Usage # l0 service get list service logs # Use the logs subcommand to display the logs from a Layer0 service that is currently running. Usage # l0 service logs [--start MM/DD HH:MM | --end MM/DD HH:MM | --tail=N] serviceName Required parameters # serviceName - The name of the Layer0 service for which you want to view logs. Optional arguments # --start MM/DD HH:MM - The start of the time range to fetch logs. --end MM/DD HH:MM - The end of the time range to fetch logs. --tail=N - Display only the last N lines of the log. service scale # Use the scale subcommand to specify how many copies of an existing Layer0 service should run. Usage # l0 service scale [environmentName:]serviceName copies Required parameters # [environmentName:]serviceName - The name of the Layer0 service that you want to scale up. You can optionally provide the Layer0 environment ( environmentName ) of the service. copies - The number of copies of the specified service that should be run. Task # A Layer0 task is a component of an environment. A task executes the contents of a Docker image, as specified in a deploy. A task differs from a service in that a task does not restart after exiting. Additionally, ports are not exposed when using a task. The task command is used with the following subcommands: create , delete , get , list , and logs . task create # Use the create subcommand to create a Layer0 task. Usage # l0 task create [--copies copies | --no-logs] environmentName taskName deployName Required parameters # environmentName - The name of the existing Layer0 environment in which you want to create the task. taskName - A name for the task. deployName - The name of an existing Layer0 deploy that the task should use. Optional arguments # --copies copies - The number of copies of the task to run (default: 1). --no-logs - Disable cloudwatch logging for the service. task delete # Use the delete subcommand to delete an existing Layer0 task. Usage # l0 task delete [environmentName:]taskName Required parameters # [environmentName:]taskName - The name of the Layer0 task that you want to delete. You can optionally specify the name of the Layer0 environment that contains the task. This parameter is only required if mulitiple environments contain tasks with exactly the same name. Additional information # Until the record has been purged, the API may indicate that the task is still running. Task records are typically purged within an hour. task get # Use the get subcommand to display information about an existing Layer0 task ( taskName ). Usage # l0 task get [environmentName:]taskName Required parameters # [environmentName:]taskName - The name of a Layer0 task for which you want to see information. You can optionally specify the name of the Layer0 Environment that contains the task. Additional information # The value of taskName does not need to exactly match the name of an existing task. If multiple results are found that match the pattern you specified in taskName , then information about all matching tasks will be returned. task list # Use the task subcommand to display a list of running tasks in your Layer0. Usage # l0 task list task logs # Use the logs subcommand to display logs for a running Layer0 task. Usage # l0 task logs [--start MM/DD HH:MM | --end MM/DD HH:MM | --tail=N] taskName Required parameters # taskName - The name of an existing Layer0 task. Optional arguments # --start MM/DD HH:MM - The start of the time range to fetch logs. --end MM/DD HH:MM - The end of the time range to fetch logs. --tail=N - Display only the last N lines of the log. Additional information # The value of taskName does not need to exactly match the name of an existing task. If multiple results are found that match the pattern you specified in taskName , then information about all matching tasks will be returned. task list # Use the list subcommand to display a list of running tasks in your Layer0. Usage # l0 task list","title":"Layer0 CLI"},{"location":"reference/cli/#layer0-cli-reference","text":"","title":"Layer0 CLI Reference"},{"location":"reference/cli/#global-options","text":"The l0 application is designed to be used with one of several commands: admin , deploy , environment , job , loadbalancer , service , and task . These commands are detailed in the sections below. There are, however, some global parameters that you may specify whenever using l0 .","title":"Global options"},{"location":"reference/cli/#usage","text":"l0 [ global options ] command subcommand [ subcommand options ] params","title":"Usage"},{"location":"reference/cli/#global-options_1","text":"-o [text|json], --output [text|json] - Specify the format of Layer0 outputs. By default, Layer0 outputs unformatted text; by issuing the --output json option, you can force l0 to output JSON-formatted text. -t value, --timeout value - Specify the timeout for running l0 commands. Values can be in h, m, s, or ms. -d, --debug - Print debug statements -v, --version - Display the version number of the l0 application.","title":"Global options"},{"location":"reference/cli/#admin","text":"The admin command is used to manage the Layer0 API server. This command is used with the following subcommands: debug , sql , and version .","title":"Admin"},{"location":"reference/cli/#admin-debug","text":"Use the debug subcommand to view the running version of your Layer0 API server and CLI.","title":"admin debug"},{"location":"reference/cli/#usage_1","text":"l0 admin debug","title":"Usage"},{"location":"reference/cli/#admin-sql","text":"Use the sql subcommand to initialize the Layer0 API database.","title":"admin sql"},{"location":"reference/cli/#usage_2","text":"l0 admin sql","title":"Usage"},{"location":"reference/cli/#additional-information","text":"The sql subcommand is automatically executed during the Layer0 installation process; we recommend that you do not use this subcommand unless specifically directed to do so.","title":"Additional information"},{"location":"reference/cli/#admin-version","text":"Use the version subcommand to display the current version of the Layer0 API.","title":"admin version"},{"location":"reference/cli/#usage_3","text":"l0 admin version","title":"Usage"},{"location":"reference/cli/#deploy","text":"Deploys are ECS Task Definitions. They are configuration files that detail how to deploy your application. The deploy command is used to manage Layer0 environments. This command is used with the following subcommands: create , delete , get , and list .","title":"Deploy"},{"location":"reference/cli/#deploy-create","text":"Use the create subcommand to upload a Docker task definition into Layer0.","title":"deploy create"},{"location":"reference/cli/#usage_4","text":"l0 deploy create taskDefPath deployName","title":"Usage"},{"location":"reference/cli/#required-parameters","text":"taskDefPath - The path to the Docker task definition that you want to upload. deployName - A name for the deploy.","title":"Required parameters"},{"location":"reference/cli/#additional-information_1","text":"If deployName exactly matches the name of an existing Layer0 deploy, then the version number of that deploy will increase by 1, and the task definition you specified will replace the task definition specified in the previous version. If you use Visual Studio to modify or create your Dockerrun file, you may see an \"Invalid Dockerrun.aws.json\" error. This error is caused by the default encoding used by Visual Studio. See the \"Common issues\" page for steps to resolve this issue. Deploys created through Layer0 are rendered with a logConfiguration section for each container. If a logConfiguration section already exists, no changes are made to the section. The additional section enables logs from each container to be sent to the the Layer0 log group. This is where logs are looked up during l0 <entity> logs commands. The added logConfiguration section uses the following template: \"logConfiguration\": { \"logDriver\": \"awslogs\", \"options\": { \"awslogs-group\": \"l0-<prefix>\", \"awslogs-region\": \"<region>\", \"awslogs-stream-prefix\": \"l0\" } } }","title":"Additional information"},{"location":"reference/cli/#deploy-delete","text":"Use the delete subcommand to delete a version of a Layer0 deploy.","title":"deploy delete"},{"location":"reference/cli/#usage_5","text":"l0 deploy delete deployName","title":"Usage"},{"location":"reference/cli/#required-parameters_1","text":"deployName - The name of the Layer0 deploy you want to delete.","title":"Required parameters"},{"location":"reference/cli/#deploy-get","text":"Use the get subcommand to view information about an existing Layer0 deploy.","title":"deploy get"},{"location":"reference/cli/#usage_6","text":"l0 deploy get deployName","title":"Usage"},{"location":"reference/cli/#required-parameters_2","text":"deployName - The name of the Layer0 deploy for which you want to view additional information.","title":"Required parameters"},{"location":"reference/cli/#additional-information_2","text":"The get subcommand supports wildcard matching: l0 deploy get dep* would return all deploys beginning with dep .","title":"Additional information"},{"location":"reference/cli/#deploy-list","text":"Use the list subcommand to view a list of deploys in your instance of Layer0.","title":"deploy list"},{"location":"reference/cli/#usage_7","text":"l0 deploy list","title":"Usage"},{"location":"reference/cli/#environment","text":"Layer0 environments allow you to isolate services and load balancers for specific applications. The environment command is used to manage Layer0 environments. This command is used with the following subcommands: create , delete , get , list , and setmincount .","title":"Environment"},{"location":"reference/cli/#environment-create","text":"Use the create subcommand to create a new Layer0 environment.","title":"environment create"},{"location":"reference/cli/#usage_8","text":"l0 environment create [--size size | --min-count mincount | --user-data path | --os os | --ami amiID] environmentName","title":"Usage"},{"location":"reference/cli/#required-parameters_3","text":"environmentName - A name for the environment.","title":"Required parameters"},{"location":"reference/cli/#optional-arguments","text":"--size size - The instance size of the EC2 instances to create in your environment (default: m3.medium). --min-count mincount - The minimum number of EC2 instances allowed in the environment's autoscaling group (default: 0). --user-data path - The user data template file to use for the environment's autoscaling group. --os os - The operating system used in the environment. Options are \"linux\" or \"windows\" (default: linux). More information on windows environments is documented below. ami amiID - A custom EC2 AMI ID to use in the environment. If not specified, Layer0 will use its default AMI ID for the specified operating system. The user data template can be used to add custom configuration to your Layer0 environment. They are usually scripts that are executed at instance launch time to ensure an EC2 instance is in the correct state after the provisioning process finishes. Layer0 uses Go Templates to render user data. Currently, two variables are passed into the template: ECSEnvironmentID and S3Bucket . Danger Please review the ECS Tutorial to better understand how to write a user data template, and use at your own risk! Linux Environments : The default Layer0 user data template is: #!/bin/bash echo ECS_CLUSTER ={{ .ECSEnvironmentID }} >> /etc/ecs/ecs.config echo ECS_ENGINE_AUTH_TYPE = dockercfg >> /etc/ecs/ecs.config yum install -y aws-cli awslogs jq aws s3 cp s3:// {{ .S3Bucket }} /bootstrap/dockercfg dockercfg cfg = $( cat dockercfg ) echo ECS_ENGINE_AUTH_DATA = $cfg >> /etc/ecs/ecs.config docker pull amazon/amazon-ecs-agent:latest start ecs Windows Environments : The default Layer0 user data template is: < powershell > # Set agent env variables for the Machine context (durable) $clusterName = \"{{ .ECSEnvironmentID }}\" Write-Host Cluster name set as : $clusterName -foreground green [Environment] :: SetEnvironmentVariable ( \"ECS_CLUSTER\" , $clusterName , \"Machine\" ) [Environment] :: SetEnvironmentVariable ( \"ECS_ENABLE_TASK_IAM_ROLE\" , \"false\" , \"Machine\" ) $agentVersion = 'v1.5.2' $agentZipUri = \"https://s3.amazonaws.com/amazon-ecs-agent/ecs-agent-windows-$agentVersion.zip\" $agentZipMD5Uri = \"$agentZipUri.md5\" # Configure docker auth Read-S3Object -BucketName {{ . S3Bucket }} -Key bootstrap / dockercfg -File dockercfg . json $dockercfgContent = [IO.File] :: ReadAllText ( \"dockercfg.json\" ) [Environment] :: SetEnvironmentVariable ( \"ECS_ENGINE_AUTH_DATA\" , $dockercfgContent , \"Machine\" ) [Environment] :: SetEnvironmentVariable ( \"ECS_ENGINE_AUTH_TYPE\" , \"dockercfg\" , \"Machine\" ) ### --- Nothing user configurable after this point --- $ecsExeDir = \"$env:ProgramFiles\\Amazon\\ECS\" $zipFile = \"$env:TEMP\\ecs-agent.zip\" $md5File = \"$env:TEMP\\ecs-agent.zip.md5\" ### Get the files from S3 Invoke-RestMethod -OutFile $zipFile -Uri $agentZipUri Invoke-RestMethod -OutFile $md5File -Uri $agentZipMD5Uri ## MD5 Checksum $expectedMD5 = ( Get-Content $md5File ) $md5 = New-Object -TypeName System . Security . Cryptography . MD5CryptoServiceProvider $actualMD5 = [System.BitConverter] :: ToString ( $md5 . ComputeHash ( [System.IO.File] :: ReadAllBytes ( $zipFile ))). replace ( '-' , '' ) if ( $expectedMD5 -ne $actualMD5 ) { echo \"Download doesn't match hash.\" echo \"Expected: $expectedMD5 - Got: $actualMD5\" exit 1 } ## Put the executables in the executable directory. Expand-Archive -Path $zipFile -DestinationPath $ecsExeDir -Force ## Start the agent script in the background. $jobname = \"ECS-Agent-Init\" $script = \"cd '$ecsExeDir'; .\\amazon-ecs-agent.ps1\" $repeat = ( New-TimeSpan -Minutes 1 ) $jobpath = $env:LOCALAPPDATA + \"\\Microsoft\\Windows\\PowerShell\\ScheduledJobs\\$jobname\\ScheduledJobDefinition.xml\" if ($( Test-Path -Path $jobpath )) { echo \"Job definition already present\" exit 0 } $scriptblock = [scriptblock] :: Create ( \"$script\" ) $trigger = New-JobTrigger -At ( Get-Date ). Date -RepeatIndefinitely -RepetitionInterval $repeat -Once $options = New-ScheduledJobOption -RunElevated -ContinueIfGoingOnBattery -StartIfOnBattery Register-ScheduledJob -Name $jobname -ScriptBlock $scriptblock -Trigger $trigger -ScheduledJobOption $options -RunNow Add-JobTrigger -Name $jobname -Trigger ( New-JobTrigger -AtStartup -RandomDelay 00 : 1 : 00 ) </ powershell > < persist > true </ persist > Note: Windows Environments Windows environments have a few quirks and idiosyncracies to be aware of: - You can view the documented caveats with ECS here . - When creating Windows environments in Layer0, the root volume sizes for instances are 200GiB to accommodate the large size of the containers. - It can take as long as 45 minutes for a new windows container to come online.","title":"Optional arguments"},{"location":"reference/cli/#environment-delete","text":"Use the delete subcommand to delete an existing Layer0 environment.","title":"environment delete"},{"location":"reference/cli/#usage_9","text":"l0 environment delete [--wait] environmentName","title":"Usage"},{"location":"reference/cli/#required-parameters_4","text":"environmentName - The name of the Layer0 environment that you want to delete.","title":"Required parameters"},{"location":"reference/cli/#optional-arguments_1","text":"--wait - Wait until the deletion is complete before exiting.","title":"Optional arguments"},{"location":"reference/cli/#additional-information_3","text":"This operation performs several tasks asynchronously. When run without the --wait option, this operation will most likely exit before all of these tasks are complete; when run with the --wait option, this operation will only exit once these tasks have completed.","title":"Additional information"},{"location":"reference/cli/#environment-get","text":"Use the get subcommand to display information about an existing Layer0 environment.","title":"environment get"},{"location":"reference/cli/#usage_10","text":"l0 environment get environmentName","title":"Usage"},{"location":"reference/cli/#required-parameters_5","text":"environmentName - The name of the Layer0 environment for which you want to view additional information.","title":"Required parameters"},{"location":"reference/cli/#additional-information_4","text":"The get subcommand supports wildcard matching: l0 environment get test* would return all environments beginning with test .","title":"Additional information"},{"location":"reference/cli/#environment-list","text":"Use the list subcommand to display a list of environments in your instance of Layer0.","title":"environment list"},{"location":"reference/cli/#usage_11","text":"l0 environment list","title":"Usage"},{"location":"reference/cli/#environment-setmincount","text":"Use the setmincount subcommand to set the minimum number of EC2 instances allowed the environment's autoscaling group.","title":"environment setmincount"},{"location":"reference/cli/#usage_12","text":"l0 environment setmincount environmentName count","title":"Usage"},{"location":"reference/cli/#required-parameters_6","text":"environmentName - The name of the Layer0 environment that you want to adjust. count - The minimum number of instances allowed in the environment's autoscaling group.","title":"Required parameters"},{"location":"reference/cli/#environment-link","text":"Use the link subcommand to link two environments together. When environments are linked, services inside the environments are allowed to communicate with each other as if they were in the same environment. This link is bidirectional. This command is idempotent; it will succeed even if the two specified environments are already linked.","title":"environment link"},{"location":"reference/cli/#usage_13","text":"l0 environment link sourceEnvironmentName destEnvironmentName","title":"Usage"},{"location":"reference/cli/#required-parameters_7","text":"sourceEnvironmentName - The name of the source environment to link. destEnvironmentName - The name of the destination environment to link.","title":"Required parameters"},{"location":"reference/cli/#environment-unlink","text":"Use the unlink subcommand to remove the link between two environments. This command is idempotent; it will succeed even if the link does not exist.","title":"environment unlink"},{"location":"reference/cli/#usage_14","text":"l0 environment unlink sourceEnvironmentName destEnvironmentName","title":"Usage"},{"location":"reference/cli/#required-parameters_8","text":"sourceEnvironmentName - The name of the source environment to unlink. destEnvironmentName - The name of the destination environment to unlink.","title":"Required parameters"},{"location":"reference/cli/#job","text":"A Job is a long-running unit of work performed on behalf of the Layer0 API. Jobs are executed as Layer0 tasks that run in the api environment. The job command is used with the following subcommands: logs , delete , get , and list .","title":"Job"},{"location":"reference/cli/#job-logs","text":"Use the logs subcommand to display the logs from a Layer0 job that is currently running.","title":"job logs"},{"location":"reference/cli/#usage_15","text":"l0 job logs [--start MM/DD HH:MM | --end MM/DD HH:MM | --tail=N] jobName","title":"Usage"},{"location":"reference/cli/#required-parameters_9","text":"jobName - The name of the Layer0 job for which you want to view logs.","title":"Required parameters"},{"location":"reference/cli/#optional-arguments_2","text":"--start MM/DD HH:MM - The start of the time range to fetch logs. --end MM/DD HH:MM - The end of the time range to fetch logs. --tail=N - Display only the last N lines of the log.","title":"Optional arguments"},{"location":"reference/cli/#job-delete","text":"Use the delete subcommand to delete an existing job.","title":"job delete"},{"location":"reference/cli/#usage_16","text":"l0 job delete jobName","title":"Usage"},{"location":"reference/cli/#required-parameters_10","text":"jobName - The name of the job that you want to delete.","title":"Required parameters"},{"location":"reference/cli/#job-get","text":"Use the get subcommand to display information about an existing Layer0 job.","title":"job get"},{"location":"reference/cli/#usage_17","text":"l0 job get jobName","title":"Usage"},{"location":"reference/cli/#required-parameters_11","text":"jobName - The name of an existing Layer0 job to display.","title":"Required parameters"},{"location":"reference/cli/#additional-information_5","text":"The get subcommand supports wildcard matching: l0 job get 2a55* would return all jobs beginning with 2a55 .","title":"Additional information"},{"location":"reference/cli/#job-list","text":"Use the list subcommand to display information about all of the existing jobs in an instance of Layer0.","title":"job list"},{"location":"reference/cli/#usage_18","text":"l0 job list","title":"Usage"},{"location":"reference/cli/#load-balancer","text":"A load balancer is a component of a Layer0 environment. Load balancers listen for traffic on certain ports, and then forward that traffic to Layer0 services . The loadbalancer command is used with the following subcommands: create , delete , addport , dropport , get , list , healthcheck , and idletimeout .","title":"Load Balancer"},{"location":"reference/cli/#loadbalancer-create","text":"Use the create subcommand to create a new load balancer.","title":"loadbalancer create"},{"location":"reference/cli/#usage_19","text":"l0 loadbalancer create [--port port ... | --certificate nameOrARN | --private | --healthcheck-target target | --healthcheck-interval interval | --healthcheck-timeout timeout | --healthcheck-healthy-threshold healthyThreshold | --healthcheck-unhealthy-threshold unhealthyThreshold | --idle-timeout idleTimeout] environmentName loadBalancerName","title":"Usage"},{"location":"reference/cli/#required-parameters_12","text":"environmentName - The name of the existing Layer0 environment in which you want to create the load balancer. loadBalancerName - A name for the load balancer you are creating.","title":"Required parameters"},{"location":"reference/cli/#optional-arguments_3","text":"--port port ... - The port configuration for the listener of the load balancer. Valid pattern is hostPort:containerPort/protocol . Multiple ports can be specified using --port port1 --port port2 ... (default: 80/80:TCP ). hostPort - The port that the load balancer will listen for traffic on. containerPort - The port that the load balancer will forward traffic to. protocol - The protocol to use when forwarding traffic (acceptable values: TCP, SSL, HTTP, and HTTPS). --certificate nameOrARN - The name or arn of an existing Layer0 certificate. You must include this option if you are using an HTTPS port configuration. --private - When you use this option, the load balancer will only be accessible from within the Layer0 environment. --healthcheck-target target - The target of the check. Valid pattern is PROTOCOL:PORT/PATH (default: \"TCP:80\" ). If PROTOCOL is HTTP or HTTPS , both PORT and PATH are required. Example: HTTP:80/admin/healthcheck . If PROTOCOL is TCP or SSL , PORT is required and PATH is not used. Example: TCP:80 --healthcheck-interval interval - The interval between checks (default: 30 ). --healthcheck-timeout timeout - The length of time before the check times out (default: 5 ). --healthcheck-healthy-threshold healthyThreshold - The number of checks before the instance is declared healthy (default: 2 ). --healthcheck-unhealthy-threshold unhealthyThreshold - The number of checks before the instance is declared unhealthy (default: 2 ). --idle-timeout idleTimeout - The idle timeout in seconds. Ports and Health Checks When both the --port and the --healthcheck-target options are omitted, Layer0 configures the load balancer with some default values: 80:80/TCP for ports and TCP:80 for healthcheck target. These default values together create a load balancer configured with a simple but functioning health check, opening up a set of ports that allows traffic to the target of the healthcheck. ( --healthcheck-target TCP:80 tells the load balancer to ping its services at port 80 to determine their status, and --port 80:80/TCP configures a security group to allow traffic to pass between port 80 of the load balancer and port 80 of its services) When creating a load balancer with non-default configurations for either --port or --healthcheck-target , make sure that a valid --port and --healthcheck-target pairing is also created.","title":"Optional arguments"},{"location":"reference/cli/#loadbalancer-delete","text":"Use the delete subcommand to delete an existing load balancer.","title":"loadbalancer delete"},{"location":"reference/cli/#usage_20","text":"l0 loadbalancer delete [--wait] loadBalancerName","title":"Usage"},{"location":"reference/cli/#required-parameters_13","text":"loadBalancerName - The name of the load balancer that you want to delete.","title":"Required parameters"},{"location":"reference/cli/#optional-arguments_4","text":"--wait - Wait until the deletion is complete before exiting.","title":"Optional arguments"},{"location":"reference/cli/#additional-information_6","text":"In order to delete a load balancer that is already attached to a service, you must first delete the service that uses the load balancer. This operation performs several tasks asynchronously. When run without the --wait option, this operation will most likely exit before all of these tasks are complete; when run with the --wait option, this operation will only exit once these tasks have completed .","title":"Additional information"},{"location":"reference/cli/#loadbalancer-addport","text":"Use the addport subcommand to add a new port configuration to an existing Layer0 load balancer.","title":"loadbalancer addport"},{"location":"reference/cli/#usage_21","text":"l0 loadbalancer addport [--certificate certificateName] loadBalancerName port","title":"Usage"},{"location":"reference/cli/#required-parameters_14","text":"loadBalancerName - The name of an existing Layer0 load balancer in which you want to add the port configuration. port - The port configuration for the listener of the load balancer. Valid pattern is hostPort:containerPort/protocol . hostPort - The port that the load balancer will listen for traffic on. containerPort - The port that the load balancer will forward traffic to. protocol - The protocol to use when forwarding traffic (acceptable values: TCP, SSL, HTTP, and HTTPS).","title":"Required parameters"},{"location":"reference/cli/#optional-arguments_5","text":"--certificate certificateName - The name of an existing Layer0 certificate. You must include this option if you are using an HTTPS port configuration.","title":"Optional arguments"},{"location":"reference/cli/#additional-information_7","text":"The port configuration you specify must not already be in use by the load balancer you specify.","title":"Additional information"},{"location":"reference/cli/#loadbalancer-dropport","text":"Use the dropport subcommand to remove a port configuration from an existing Layer0 load balancer.","title":"loadbalancer dropport"},{"location":"reference/cli/#usage_22","text":"l0 loadbalancer dropport loadBalancerName hostPort","title":"Usage"},{"location":"reference/cli/#required-parameters_15","text":"loadBalancerName - The name of an existing Layer0 load balancer from which you want to remove the port configuration. hostPort - The host port to remove from the load balancer.","title":"Required parameters"},{"location":"reference/cli/#loadbalancer-get","text":"Use the get subcommand to display information about an existing Layer0 load balancer.","title":"loadbalancer get"},{"location":"reference/cli/#usage_23","text":"l0 loadbalancer get [environmentName:]loadBalancerName","title":"Usage"},{"location":"reference/cli/#required-parameters_16","text":"[environmentName:]loadBalancerName - The name of an existing Layer0 load balancer. You can optionally provide the Layer0 environment ( environmentName ) associated with the Load Balancer","title":"Required parameters"},{"location":"reference/cli/#additional-information_8","text":"The get subcommand supports wildcard matching: l0 loadbalancer get entrypoint* would return all jobs beginning with entrypoint .","title":"Additional information"},{"location":"reference/cli/#loadbalancer-list","text":"Use the list subcommand to display information about all of the existing load balancers in an instance of Layer0.","title":"loadbalancer list"},{"location":"reference/cli/#usage_24","text":"l0 loadbalancer list","title":"Usage"},{"location":"reference/cli/#loadbalancer-healthcheck","text":"Use the healthcheck subcommand to display information about or update the configuration of a load balancer's health check.","title":"loadbalancer healthcheck"},{"location":"reference/cli/#usage_25","text":"l0 loadbalancer healthcheck [--set-target target | --set-interval interval | --set-timeout timeout | --set-healthy-threshold healthyThreshold | --set-unhealthy-threshold unhealthyThreshold] loadbalancerName","title":"Usage"},{"location":"reference/cli/#required-parameters_17","text":"loadBalancerName - The name of the existing Layer0 load balancer you are modifying.","title":"Required parameters"},{"location":"reference/cli/#optional-arguments_6","text":"--set-target target - The target of the check. Valid pattern is PROTOCOL:PORT/PATH . If PROTOCOL is HTTP or HTTPS , both PORT and PATH are required. Example: HTTP:80/admin/healthcheck . If PROTOCOL is TCP or SSL , PORT is required and PATH is not used. Example: TCP:80 --set-interval interval - The interval between health checks. --set-timeout timeout - The length of time in seconds before the health check times out. --set-healthy-threshold healthyThreshold - The number of checks before the instance is declared healthy. --set-unhealthy-threshold unhealthyThreshold - The number of checks before the instance is declared unhealthy.","title":"Optional arguments"},{"location":"reference/cli/#additional-information_9","text":"Calling the subcommand without flags will display the current configuration of the load balancer's health check. Setting any of the flags will update the corresponding field in the health check, and all omitted flags will leave the corresponding fields unchanged.","title":"Additional information"},{"location":"reference/cli/#loadbalancer-idletimeout","text":"Use the idletimeout subcommand to update the idle timeout of a load balancer. The load balancer manages an idle timeout that is triggered when no data is sent over a connection for the specified time period. If no data has been sent or received by the time that the idle timeout period elapses, the load balancer closes the connection. See the following documentation for more information: https://docs.aws.amazon.com/elasticloadbalancing/latest/application/application-load-balancers.html#connection-idle-timeout","title":"loadbalancer idletimeout"},{"location":"reference/cli/#usage_26","text":"l0 loadbalancer idletimeout loadbalancerName idleTimeout","title":"Usage"},{"location":"reference/cli/#required-parameters_18","text":"loadBalancerName - The name of the existing Layer0 load balancer you are modifying. idleTimeout - The idle timeout in seconds.","title":"Required parameters"},{"location":"reference/cli/#service","text":"A service is a component of a Layer0 environment. The purpose of a service is to execute a Docker image specified in a deploy . In order to create a service, you must first create an environment and a deploy ; in most cases, you should also create a load balancer before creating the service. The service command is used with the following subcommands: create , delete , get , update , list , logs , and scale .","title":"Service"},{"location":"reference/cli/#service-create","text":"Use the create subcommand to create a Layer0 service.","title":"service create"},{"location":"reference/cli/#usage_27","text":"l0 service create [--loadbalancer [environmentName:]loadBalancerName | --no-logs] environmentName serviceName deployName[:deployVersion]","title":"Usage"},{"location":"reference/cli/#required-parameters_19","text":"serviceName - A name for the service that you are creating. environmentName - The name of an existing Layer0 environment. deployName[:deployVersion] - The name of a Layer0 deploy that exists in the environment environmentName . You can optionally specify the version number of the Layer0 deploy that you want to deploy. If you do not specify a version number, the latest version of the deploy will be used.","title":"Required parameters"},{"location":"reference/cli/#optional-arguments_7","text":"--loadbalancer [environmentName:]loadBalancerName - Place the new service behind an existing load balancer loadBalancerName . You can optionally specify the Layer0 environment ( environmentName ) where the load balancer exists. --no-logs - Disable cloudwatch logging for the service","title":"Optional arguments"},{"location":"reference/cli/#service-update","text":"Use the update subcommand to apply an existing Layer0 Deploy to an existing Layer0 service.","title":"service update"},{"location":"reference/cli/#usage_28","text":"l0 service update [--no-logs] [environmentName:]serviceName deployName[:deployVersion]","title":"Usage"},{"location":"reference/cli/#required-parameters_20","text":"[environmentName:]serviceName - The name of an existing Layer0 service into which you want to apply the deploy. You can optionally specify the Layer0 environment ( environmentName ) of the service. deployName[:deployVersion] - The name of the Layer0 deploy that you want to apply to the service. You can optionally specify a specific version of the deploy ( deployVersion ). If you do not specify a version number, the latest version of the deploy will be applied.","title":"Required parameters"},{"location":"reference/cli/#optional-arguments_8","text":"--no-logs - Disable cloudwatch logging for the service","title":"Optional arguments"},{"location":"reference/cli/#additional-information_10","text":"If your service uses a load balancer, when you update the task definition for the service, the container name and container port that were specified when the service was created must remain the same in the task definition. In other words, if your service has a load balancer, you cannot apply any deploy you want to that service. If you are varying the container name or exposed ports, you must create a new service instead.","title":"Additional information"},{"location":"reference/cli/#service-delete","text":"Use the delete subcommand to delete an existing Layer0 service.","title":"service delete"},{"location":"reference/cli/#usage_29","text":"l0 service delete [--wait] [environmentName:]serviceName","title":"Usage"},{"location":"reference/cli/#required-parameters_21","text":"[environmentName:]serviceName - The name of the Layer0 service that you want to delete. You can optionally provide the Layer0 environment ( environmentName ) of the service.","title":"Required parameters"},{"location":"reference/cli/#optional-arguments_9","text":"--wait - Wait until the deletion is complete before exiting.","title":"Optional arguments"},{"location":"reference/cli/#additional-information_11","text":"This operation performs several tasks asynchronously. When run without the --wait option, this operation will most likely exit before all of these tasks are complete; when run with the --wait option, this operation will only exit once these tasks have completed.","title":"Additional information"},{"location":"reference/cli/#service-get","text":"Use the get subcommand to display information about an existing Layer0 service.","title":"service get"},{"location":"reference/cli/#usage_30","text":"l0 service get [environmentName:]serviceName","title":"Usage"},{"location":"reference/cli/#required-parameters_22","text":"[environmentName:]serviceName - The name of an existing Layer0 service. You can optionally provide the Layer0 environment ( environmentName ) of the service.","title":"Required parameters"},{"location":"reference/cli/#service-list","text":"Use the list subcommand to list all of the existing services in your Layer0 instance.","title":"service list"},{"location":"reference/cli/#usage_31","text":"l0 service get list","title":"Usage"},{"location":"reference/cli/#service-logs","text":"Use the logs subcommand to display the logs from a Layer0 service that is currently running.","title":"service logs"},{"location":"reference/cli/#usage_32","text":"l0 service logs [--start MM/DD HH:MM | --end MM/DD HH:MM | --tail=N] serviceName","title":"Usage"},{"location":"reference/cli/#required-parameters_23","text":"serviceName - The name of the Layer0 service for which you want to view logs.","title":"Required parameters"},{"location":"reference/cli/#optional-arguments_10","text":"--start MM/DD HH:MM - The start of the time range to fetch logs. --end MM/DD HH:MM - The end of the time range to fetch logs. --tail=N - Display only the last N lines of the log.","title":"Optional arguments"},{"location":"reference/cli/#service-scale","text":"Use the scale subcommand to specify how many copies of an existing Layer0 service should run.","title":"service scale"},{"location":"reference/cli/#usage_33","text":"l0 service scale [environmentName:]serviceName copies","title":"Usage"},{"location":"reference/cli/#required-parameters_24","text":"[environmentName:]serviceName - The name of the Layer0 service that you want to scale up. You can optionally provide the Layer0 environment ( environmentName ) of the service. copies - The number of copies of the specified service that should be run.","title":"Required parameters"},{"location":"reference/cli/#task","text":"A Layer0 task is a component of an environment. A task executes the contents of a Docker image, as specified in a deploy. A task differs from a service in that a task does not restart after exiting. Additionally, ports are not exposed when using a task. The task command is used with the following subcommands: create , delete , get , list , and logs .","title":"Task"},{"location":"reference/cli/#task-create","text":"Use the create subcommand to create a Layer0 task.","title":"task create"},{"location":"reference/cli/#usage_34","text":"l0 task create [--copies copies | --no-logs] environmentName taskName deployName","title":"Usage"},{"location":"reference/cli/#required-parameters_25","text":"environmentName - The name of the existing Layer0 environment in which you want to create the task. taskName - A name for the task. deployName - The name of an existing Layer0 deploy that the task should use.","title":"Required parameters"},{"location":"reference/cli/#optional-arguments_11","text":"--copies copies - The number of copies of the task to run (default: 1). --no-logs - Disable cloudwatch logging for the service.","title":"Optional arguments"},{"location":"reference/cli/#task-delete","text":"Use the delete subcommand to delete an existing Layer0 task.","title":"task delete"},{"location":"reference/cli/#usage_35","text":"l0 task delete [environmentName:]taskName","title":"Usage"},{"location":"reference/cli/#required-parameters_26","text":"[environmentName:]taskName - The name of the Layer0 task that you want to delete. You can optionally specify the name of the Layer0 environment that contains the task. This parameter is only required if mulitiple environments contain tasks with exactly the same name.","title":"Required parameters"},{"location":"reference/cli/#additional-information_12","text":"Until the record has been purged, the API may indicate that the task is still running. Task records are typically purged within an hour.","title":"Additional information"},{"location":"reference/cli/#task-get","text":"Use the get subcommand to display information about an existing Layer0 task ( taskName ).","title":"task get"},{"location":"reference/cli/#usage_36","text":"l0 task get [environmentName:]taskName","title":"Usage"},{"location":"reference/cli/#required-parameters_27","text":"[environmentName:]taskName - The name of a Layer0 task for which you want to see information. You can optionally specify the name of the Layer0 Environment that contains the task.","title":"Required parameters"},{"location":"reference/cli/#additional-information_13","text":"The value of taskName does not need to exactly match the name of an existing task. If multiple results are found that match the pattern you specified in taskName , then information about all matching tasks will be returned.","title":"Additional information"},{"location":"reference/cli/#task-list","text":"Use the task subcommand to display a list of running tasks in your Layer0.","title":"task list"},{"location":"reference/cli/#usage_37","text":"l0 task list","title":"Usage"},{"location":"reference/cli/#task-logs","text":"Use the logs subcommand to display logs for a running Layer0 task.","title":"task logs"},{"location":"reference/cli/#usage_38","text":"l0 task logs [--start MM/DD HH:MM | --end MM/DD HH:MM | --tail=N] taskName","title":"Usage"},{"location":"reference/cli/#required-parameters_28","text":"taskName - The name of an existing Layer0 task.","title":"Required parameters"},{"location":"reference/cli/#optional-arguments_12","text":"--start MM/DD HH:MM - The start of the time range to fetch logs. --end MM/DD HH:MM - The end of the time range to fetch logs. --tail=N - Display only the last N lines of the log.","title":"Optional arguments"},{"location":"reference/cli/#additional-information_14","text":"The value of taskName does not need to exactly match the name of an existing task. If multiple results are found that match the pattern you specified in taskName , then information about all matching tasks will be returned.","title":"Additional information"},{"location":"reference/cli/#task-list_1","text":"Use the list subcommand to display a list of running tasks in your Layer0.","title":"task list"},{"location":"reference/cli/#usage_39","text":"l0 task list","title":"Usage"},{"location":"reference/consul/","text":"Consul reference # Consul is an open-source tool for discovering and configuring services in your network architecture. Specifically, Consul provides the following features: Discovery of services Monitoring of the health of services Key/value storage with a simple HTTP API Consul Agent # The Consul Agent exposes a DNS API for easy consumption of data generated by Registrator . The Consul Agent can run either in server or client mode. When run as a Layer0 Service, the Consul Agent runs in server mode. To ensure the integrity of your data, the service in which you run consul should be scaled to size 3 or greater. A group of several consul deployments is known as a \" cluster .\" Other Layer0 Services that use Consul will run the Consul Agent in client mode, alongside their application containers. The client is a very lightweight process that registers services, runs health checks, and forwards queries to servers. Registrator # Registrator is a tool that automatically registers and deregisters services into a Consul Cluster by inspecting Docker containers as they come online. Container registration is based off of environment variables on the container. Layer0 Services that use Consul will run Registrator alongside their application containers. Service Configuration # Layer0 Services that use Consul will need to add the Registrator and Consul Agent definitions to the containerDefinitions section of your Deploys. You must also add the Docker Socket definition to the volumes section of your Deploys. Registrator Container Definition # { \"name\": \"registrator\", \"image\": \"gliderlabs/registrator:master\", \"essential\": true, \"links\": [\"consul-agent\"], \"entrypoint\": [\"/bin/sh\", \"-c\"], \"command\": [\"/bin/registrator -retry-attempts=-1 -retry-interval=30000 -ip $(wget http://169.254.169.254/latest/meta-data/local-ipv4 -q -O -) consul://consul-agent:8500\"], \"memory\": 128, \"mountPoints\": [ { \"sourceVolume\": \"dockersocket\", \"containerPath\": \"/tmp/docker.sock\" } ] }, Consul Agent Container Definition # Warning You must replace <url> with your Layer0 Consul Load Balancer's URL. { \"name\": \"consul-agent\", \"image\": \"progrium/consul\", \"essential\": true, \"entrypoint\": [\"/bin/bash\", \"-c\"], \"command\": [\"/bin/start -advertise $(wget http://169.254.169.254/latest/meta-data/local-ipv4 -q -O -) -retry-join $EXTERNAL_URL -recursor $UPSTREAM_DNS -retry-interval 30s\"], \"memory\": 128, \"portMappings\": [ { \"hostPort\": 8500, \"containerPort\": 8500 }, { \"hostPort\": 53, \"containerPort\": 53, \"protocol\": \"udp\" } ], \"environment\": [ { \"name\": \"EXTERNAL_URL\", \"value\": \"<url>\" }, { \"name\": \"UPSTREAM_DNS\", \"value\": \"10.100.0.2\" } ] }, Environment Variables # EXTERNAL_URL - URL of the consul cluster UPSTREAM_DNS - The DNS server consul-agent queries for DNS entries that it cannot resolve internally (e.g. google.com) The default value for UPSTREAM_DNS assumes you're using the default Layer0 configuration, making your internal DNS endpoint 10.100.0.2. If you are a using a non standard configuration (e.g. installing Layer0 in an existing VPC with a CIDR other than 10.100.0.0/16 ) please modify this variable accordingly. Docker Socket Volume Definition # \"volumes\": [ { \"name\": \"dockersocket\", \"host\": { \"sourcePath\": \"/var/run/docker.sock\" } } ],","title":"Consul"},{"location":"reference/consul/#consul-reference","text":"Consul is an open-source tool for discovering and configuring services in your network architecture. Specifically, Consul provides the following features: Discovery of services Monitoring of the health of services Key/value storage with a simple HTTP API","title":"Consul reference"},{"location":"reference/consul/#consul-agent","text":"The Consul Agent exposes a DNS API for easy consumption of data generated by Registrator . The Consul Agent can run either in server or client mode. When run as a Layer0 Service, the Consul Agent runs in server mode. To ensure the integrity of your data, the service in which you run consul should be scaled to size 3 or greater. A group of several consul deployments is known as a \" cluster .\" Other Layer0 Services that use Consul will run the Consul Agent in client mode, alongside their application containers. The client is a very lightweight process that registers services, runs health checks, and forwards queries to servers.","title":"Consul Agent"},{"location":"reference/consul/#registrator","text":"Registrator is a tool that automatically registers and deregisters services into a Consul Cluster by inspecting Docker containers as they come online. Container registration is based off of environment variables on the container. Layer0 Services that use Consul will run Registrator alongside their application containers.","title":"Registrator"},{"location":"reference/consul/#service-configuration","text":"Layer0 Services that use Consul will need to add the Registrator and Consul Agent definitions to the containerDefinitions section of your Deploys. You must also add the Docker Socket definition to the volumes section of your Deploys.","title":"Service Configuration"},{"location":"reference/consul/#registrator-container-definition","text":"{ \"name\": \"registrator\", \"image\": \"gliderlabs/registrator:master\", \"essential\": true, \"links\": [\"consul-agent\"], \"entrypoint\": [\"/bin/sh\", \"-c\"], \"command\": [\"/bin/registrator -retry-attempts=-1 -retry-interval=30000 -ip $(wget http://169.254.169.254/latest/meta-data/local-ipv4 -q -O -) consul://consul-agent:8500\"], \"memory\": 128, \"mountPoints\": [ { \"sourceVolume\": \"dockersocket\", \"containerPath\": \"/tmp/docker.sock\" } ] },","title":"Registrator Container Definition"},{"location":"reference/consul/#consul-agent-container-definition","text":"Warning You must replace <url> with your Layer0 Consul Load Balancer's URL. { \"name\": \"consul-agent\", \"image\": \"progrium/consul\", \"essential\": true, \"entrypoint\": [\"/bin/bash\", \"-c\"], \"command\": [\"/bin/start -advertise $(wget http://169.254.169.254/latest/meta-data/local-ipv4 -q -O -) -retry-join $EXTERNAL_URL -recursor $UPSTREAM_DNS -retry-interval 30s\"], \"memory\": 128, \"portMappings\": [ { \"hostPort\": 8500, \"containerPort\": 8500 }, { \"hostPort\": 53, \"containerPort\": 53, \"protocol\": \"udp\" } ], \"environment\": [ { \"name\": \"EXTERNAL_URL\", \"value\": \"<url>\" }, { \"name\": \"UPSTREAM_DNS\", \"value\": \"10.100.0.2\" } ] },","title":"Consul Agent Container Definition"},{"location":"reference/consul/#environment-variables","text":"EXTERNAL_URL - URL of the consul cluster UPSTREAM_DNS - The DNS server consul-agent queries for DNS entries that it cannot resolve internally (e.g. google.com) The default value for UPSTREAM_DNS assumes you're using the default Layer0 configuration, making your internal DNS endpoint 10.100.0.2. If you are a using a non standard configuration (e.g. installing Layer0 in an existing VPC with a CIDR other than 10.100.0.0/16 ) please modify this variable accordingly.","title":"Environment Variables"},{"location":"reference/consul/#docker-socket-volume-definition","text":"\"volumes\": [ { \"name\": \"dockersocket\", \"host\": { \"sourcePath\": \"/var/run/docker.sock\" } } ],","title":"Docker Socket Volume Definition"},{"location":"reference/ecr/","text":"EC2 Container Registry # ECR is an Amazon implementation of a docker registry. It acts as a private registry in your AWS account, which can be accessed from any docker client, and Layer0. Consider using ECR if you have stability issues with hosted docker registries, and do not wish to share your images publicly on dockerhub . Setup # When interacting with ECR, you will first need to create a repository and a login to interact from your development machine. Repository # Each repository needs to be created by an AWS api call. > aws ecr create-repository --repository-name myteam/myproject Login # To authenticate with the ECR service, Amazon provides the get-login command, which generates an authentication token, and returns a docker command to set it up > aws ecr get-login # this command will return the following: (password is typically hundreds of characters) docker login -u AWS -p password -e none https://aws_account_id.dkr.ecr.us-east-1.amazonaws.com Execute the provided docker command to store the login credentials Afterward creating the repository and local login credentials you may interact with images (and tags) under this path from a local docker client. docker pull ${ ecr - url } /myteam/myproject docker push ${ ecr - url } /myteam/myproject:custom-tag-1 Deploy Example # Here we'll walk through using ECR when deploying to Layer0, Using a very basic wait container. Make docker image # Your docker image can be built locally or pulled from dockerhub. For this example, we made a service that waits and then exits (useful for triggering regular restarts). FROM busybox ENV SLEEP_TIME=60 CMD sleep $SLEEP_TIME Then build the file, with the tag xfra/wait > docker build -f Dockerfile.wait -t xfra/wait . Upload to ECR # After preparing a login and registry, tag the image with the remote url, and use docker push docker tag xfra/wait 111222333444.dkr.ecr.us-east-1.amazonaws.com/xfra-wait docker push 111222333444.dkr.ecr.us-east-1.amazonaws.com/xfra-wait Note: your account id in this url will be different. Create a deploy # To run this image in Layer0, we create a dockerrun file, describing the instance and any additional variables { \"containerDefinitions\": [ { \"name\": \"timeout\", \"image\": \"111222333444.dkr.ecr.us-east-1.amazonaws.com/xfra-wait:latest\", \"essential\": true, \"memory\": 10, \"environment\": [ { \"name\": \"SLEEP_TIME\", \"value\": \"43200\" } ] } ] } And create that in Layer0 l0 deploy create timeout.dockerrun.aws.json timeout Deploy # Finally, run that deploy as a service or a task. (the service will restart every 12 hours) l0 service create demo timeoutsvc timeout:latest References # ECR User Guide create-repository get-login","title":"ECR"},{"location":"reference/ecr/#ec2-container-registry","text":"ECR is an Amazon implementation of a docker registry. It acts as a private registry in your AWS account, which can be accessed from any docker client, and Layer0. Consider using ECR if you have stability issues with hosted docker registries, and do not wish to share your images publicly on dockerhub .","title":"EC2 Container Registry"},{"location":"reference/ecr/#setup","text":"When interacting with ECR, you will first need to create a repository and a login to interact from your development machine.","title":"Setup"},{"location":"reference/ecr/#repository","text":"Each repository needs to be created by an AWS api call. > aws ecr create-repository --repository-name myteam/myproject","title":"Repository"},{"location":"reference/ecr/#login","text":"To authenticate with the ECR service, Amazon provides the get-login command, which generates an authentication token, and returns a docker command to set it up > aws ecr get-login # this command will return the following: (password is typically hundreds of characters) docker login -u AWS -p password -e none https://aws_account_id.dkr.ecr.us-east-1.amazonaws.com Execute the provided docker command to store the login credentials Afterward creating the repository and local login credentials you may interact with images (and tags) under this path from a local docker client. docker pull ${ ecr - url } /myteam/myproject docker push ${ ecr - url } /myteam/myproject:custom-tag-1","title":"Login"},{"location":"reference/ecr/#deploy-example","text":"Here we'll walk through using ECR when deploying to Layer0, Using a very basic wait container.","title":"Deploy Example"},{"location":"reference/ecr/#make-docker-image","text":"Your docker image can be built locally or pulled from dockerhub. For this example, we made a service that waits and then exits (useful for triggering regular restarts). FROM busybox ENV SLEEP_TIME=60 CMD sleep $SLEEP_TIME Then build the file, with the tag xfra/wait > docker build -f Dockerfile.wait -t xfra/wait .","title":"Make docker image"},{"location":"reference/ecr/#upload-to-ecr","text":"After preparing a login and registry, tag the image with the remote url, and use docker push docker tag xfra/wait 111222333444.dkr.ecr.us-east-1.amazonaws.com/xfra-wait docker push 111222333444.dkr.ecr.us-east-1.amazonaws.com/xfra-wait Note: your account id in this url will be different.","title":"Upload to ECR"},{"location":"reference/ecr/#create-a-deploy","text":"To run this image in Layer0, we create a dockerrun file, describing the instance and any additional variables { \"containerDefinitions\": [ { \"name\": \"timeout\", \"image\": \"111222333444.dkr.ecr.us-east-1.amazonaws.com/xfra-wait:latest\", \"essential\": true, \"memory\": 10, \"environment\": [ { \"name\": \"SLEEP_TIME\", \"value\": \"43200\" } ] } ] } And create that in Layer0 l0 deploy create timeout.dockerrun.aws.json timeout","title":"Create a deploy"},{"location":"reference/ecr/#deploy","text":"Finally, run that deploy as a service or a task. (the service will restart every 12 hours) l0 service create demo timeoutsvc timeout:latest","title":"Deploy"},{"location":"reference/ecr/#references","text":"ECR User Guide create-repository get-login","title":"References"},{"location":"reference/setup-cli/","text":"Layer0 Setup Reference # The Layer0 Setup application (commonly called l0-setup ), is used for administrative tasks on Layer0 instances. Global options # l0-setup can be used with one of several commands: init , plan , apply , list , push , pull , endpoint , destroy , upgrade , and set . These commands are detailed in teh sections below. There are, however, some global paramters that you may specify whenever using l0-setup Usage # l0 - setup [ global options ] command [ command options ] params Global options # -l value, --log value - The log level to display on the console when you run commands. (default: info) --version - Display the version number of the l0-setup application. Init # The init command is used to initialize or reconfigure a Layer0 instance. This command will prompt the user for inputs required to create/update a Layer0 instance. Each of the inputs can be specified through an optional flag. Usage # l0-setup init [--docker-path path | --module-source path | --version version | --aws-region region | --aws-access-key accessKey | --aws-secret-key secretKey] instanceName Optional arguments # --docker-path - Path to docker config.json file. This is used to include private Docker Registry authentication for this Layer0 instance. --module-source - The source input variable is the path to the Terraform Layer0. By default, this points to the Layer0 github repository. Using values other than the default may result in undesired consequences. --version - The version input variable specifies the tag to use for the Layer0 Docker images: quintilesims/l0-api and quintilesims/l0-runner . --aws-ssh-key-pair - The ssh_key_pair input variable specifies the name of the ssh key pair to include in EC2 instances provisioned by Layer0. This key pair must already exist in the AWS account. The names of existing key pairs can be found in the EC2 dashboard. --aws-access-key - The access_key input variable is used to provision the AWS resources required for Layer0. This corresponds to the Access Key ID portion of an AWS Access Key. It is recommended this key has the AdministratorAccess policy. --aws-secret-key - The secret_key input variable is used to provision the AWS resources required for Layer0. This corresponds to the Secret Access Key portion of an AWS Access Key. It is recommended this key has the AdministratorAccess policy. Plan # The plan command is used to show the planned operation(s) to run during the next apply on a Layer0 instance without actually executing any actions Usage # l0-setup plan instanceName Apply # The apply command is used to create and update Layer0 instances. Note that the default behavior of apply is to push the layer0 configuration to an S3 bucket unless the --push=false flag is set to false. Pushing the configuration to an S3 bucket requires aws credentials which if not set via the optional --aws-* flags, are read from the environment variables or a credentials file. Usage # l0-setup apply [--quick | --push=false | --aws-access-key accessKey | --aws-secret-key secretKey] instanceName Optional arguments # --quick - Skips verification checks that normally run after terraform apply has completed --push=false - Skips uploading local Layer0 configuration files to an S3 bucket --aws-access-key - The access_key input variable is used to provision the AWS resources required for Layer0. This corresponds to the Access Key ID portion of an AWS Access Key. It is recommended this key has the AdministratorAccess policy. --aws-secret-key - The secret_key input variable is used to provision the AWS resources required for Layer0. This corresponds to the Secret Access Key portion of an AWS Access Key. It is recommended this key has the AdministratorAccess policy. List # The list command is used to list local and remote Layer0 instances. Usage # l0-setup list [--local=false | --remote=false | --aws-access-key accessKey | --aws-secret-key secretKey] Optional arguments # -l, --local - Show local Layer0 instances. This value is true by default. -r, --remote - Show remote Layer0 instances. This value is true by default. Push # The push command is used to back up your Layer0 configuration files to an S3 bucket. Usage # l0-setup push [--aws-access-key accessKey | --aws-secret-key secretKey] instanceName Optional arguments # --aws-access-key - The access_key input variable is used to provision the AWS resources required for Layer0. This corresponds to the Access Key ID portion of an AWS Access Key. It is recommended this key has the AdministratorAccess policy. --aws-secret-key - The secret_key input variable is used to provision the AWS resources required for Layer0. This corresponds to the Secret Access Key portion of an AWS Access Key. It is recommended this key has the AdministratorAccess policy. Pull # The pull command is used copy Layer0 configuration files from an S3 bucket. Usage # l0-setup pull [--aws-access-key accessKey | --aws-secret-key secretKey] instanceName Optional arguments # --aws-access-key - The access_key input variable is used to provision the AWS resources required for Layer0. This corresponds to the Access Key ID portion of an AWS Access Key. It is recommended this key has the AdministratorAccess policy. --aws-secret-key - The secret_key input variable is used to provision the AWS resources required for Layer0. This corresponds to the Secret Access Key portion of an AWS Access Key. It is recommended this key has the AdministratorAccess policy. Endpoint # The endpoint command is used to show environment variables used to connect to a Layer0 instance Usage # l0-setup endpoint [-i | -d | -s syntax] instanceName Optional arguments # -i, --insecure - Show environment variables that allow for insecure settings -d, --dev - Show environment variables that are required for local development -s --syntax - Choose the syntax to display environment variables (choices: bash , cmd , powershell ) (default: bash ) Destroy # The destroy command is used to destroy all resources associated with a Layer0 instance. Caution Destroying a Layer0 instance cannot be undone. If you created backups of your Layer0 configuration using the push command, those backups will also be deleted when you run the destroy command. Usage # l0-setup destroy [--force] instanceName Optional arguments # --force - Skips confirmation prompt Upgrade # The upgrade command is used to upgrade a Layer0 instance to a new version. You will need to run an apply after this command has completed. Usage # l0-setup upgrade [--force] instanceName version Optional arguments # --force - Skips confirmation prompt Set # The set command is used set input variable(s) for a Layer0 instance's Terraform module. This command can be used to shorthand the init and upgrade commands, and can also be used with custom Layer0 modules. You will need to run an apply after this command has completed. Usage # l0-setup set [--input key=value] instanceName Options # --input key=val - Specify an input using key=val format Example Usage # l0-setup set --input username=admin --input password=pass123 mylayer0","title":"Layer0 Setup CLI"},{"location":"reference/setup-cli/#layer0-setup-reference","text":"The Layer0 Setup application (commonly called l0-setup ), is used for administrative tasks on Layer0 instances.","title":"Layer0 Setup Reference"},{"location":"reference/setup-cli/#global-options","text":"l0-setup can be used with one of several commands: init , plan , apply , list , push , pull , endpoint , destroy , upgrade , and set . These commands are detailed in teh sections below. There are, however, some global paramters that you may specify whenever using l0-setup","title":"Global options"},{"location":"reference/setup-cli/#usage","text":"l0 - setup [ global options ] command [ command options ] params","title":"Usage"},{"location":"reference/setup-cli/#global-options_1","text":"-l value, --log value - The log level to display on the console when you run commands. (default: info) --version - Display the version number of the l0-setup application.","title":"Global options"},{"location":"reference/setup-cli/#init","text":"The init command is used to initialize or reconfigure a Layer0 instance. This command will prompt the user for inputs required to create/update a Layer0 instance. Each of the inputs can be specified through an optional flag.","title":"Init"},{"location":"reference/setup-cli/#usage_1","text":"l0-setup init [--docker-path path | --module-source path | --version version | --aws-region region | --aws-access-key accessKey | --aws-secret-key secretKey] instanceName","title":"Usage"},{"location":"reference/setup-cli/#optional-arguments","text":"--docker-path - Path to docker config.json file. This is used to include private Docker Registry authentication for this Layer0 instance. --module-source - The source input variable is the path to the Terraform Layer0. By default, this points to the Layer0 github repository. Using values other than the default may result in undesired consequences. --version - The version input variable specifies the tag to use for the Layer0 Docker images: quintilesims/l0-api and quintilesims/l0-runner . --aws-ssh-key-pair - The ssh_key_pair input variable specifies the name of the ssh key pair to include in EC2 instances provisioned by Layer0. This key pair must already exist in the AWS account. The names of existing key pairs can be found in the EC2 dashboard. --aws-access-key - The access_key input variable is used to provision the AWS resources required for Layer0. This corresponds to the Access Key ID portion of an AWS Access Key. It is recommended this key has the AdministratorAccess policy. --aws-secret-key - The secret_key input variable is used to provision the AWS resources required for Layer0. This corresponds to the Secret Access Key portion of an AWS Access Key. It is recommended this key has the AdministratorAccess policy.","title":"Optional arguments"},{"location":"reference/setup-cli/#plan","text":"The plan command is used to show the planned operation(s) to run during the next apply on a Layer0 instance without actually executing any actions","title":"Plan"},{"location":"reference/setup-cli/#usage_2","text":"l0-setup plan instanceName","title":"Usage"},{"location":"reference/setup-cli/#apply","text":"The apply command is used to create and update Layer0 instances. Note that the default behavior of apply is to push the layer0 configuration to an S3 bucket unless the --push=false flag is set to false. Pushing the configuration to an S3 bucket requires aws credentials which if not set via the optional --aws-* flags, are read from the environment variables or a credentials file.","title":"Apply"},{"location":"reference/setup-cli/#usage_3","text":"l0-setup apply [--quick | --push=false | --aws-access-key accessKey | --aws-secret-key secretKey] instanceName","title":"Usage"},{"location":"reference/setup-cli/#optional-arguments_1","text":"--quick - Skips verification checks that normally run after terraform apply has completed --push=false - Skips uploading local Layer0 configuration files to an S3 bucket --aws-access-key - The access_key input variable is used to provision the AWS resources required for Layer0. This corresponds to the Access Key ID portion of an AWS Access Key. It is recommended this key has the AdministratorAccess policy. --aws-secret-key - The secret_key input variable is used to provision the AWS resources required for Layer0. This corresponds to the Secret Access Key portion of an AWS Access Key. It is recommended this key has the AdministratorAccess policy.","title":"Optional arguments"},{"location":"reference/setup-cli/#list","text":"The list command is used to list local and remote Layer0 instances.","title":"List"},{"location":"reference/setup-cli/#usage_4","text":"l0-setup list [--local=false | --remote=false | --aws-access-key accessKey | --aws-secret-key secretKey]","title":"Usage"},{"location":"reference/setup-cli/#optional-arguments_2","text":"-l, --local - Show local Layer0 instances. This value is true by default. -r, --remote - Show remote Layer0 instances. This value is true by default.","title":"Optional arguments"},{"location":"reference/setup-cli/#push","text":"The push command is used to back up your Layer0 configuration files to an S3 bucket.","title":"Push"},{"location":"reference/setup-cli/#usage_5","text":"l0-setup push [--aws-access-key accessKey | --aws-secret-key secretKey] instanceName","title":"Usage"},{"location":"reference/setup-cli/#optional-arguments_3","text":"--aws-access-key - The access_key input variable is used to provision the AWS resources required for Layer0. This corresponds to the Access Key ID portion of an AWS Access Key. It is recommended this key has the AdministratorAccess policy. --aws-secret-key - The secret_key input variable is used to provision the AWS resources required for Layer0. This corresponds to the Secret Access Key portion of an AWS Access Key. It is recommended this key has the AdministratorAccess policy.","title":"Optional arguments"},{"location":"reference/setup-cli/#pull","text":"The pull command is used copy Layer0 configuration files from an S3 bucket.","title":"Pull"},{"location":"reference/setup-cli/#usage_6","text":"l0-setup pull [--aws-access-key accessKey | --aws-secret-key secretKey] instanceName","title":"Usage"},{"location":"reference/setup-cli/#optional-arguments_4","text":"--aws-access-key - The access_key input variable is used to provision the AWS resources required for Layer0. This corresponds to the Access Key ID portion of an AWS Access Key. It is recommended this key has the AdministratorAccess policy. --aws-secret-key - The secret_key input variable is used to provision the AWS resources required for Layer0. This corresponds to the Secret Access Key portion of an AWS Access Key. It is recommended this key has the AdministratorAccess policy.","title":"Optional arguments"},{"location":"reference/setup-cli/#endpoint","text":"The endpoint command is used to show environment variables used to connect to a Layer0 instance","title":"Endpoint"},{"location":"reference/setup-cli/#usage_7","text":"l0-setup endpoint [-i | -d | -s syntax] instanceName","title":"Usage"},{"location":"reference/setup-cli/#optional-arguments_5","text":"-i, --insecure - Show environment variables that allow for insecure settings -d, --dev - Show environment variables that are required for local development -s --syntax - Choose the syntax to display environment variables (choices: bash , cmd , powershell ) (default: bash )","title":"Optional arguments"},{"location":"reference/setup-cli/#destroy","text":"The destroy command is used to destroy all resources associated with a Layer0 instance. Caution Destroying a Layer0 instance cannot be undone. If you created backups of your Layer0 configuration using the push command, those backups will also be deleted when you run the destroy command.","title":"Destroy"},{"location":"reference/setup-cli/#usage_8","text":"l0-setup destroy [--force] instanceName","title":"Usage"},{"location":"reference/setup-cli/#optional-arguments_6","text":"--force - Skips confirmation prompt","title":"Optional arguments"},{"location":"reference/setup-cli/#upgrade","text":"The upgrade command is used to upgrade a Layer0 instance to a new version. You will need to run an apply after this command has completed.","title":"Upgrade"},{"location":"reference/setup-cli/#usage_9","text":"l0-setup upgrade [--force] instanceName version","title":"Usage"},{"location":"reference/setup-cli/#optional-arguments_7","text":"--force - Skips confirmation prompt","title":"Optional arguments"},{"location":"reference/setup-cli/#set","text":"The set command is used set input variable(s) for a Layer0 instance's Terraform module. This command can be used to shorthand the init and upgrade commands, and can also be used with custom Layer0 modules. You will need to run an apply after this command has completed.","title":"Set"},{"location":"reference/setup-cli/#usage_10","text":"l0-setup set [--input key=value] instanceName","title":"Usage"},{"location":"reference/setup-cli/#options","text":"--input key=val - Specify an input using key=val format","title":"Options"},{"location":"reference/setup-cli/#example-usage","text":"l0-setup set --input username=admin --input password=pass123 mylayer0","title":"Example Usage"},{"location":"reference/task_definition/","text":"Task Definitions # This guide gives some overview into the composition of a task definition. For more comprehensive documentation, we recommend taking a look at the official AWS docs: Creating a Task Definition Task Definition Parameters Sample # The following snippet contains the task definition for the Guestbook application { \"AWSEBDockerrunVersion\": 2, \"containerDefinitions\": [ { \"name\": \"guestbook\", \"image\": \"quintilesims/guestbook\", \"essential\": true, \"memory\": 128, \"portMappings\": [ { \"hostPort\": 80, \"containerPort\": 80 } ], } ] } Name The name of the container Warning If you wish to update your task definition, the container names must remain the same. If any container names are changed or removed in an updated task definition, ECS will not know how the existing container(s) should be mapped over and you will not be able to deploy the updated task definition. If you encounter a scenario where you must change or remove a container's name in a task definition, we recommend re-creating the Layer0 Deploy and Service. Image The Docker image used to build the container. The image format is url/image:tag The url specifies which Docker Repo to pull the image from If a non-Docker-Hub url is not specified, Docker Hub is used (as is the case here) The image specifies the name of the image to grab (in this case, the guestbook image from the quintilesims Docker Hub group) The tag specifies which version of image to grab If tag is not specified, :latest is used Essential If set to true , all other containers in the task definition will be stopped if that container fails or stops for any reason. Otherwise, the container's failure will not affect the rest of the containers in the task definition. Memory The number of MiB of memory to reserve for the container. If your container attempts to exceed the memory allocated here, the container is killed PortMappings A list of hostPort, containerPort mappings for the container HostPort The port number on the host instance reserved for your container. If your Layer0 Service is behind a Layer0 Load Balancer, this should map to an instancePort on the Layer0 Load Balancer. ContainerPort The port number the container should receive traffic on. Any traffic received from the instance's hostPort will be forwarded to the container on this port","title":"Task Definitions"},{"location":"reference/task_definition/#task-definitions","text":"This guide gives some overview into the composition of a task definition. For more comprehensive documentation, we recommend taking a look at the official AWS docs: Creating a Task Definition Task Definition Parameters","title":"Task Definitions"},{"location":"reference/task_definition/#sample","text":"The following snippet contains the task definition for the Guestbook application { \"AWSEBDockerrunVersion\": 2, \"containerDefinitions\": [ { \"name\": \"guestbook\", \"image\": \"quintilesims/guestbook\", \"essential\": true, \"memory\": 128, \"portMappings\": [ { \"hostPort\": 80, \"containerPort\": 80 } ], } ] } Name The name of the container Warning If you wish to update your task definition, the container names must remain the same. If any container names are changed or removed in an updated task definition, ECS will not know how the existing container(s) should be mapped over and you will not be able to deploy the updated task definition. If you encounter a scenario where you must change or remove a container's name in a task definition, we recommend re-creating the Layer0 Deploy and Service. Image The Docker image used to build the container. The image format is url/image:tag The url specifies which Docker Repo to pull the image from If a non-Docker-Hub url is not specified, Docker Hub is used (as is the case here) The image specifies the name of the image to grab (in this case, the guestbook image from the quintilesims Docker Hub group) The tag specifies which version of image to grab If tag is not specified, :latest is used Essential If set to true , all other containers in the task definition will be stopped if that container fails or stops for any reason. Otherwise, the container's failure will not affect the rest of the containers in the task definition. Memory The number of MiB of memory to reserve for the container. If your container attempts to exceed the memory allocated here, the container is killed PortMappings A list of hostPort, containerPort mappings for the container HostPort The port number on the host instance reserved for your container. If your Layer0 Service is behind a Layer0 Load Balancer, this should map to an instancePort on the Layer0 Load Balancer. ContainerPort The port number the container should receive traffic on. Any traffic received from the instance's hostPort will be forwarded to the container on this port","title":"Sample"},{"location":"reference/terraform-plugin/","text":"Layer0 Terraform Provider Reference # Terraform is an open-source tool for provisioning and managing infrastructure. If you are new to Terraform, we recommend checking out their documentation . Layer0 has built a custom provider for Layer0. This provider allows users to create, manage, and update Layer0 entities using Terraform. Prerequisites # Terraform v0.11+ ( download ), accessible in your system path. Install # Download a Layer0 v0.8.4+ release . The Terraform plugin binary is located in the release zip file as terraform-provider-layer0 . Copy this terraform-provider-layer0 binary into the same directory as your Terraform binary - and you're done! For further information, see Terraform's documentation on installing a Terraform plugin here . Getting Started # Checkout the Terraform section of the Guestbook walkthrough here . We've added some tips and links to helpful resources in the Best Practices section below. Provider # The Layer0 provider is used to interact with a Layer0 API. The provider needs to be configured with the proper credentials before it can be used. Example Usage # # Add 'endpoint' and 'token' variables variable \"endpoint\" {} variable \"token\" {} # Configure the layer0 provider provider \"layer0\" { endpoint = \" ${ var . endpoint } \" token = \" ${ var . token } \" skip_ssl_verify = true } Argument Reference # The following arguments are supported: Note The endpoint and token variables for your layer0 api can be found using the l0-setup endpoint command. endpoint - (Required) The endpoint of the layer0 api token - (Required) The authentication token for the layer0 api skip_ssl_verify - (Optional) If true, ssl certificate mismatch warnings will be ignored API Data Source # The API data source is used to extract useful read-only variables from the Layer0 API. Example Usage # # Configure the api data source data \"layer0_api\" \"config\" {} # Output the layer0 vpc id output \"vpc id\" { val = \" ${ data . layer0_api . config . vpc_id } \" } Attribute Reference # The following attributes are exported: prefix - The prefix of the layer0 instance vpc_id - The vpc id of the layer0 instance public_subnets - A list containing the 2 public subnet ids in the layer0 vpc private_subnets - A list containing the 2 private subnet ids in the layer0 vpc Deploy Data Source # The Deploy data source is used to extract Layer0 Deploy attributes. Example Usage # # Configure the deploy data source data \"layer0_deploy\" \"dpl\" { name = \"my-deploy\" version = \"1\" } # Output the layer0 deploy id output \"deploy_id\" { val = \" ${ data . layer0_deploy . dpl . id } \" } Argument Reference # The following arguments are supported: name - (Required) The name of the deploy version - (Required) The version of the deploy Attribute Reference # The following attributes are exported: name - The name of the deploy version - The version of the deploy id - The id of the deploy Environment Data Source # The Environment data source is used to extract Layer0 Environment attributes. Example Usage # # Configure the environment data source data \"layer0_environment\" \"env\" { name = \"my-environment\" } # Output the layer0 environment id output \"environment_id\" { val = \" ${ data . layer0_environment . env . id } \" } Argument Reference # The following arguments are supported: name - (Required) The name of the environment Attribute Reference # The following attributes are exported: id - The id of the environment name - The name of the environment size - The size of the instances in the environment min_count - The current number instances in the environment os - The operating system used for the environment ami - The AMI ID used for the environment Load Balancer Data Source # The Load Balancer data source is used to extract Layer0 Load Balancer attributes. Example Usage # # Configure the load balancer source data \"layer0_load_balancer\" \"lb\" { name = \"my-loadbalancer\" environment_id = \" ${ data . layer0_environment . env . environment_id } \" } # Output the layer0 load balancer id output \"load_balancer_id\" { val = \" ${ data . layer0_load_balancer . lb . id } \" } Argument Reference # The following arguments are supported: name - (required) The name of the load balancer environment_id - (required) The id of the environment the load balancer exists in Attribute Reference # The following attributes are exported: id - The id of the load balancer name - The name of the load balancer environment_id - The id of the environment the load balancer exists in environment_name - The name of the environment the load balancer exists in private - Whether or not the load balancer is private url - The URL of the load balancer Service Data Source # The Service data source is used to extract Layer0 Service attributes. Example Usage # # Configure the service data source data \"layer0_service\" \"svc\" { name = \"my-service\" environment_id = \" ${ data . layer0_environment . env . environment_id } \" } # Output the layer0 service id output \"service_id\" { val = \" ${ data . layer0_service . svc . id } \" } Argument Reference # The following arguments are supported: name - (required) The name of the service environment_id - (required) The id of the environment the service exists in Attribute Reference # The following attributes are exported: id - The id of the service name - The name of the service environment_id - The id of the environment the service exists in environment_name - The name of the environment the service exists in scale - The current desired scale of the service Deploy Resource # Provides a Layer0 Deploy. Performing variable substitution inside of your deploy's json file (typically named Dockerrun.aws.json ) can be done through Terraform's template_file . For a working example, please see the sample Guestbook application Example Usage # # Configure the deploy template data \"template_file\" \"guestbook\" { template = \" ${ file ( \"Dockerrun.aws.json\" ) } \" vars { docker_image_tag = \"latest\" } } # Create a deploy using the rendered template resource \"layer0_deploy\" \"guestbook\" { name = \"guestbook\" content = \" ${ data . template_file . guestbook . rendered } \" } Argument Reference # The following arguments are supported: name - (Required) The name of the deploy content - (Required) The content of the deploy Attribute Reference # The following attributes are exported: id - The id of the deploy name - The name of the deploy version - The version number of the deploy Environment Resource # Provides a Layer0 Environment Example Usage # # Create a new environment resource \"layer0_environment\" \"demo\" { name = \"demo\" size = \"m3.medium\" min_count = 0 user_data = \"echo hello, world\" os = \"linux\" ami = \"ami123\" } Argument Reference # The following arguments are supported: name - (Required) The name of the environment size - (Optional, Default: \"m3.medium\") The size of the instances in the environment. Available instance sizes can be found here min_count - (Optional, Default: 0) The minimum number of instances allowed in the environment user-data - (Optional) The user data template to use for the environment's autoscaling group. See the cli reference for the default template. os - (Optional, Default: \"linux\") Specifies the type of operating system used in the environment. Options are \"linux\" or \"windows\". ami - (Optional) A custom AMI ID to use in the environment. If not specified, Layer0 will use its default AMI ID for the specified operating system. Attribute Reference # The following attributes are exported: id - The id of the environment name - The name of the environment size - The size of the instances in the environment cluster_count - The current number instances in the environment security_group_id - The ID of the environment's security group os - The operating system used for the environment ami - The AMI ID used for the environment Load Balancer Resource # Provides a Layer0 Load Balancer Example Usage # # Create a new load balancer resource \"layer0_load_balancer\" \"guestbook\" { name = \"guestbook\" environment = \"demo123\" private = false port { host_port = 80 container_port = 80 protocol = \"http\" } port { host_port = 443 container_port = 443 protocol = \"https\" certificate = \"cert\" } health_check { target = \"tcp:80\" interval = 30 timeout = 5 healthy_threshold = 2 unhealthy_threshold = 2 } idle_timeout = 300 } Argument Reference # The following arguments are supported: name - (Required) The name of the load balancer environment - (Required) The id of the environment to place the load balancer inside of private - (Optional) If true, the load balancer will not be exposed to the public internet port - (Optional, Default: 80:80/tcp) A list of port blocks. Ports documented below health_check - (Optional, Default: {\"TCP:80\" 30 5 2 2} ) A health_check block. Health check documented below idle_timeout - (Optiona, Default: 60) The idle timeout of the load balancer in seconds Ports ( port ) support the following: host_port - (Required) The port on the load balancer to listen on container_port - (Required) The port on the docker container to route to protocol - (Required) The protocol to listen on. Valid values are HTTP, HTTPS, TCP, or SSL certificate - (Optional) The name of an SSL certificate. Only required if the HTTP or SSL protocol is used. Healthcheck ( health_check ) supports the following: target - (Required) The target of the check. Valid pattern is PROTOCOL:PORT/PATH , where PROTOCOL values are: HTTP , HTTPS - PORT and PATH are required TCP , SSL - PORT is required, PATH is not supported interval - (Required) The interval between checks. timeout - (Required) The length of time before the check times out. healthy_threshold - (Required) The number of checks before the instance is declared healthy. unhealthy_threshold - (Required) The number of checks before the instance is declared unhealthy. The Idle Timeout ( idle_timeout ) is managed by the load balancer and is triggered when no data is sent over a connection for the specified time period. If no data has been sent or received by the time that the idle timeout period elapses, the load balancer closes the connection. See the following documentation for more information: https://docs.aws.amazon.com/elasticloadbalancing/latest/application/application-load-balancers.html#connection-idle-timeout Attribute Reference # The following attributes are exported: id - The id of the load balancer name - The name of the load balancer environment - The id of the environment the load balancer exists in private - Whether or not the load balancer is private url - The URL of the load balancer Service Resource # Provides a Layer0 Service Example Usage # # Create a new service resource \"layer0_service\" \"guestbook\" { name = \"guestbook\" environment = \"environment123\" deploy = \"deploy123\" load_balancer = \"loadbalancer123\" scale = 3 } Argument Reference # The following arguments are supported: name - (Required) The name of the service environment - (Required) The id of the environment to place the service inside of deploy - (Required) The id of the deploy for the service to run load_balancer (Optional) The id of the load balancer to place the service behind scale (Optional, Default: 1) The number of copies of the service to run Attribute Reference # The following attributes are exported: id - The id of the service name - The name of the service environment - The id of the environment the service exists in deploy - The id of the deploy the service is running load_balancer - The id of the load balancer the service is behind (if load_balancer was set) scale - The current desired scale of the service Best Practices # Always run Terraform plan before terraform apply . This will show you what action(s) Terraform plans to make before actually executing them. Use variables to reference secrets. Secrets can be placed in a file named terraform.tfvars , or by setting TF_VAR_* environment variables. More information can be found here . Use Terraform's remote command to backup and sync your terraform.tfstate file across different members in your organization. Terraform has documentation for using S3 as a backend here . Terraform modules allow you to define and consume reusable components. Example configurations can be found here","title":"Layer0 Terraform Plugin"},{"location":"reference/terraform-plugin/#layer0-terraform-provider-reference","text":"Terraform is an open-source tool for provisioning and managing infrastructure. If you are new to Terraform, we recommend checking out their documentation . Layer0 has built a custom provider for Layer0. This provider allows users to create, manage, and update Layer0 entities using Terraform.","title":"Layer0 Terraform Provider Reference"},{"location":"reference/terraform-plugin/#prerequisites","text":"Terraform v0.11+ ( download ), accessible in your system path.","title":"Prerequisites"},{"location":"reference/terraform-plugin/#install","text":"Download a Layer0 v0.8.4+ release . The Terraform plugin binary is located in the release zip file as terraform-provider-layer0 . Copy this terraform-provider-layer0 binary into the same directory as your Terraform binary - and you're done! For further information, see Terraform's documentation on installing a Terraform plugin here .","title":"Install"},{"location":"reference/terraform-plugin/#getting-started","text":"Checkout the Terraform section of the Guestbook walkthrough here . We've added some tips and links to helpful resources in the Best Practices section below.","title":"Getting Started"},{"location":"reference/terraform-plugin/#provider","text":"The Layer0 provider is used to interact with a Layer0 API. The provider needs to be configured with the proper credentials before it can be used.","title":"Provider"},{"location":"reference/terraform-plugin/#example-usage","text":"# Add 'endpoint' and 'token' variables variable \"endpoint\" {} variable \"token\" {} # Configure the layer0 provider provider \"layer0\" { endpoint = \" ${ var . endpoint } \" token = \" ${ var . token } \" skip_ssl_verify = true }","title":"Example Usage"},{"location":"reference/terraform-plugin/#argument-reference","text":"The following arguments are supported: Note The endpoint and token variables for your layer0 api can be found using the l0-setup endpoint command. endpoint - (Required) The endpoint of the layer0 api token - (Required) The authentication token for the layer0 api skip_ssl_verify - (Optional) If true, ssl certificate mismatch warnings will be ignored","title":"Argument Reference"},{"location":"reference/terraform-plugin/#api-data-source","text":"The API data source is used to extract useful read-only variables from the Layer0 API.","title":"API Data Source"},{"location":"reference/terraform-plugin/#example-usage_1","text":"# Configure the api data source data \"layer0_api\" \"config\" {} # Output the layer0 vpc id output \"vpc id\" { val = \" ${ data . layer0_api . config . vpc_id } \" }","title":"Example Usage"},{"location":"reference/terraform-plugin/#attribute-reference","text":"The following attributes are exported: prefix - The prefix of the layer0 instance vpc_id - The vpc id of the layer0 instance public_subnets - A list containing the 2 public subnet ids in the layer0 vpc private_subnets - A list containing the 2 private subnet ids in the layer0 vpc","title":"Attribute Reference"},{"location":"reference/terraform-plugin/#deploy-data-source","text":"The Deploy data source is used to extract Layer0 Deploy attributes.","title":"Deploy Data Source"},{"location":"reference/terraform-plugin/#example-usage_2","text":"# Configure the deploy data source data \"layer0_deploy\" \"dpl\" { name = \"my-deploy\" version = \"1\" } # Output the layer0 deploy id output \"deploy_id\" { val = \" ${ data . layer0_deploy . dpl . id } \" }","title":"Example Usage"},{"location":"reference/terraform-plugin/#argument-reference_1","text":"The following arguments are supported: name - (Required) The name of the deploy version - (Required) The version of the deploy","title":"Argument Reference"},{"location":"reference/terraform-plugin/#attribute-reference_1","text":"The following attributes are exported: name - The name of the deploy version - The version of the deploy id - The id of the deploy","title":"Attribute Reference"},{"location":"reference/terraform-plugin/#environment-data-source","text":"The Environment data source is used to extract Layer0 Environment attributes.","title":"Environment Data Source"},{"location":"reference/terraform-plugin/#example-usage_3","text":"# Configure the environment data source data \"layer0_environment\" \"env\" { name = \"my-environment\" } # Output the layer0 environment id output \"environment_id\" { val = \" ${ data . layer0_environment . env . id } \" }","title":"Example Usage"},{"location":"reference/terraform-plugin/#argument-reference_2","text":"The following arguments are supported: name - (Required) The name of the environment","title":"Argument Reference"},{"location":"reference/terraform-plugin/#attribute-reference_2","text":"The following attributes are exported: id - The id of the environment name - The name of the environment size - The size of the instances in the environment min_count - The current number instances in the environment os - The operating system used for the environment ami - The AMI ID used for the environment","title":"Attribute Reference"},{"location":"reference/terraform-plugin/#load-balancer-data-source","text":"The Load Balancer data source is used to extract Layer0 Load Balancer attributes.","title":"Load Balancer Data Source"},{"location":"reference/terraform-plugin/#example-usage_4","text":"# Configure the load balancer source data \"layer0_load_balancer\" \"lb\" { name = \"my-loadbalancer\" environment_id = \" ${ data . layer0_environment . env . environment_id } \" } # Output the layer0 load balancer id output \"load_balancer_id\" { val = \" ${ data . layer0_load_balancer . lb . id } \" }","title":"Example Usage"},{"location":"reference/terraform-plugin/#argument-reference_3","text":"The following arguments are supported: name - (required) The name of the load balancer environment_id - (required) The id of the environment the load balancer exists in","title":"Argument Reference"},{"location":"reference/terraform-plugin/#attribute-reference_3","text":"The following attributes are exported: id - The id of the load balancer name - The name of the load balancer environment_id - The id of the environment the load balancer exists in environment_name - The name of the environment the load balancer exists in private - Whether or not the load balancer is private url - The URL of the load balancer","title":"Attribute Reference"},{"location":"reference/terraform-plugin/#service-data-source","text":"The Service data source is used to extract Layer0 Service attributes.","title":"Service Data Source"},{"location":"reference/terraform-plugin/#example-usage_5","text":"# Configure the service data source data \"layer0_service\" \"svc\" { name = \"my-service\" environment_id = \" ${ data . layer0_environment . env . environment_id } \" } # Output the layer0 service id output \"service_id\" { val = \" ${ data . layer0_service . svc . id } \" }","title":"Example Usage"},{"location":"reference/terraform-plugin/#argument-reference_4","text":"The following arguments are supported: name - (required) The name of the service environment_id - (required) The id of the environment the service exists in","title":"Argument Reference"},{"location":"reference/terraform-plugin/#attribute-reference_4","text":"The following attributes are exported: id - The id of the service name - The name of the service environment_id - The id of the environment the service exists in environment_name - The name of the environment the service exists in scale - The current desired scale of the service","title":"Attribute Reference"},{"location":"reference/terraform-plugin/#deploy-resource","text":"Provides a Layer0 Deploy. Performing variable substitution inside of your deploy's json file (typically named Dockerrun.aws.json ) can be done through Terraform's template_file . For a working example, please see the sample Guestbook application","title":"Deploy Resource"},{"location":"reference/terraform-plugin/#example-usage_6","text":"# Configure the deploy template data \"template_file\" \"guestbook\" { template = \" ${ file ( \"Dockerrun.aws.json\" ) } \" vars { docker_image_tag = \"latest\" } } # Create a deploy using the rendered template resource \"layer0_deploy\" \"guestbook\" { name = \"guestbook\" content = \" ${ data . template_file . guestbook . rendered } \" }","title":"Example Usage"},{"location":"reference/terraform-plugin/#argument-reference_5","text":"The following arguments are supported: name - (Required) The name of the deploy content - (Required) The content of the deploy","title":"Argument Reference"},{"location":"reference/terraform-plugin/#attribute-reference_5","text":"The following attributes are exported: id - The id of the deploy name - The name of the deploy version - The version number of the deploy","title":"Attribute Reference"},{"location":"reference/terraform-plugin/#environment-resource","text":"Provides a Layer0 Environment","title":"Environment Resource"},{"location":"reference/terraform-plugin/#example-usage_7","text":"# Create a new environment resource \"layer0_environment\" \"demo\" { name = \"demo\" size = \"m3.medium\" min_count = 0 user_data = \"echo hello, world\" os = \"linux\" ami = \"ami123\" }","title":"Example Usage"},{"location":"reference/terraform-plugin/#argument-reference_6","text":"The following arguments are supported: name - (Required) The name of the environment size - (Optional, Default: \"m3.medium\") The size of the instances in the environment. Available instance sizes can be found here min_count - (Optional, Default: 0) The minimum number of instances allowed in the environment user-data - (Optional) The user data template to use for the environment's autoscaling group. See the cli reference for the default template. os - (Optional, Default: \"linux\") Specifies the type of operating system used in the environment. Options are \"linux\" or \"windows\". ami - (Optional) A custom AMI ID to use in the environment. If not specified, Layer0 will use its default AMI ID for the specified operating system.","title":"Argument Reference"},{"location":"reference/terraform-plugin/#attribute-reference_6","text":"The following attributes are exported: id - The id of the environment name - The name of the environment size - The size of the instances in the environment cluster_count - The current number instances in the environment security_group_id - The ID of the environment's security group os - The operating system used for the environment ami - The AMI ID used for the environment","title":"Attribute Reference"},{"location":"reference/terraform-plugin/#load-balancer-resource","text":"Provides a Layer0 Load Balancer","title":"Load Balancer Resource"},{"location":"reference/terraform-plugin/#example-usage_8","text":"# Create a new load balancer resource \"layer0_load_balancer\" \"guestbook\" { name = \"guestbook\" environment = \"demo123\" private = false port { host_port = 80 container_port = 80 protocol = \"http\" } port { host_port = 443 container_port = 443 protocol = \"https\" certificate = \"cert\" } health_check { target = \"tcp:80\" interval = 30 timeout = 5 healthy_threshold = 2 unhealthy_threshold = 2 } idle_timeout = 300 }","title":"Example Usage"},{"location":"reference/terraform-plugin/#argument-reference_7","text":"The following arguments are supported: name - (Required) The name of the load balancer environment - (Required) The id of the environment to place the load balancer inside of private - (Optional) If true, the load balancer will not be exposed to the public internet port - (Optional, Default: 80:80/tcp) A list of port blocks. Ports documented below health_check - (Optional, Default: {\"TCP:80\" 30 5 2 2} ) A health_check block. Health check documented below idle_timeout - (Optiona, Default: 60) The idle timeout of the load balancer in seconds Ports ( port ) support the following: host_port - (Required) The port on the load balancer to listen on container_port - (Required) The port on the docker container to route to protocol - (Required) The protocol to listen on. Valid values are HTTP, HTTPS, TCP, or SSL certificate - (Optional) The name of an SSL certificate. Only required if the HTTP or SSL protocol is used. Healthcheck ( health_check ) supports the following: target - (Required) The target of the check. Valid pattern is PROTOCOL:PORT/PATH , where PROTOCOL values are: HTTP , HTTPS - PORT and PATH are required TCP , SSL - PORT is required, PATH is not supported interval - (Required) The interval between checks. timeout - (Required) The length of time before the check times out. healthy_threshold - (Required) The number of checks before the instance is declared healthy. unhealthy_threshold - (Required) The number of checks before the instance is declared unhealthy. The Idle Timeout ( idle_timeout ) is managed by the load balancer and is triggered when no data is sent over a connection for the specified time period. If no data has been sent or received by the time that the idle timeout period elapses, the load balancer closes the connection. See the following documentation for more information: https://docs.aws.amazon.com/elasticloadbalancing/latest/application/application-load-balancers.html#connection-idle-timeout","title":"Argument Reference"},{"location":"reference/terraform-plugin/#attribute-reference_7","text":"The following attributes are exported: id - The id of the load balancer name - The name of the load balancer environment - The id of the environment the load balancer exists in private - Whether or not the load balancer is private url - The URL of the load balancer","title":"Attribute Reference"},{"location":"reference/terraform-plugin/#service-resource","text":"Provides a Layer0 Service","title":"Service Resource"},{"location":"reference/terraform-plugin/#example-usage_9","text":"# Create a new service resource \"layer0_service\" \"guestbook\" { name = \"guestbook\" environment = \"environment123\" deploy = \"deploy123\" load_balancer = \"loadbalancer123\" scale = 3 }","title":"Example Usage"},{"location":"reference/terraform-plugin/#argument-reference_8","text":"The following arguments are supported: name - (Required) The name of the service environment - (Required) The id of the environment to place the service inside of deploy - (Required) The id of the deploy for the service to run load_balancer (Optional) The id of the load balancer to place the service behind scale (Optional, Default: 1) The number of copies of the service to run","title":"Argument Reference"},{"location":"reference/terraform-plugin/#attribute-reference_8","text":"The following attributes are exported: id - The id of the service name - The name of the service environment - The id of the environment the service exists in deploy - The id of the deploy the service is running load_balancer - The id of the load balancer the service is behind (if load_balancer was set) scale - The current desired scale of the service","title":"Attribute Reference"},{"location":"reference/terraform-plugin/#best-practices","text":"Always run Terraform plan before terraform apply . This will show you what action(s) Terraform plans to make before actually executing them. Use variables to reference secrets. Secrets can be placed in a file named terraform.tfvars , or by setting TF_VAR_* environment variables. More information can be found here . Use Terraform's remote command to backup and sync your terraform.tfstate file across different members in your organization. Terraform has documentation for using S3 as a backend here . Terraform modules allow you to define and consume reusable components. Example configurations can be found here","title":"Best Practices"},{"location":"reference/terraform_introduction/","text":"Introduction to Terraform # What does Terraform do? # Terraform is a powerful orchestration tool for creating, updating, deleting, and otherwise managing infrastructure in an easy-to-understand, declarative manner. Terraform's documentation is very good, but at a glance: Be Declarative - Specify desired infrastructure results in Terraform ( *.tf ) files, and let Terraform do the heavy work of figuring out how to make that specification a reality. Scry the Future - Use terraform plan to see a list of everything that Terraform would do without actually making those changes. Version Infrastructure - Check Terraform files into a VCS to track changes to and manage versions of your infrastructure. Why Terraform? # Why did we latch onto Terraform instead of something like CloudFormation? Cloud-Agnostic - Unlike CloudFormation, Terraform is able to incorporate different resource providers to manage infrastructure across multiple cloud services (not just AWS). Custom Providers - Terraform can be extended to manage tools that don't come natively through use of custom providers. We wrote a Layer0 provider so that Terraform can manage Layer0 resources in addition to tools and resources and infrastructure beyond Layer0's scope. Terraform has some things to say on the matter as well. Advantages Versus Layer0 CLI? # Why should you move from using (or scripting) the Layer0 CLI directly? Reduce Fat-Fingering Mistakes - Creating Terraform files (and using terraform plan ) allows you to review your deployment and catch errors. Executing Layer0 CLI commands one-by-one is tiresome, non-transportable, and a process ripe for typos. Go Beyond Layer0 - Retain the benefits of leveraging Layer0's concepts and resources using our provider , but also gain the ability to orchestrate resources and tools beyond the CLI's scope. How do I get Terraform? # Check out Terraform's documentation on the subject.","title":"Terraform"},{"location":"reference/terraform_introduction/#introduction-to-terraform","text":"","title":"Introduction to Terraform"},{"location":"reference/terraform_introduction/#what-does-terraform-do","text":"Terraform is a powerful orchestration tool for creating, updating, deleting, and otherwise managing infrastructure in an easy-to-understand, declarative manner. Terraform's documentation is very good, but at a glance: Be Declarative - Specify desired infrastructure results in Terraform ( *.tf ) files, and let Terraform do the heavy work of figuring out how to make that specification a reality. Scry the Future - Use terraform plan to see a list of everything that Terraform would do without actually making those changes. Version Infrastructure - Check Terraform files into a VCS to track changes to and manage versions of your infrastructure.","title":"What does Terraform do?"},{"location":"reference/terraform_introduction/#why-terraform","text":"Why did we latch onto Terraform instead of something like CloudFormation? Cloud-Agnostic - Unlike CloudFormation, Terraform is able to incorporate different resource providers to manage infrastructure across multiple cloud services (not just AWS). Custom Providers - Terraform can be extended to manage tools that don't come natively through use of custom providers. We wrote a Layer0 provider so that Terraform can manage Layer0 resources in addition to tools and resources and infrastructure beyond Layer0's scope. Terraform has some things to say on the matter as well.","title":"Why Terraform?"},{"location":"reference/terraform_introduction/#advantages-versus-layer0-cli","text":"Why should you move from using (or scripting) the Layer0 CLI directly? Reduce Fat-Fingering Mistakes - Creating Terraform files (and using terraform plan ) allows you to review your deployment and catch errors. Executing Layer0 CLI commands one-by-one is tiresome, non-transportable, and a process ripe for typos. Go Beyond Layer0 - Retain the benefits of leveraging Layer0's concepts and resources using our provider , but also gain the ability to orchestrate resources and tools beyond the CLI's scope.","title":"Advantages Versus Layer0 CLI?"},{"location":"reference/terraform_introduction/#how-do-i-get-terraform","text":"Check out Terraform's documentation on the subject.","title":"How do I get Terraform?"},{"location":"reference/updateservice/","text":"Updating a Layer0 service # There are three methods of updating an existing Layer0 service. The first method is to update the existing Deploy to refer to a new Docker task definition. The second method is to create a new Service that uses the same Loadbalancer. The third method is to create both a new Loadbalancer and a new Service. There are advantages and disadvantages to each of these methods. The following sections discuss the advantages and disadvantages of using each method, and include procedures for implementing each method. Method 1: Refer to a new task definition # This method of updating a Layer0 application is the easiest to implement, because you do not need to rescale the Service or modify the Loadbalancer. This method is completely transparent to all other components of the application, and using this method does not involve any downtime. The disadvantage of using this method is that you cannot perform A/B testing of the old and new services, and you cannot control which traffic goes to the old service and which goes to the new one. To replace a Deploy to refer to a new task definition: At the command line, type the following to create a new Deploy: l0 deploy create taskDefPath deployName taskDefPath is the path to the ECS Task Definition. Note that if deployName already exists, this step will create a new version of that Deploy. Use l0 service update to update the existing service: l0 service update serviceName deployName[:deployVersion] By default, the service name you specify in this command will refer to the latest version of deployName . You can optionally specify a specific version of the deploy, as shown above. Method 2: Create a new Deploy and Service using the same Loadbalancer # This method of updating a Layer0 application is also rather easy to implement. Like the method described in the previous section, this method is completely transparent to all other services and components of the application. This method also you allows you to re-scale the service if necessary, using the l0 service scale command. Finally, this method allows for indirect A/B testing of the application; you can change the scale of the application, and observe the success and failure rates. The disadvantage of using this method is that you cannot control the routing of traffic between the old and new versions of the application. To create a new Deploy and Service: At the command line, type the following to create a new deploy or a new version of a deploy: l0 deploy create taskDefPath deployName taskDefPath is the path to the ECS Task Definition. Note that if deployName already exists, this step will create a new version of that Deploy. Use l0 service create to create a new service that uses deployName behind an existing load balancer named loadBalancerName l0 service create --loadbalancer [environmentName:]loadBalancerName environmentName serviceName deployName[:deployVersion] By default, the service name you specify in this command will refer to the latest version of deployName . You can optionally specify a specific version of the deploy, as shown above. You can also optionally specify the name of the environment, environmentName where the load balancer exists. Check to make sure that the new service is working as expected. If it is, and you do not want to keep the old service, delete the old service: l0 service delete service Method 3: Create a new Deploy, Loadbalancer and Service # The final method of updating a Layer0 service is to create an entirely new Deploy, Load Balancer and Service. This method gives you complete control over both the new and the old Service, and allows you to perform true A/B testing by routing traffic to individual Services. The disadvantage of using this method is that you need to implement a method of routing traffic between the new and the old Load Balancer. To create a new Deploy, Load Balancer and Service: Type the following command to create a new Deploy: l0 deploy create taskDefPath deployName taskDefPath is the path to the ECS Task Definition. Note that if deployName already exists, this step will create a new version of that Deploy. Use l0 loadbalancer create to create a new Load Balancer: l0 loadbalancer create --port port environmentName loadBalancerName deployName port is the port configuration for the listener of the Load Balancer. Valid pattern is hostPort:containerPort/protocol . Multiple ports can be specified using --port port1 --port port2 ... . hostPort - The port that the load balancer will listen for traffic on. containerPort - The port that the load balancer will forward traffic to. protocol - The protocol to use when forwarding traffic (acceptable values: TCP, SSL, HTTP, and HTTPS). Note The value of loadbalancerName in the above command must be unique to the Environment. Use l0 service create to create a new Service using the Load Balancer you just created: l0 service create --loadbalancer loadBalancerName environmentName serviceName deployName Note The value of serviceName in the above command must be unique to the Environment. Implement a method of routing traffic between the old and new Services, such as HAProxy or Consul .","title":"Updating a Service"},{"location":"reference/updateservice/#updating-a-layer0-service","text":"There are three methods of updating an existing Layer0 service. The first method is to update the existing Deploy to refer to a new Docker task definition. The second method is to create a new Service that uses the same Loadbalancer. The third method is to create both a new Loadbalancer and a new Service. There are advantages and disadvantages to each of these methods. The following sections discuss the advantages and disadvantages of using each method, and include procedures for implementing each method.","title":"Updating a Layer0 service"},{"location":"reference/updateservice/#method-1-refer-to-a-new-task-definition","text":"This method of updating a Layer0 application is the easiest to implement, because you do not need to rescale the Service or modify the Loadbalancer. This method is completely transparent to all other components of the application, and using this method does not involve any downtime. The disadvantage of using this method is that you cannot perform A/B testing of the old and new services, and you cannot control which traffic goes to the old service and which goes to the new one. To replace a Deploy to refer to a new task definition: At the command line, type the following to create a new Deploy: l0 deploy create taskDefPath deployName taskDefPath is the path to the ECS Task Definition. Note that if deployName already exists, this step will create a new version of that Deploy. Use l0 service update to update the existing service: l0 service update serviceName deployName[:deployVersion] By default, the service name you specify in this command will refer to the latest version of deployName . You can optionally specify a specific version of the deploy, as shown above.","title":"Method 1: Refer to a new task definition"},{"location":"reference/updateservice/#method-2-create-a-new-deploy-and-service-using-the-same-loadbalancer","text":"This method of updating a Layer0 application is also rather easy to implement. Like the method described in the previous section, this method is completely transparent to all other services and components of the application. This method also you allows you to re-scale the service if necessary, using the l0 service scale command. Finally, this method allows for indirect A/B testing of the application; you can change the scale of the application, and observe the success and failure rates. The disadvantage of using this method is that you cannot control the routing of traffic between the old and new versions of the application. To create a new Deploy and Service: At the command line, type the following to create a new deploy or a new version of a deploy: l0 deploy create taskDefPath deployName taskDefPath is the path to the ECS Task Definition. Note that if deployName already exists, this step will create a new version of that Deploy. Use l0 service create to create a new service that uses deployName behind an existing load balancer named loadBalancerName l0 service create --loadbalancer [environmentName:]loadBalancerName environmentName serviceName deployName[:deployVersion] By default, the service name you specify in this command will refer to the latest version of deployName . You can optionally specify a specific version of the deploy, as shown above. You can also optionally specify the name of the environment, environmentName where the load balancer exists. Check to make sure that the new service is working as expected. If it is, and you do not want to keep the old service, delete the old service: l0 service delete service","title":"Method 2: Create a new Deploy and Service using the same Loadbalancer"},{"location":"reference/updateservice/#method-3-create-a-new-deploy-loadbalancer-and-service","text":"The final method of updating a Layer0 service is to create an entirely new Deploy, Load Balancer and Service. This method gives you complete control over both the new and the old Service, and allows you to perform true A/B testing by routing traffic to individual Services. The disadvantage of using this method is that you need to implement a method of routing traffic between the new and the old Load Balancer. To create a new Deploy, Load Balancer and Service: Type the following command to create a new Deploy: l0 deploy create taskDefPath deployName taskDefPath is the path to the ECS Task Definition. Note that if deployName already exists, this step will create a new version of that Deploy. Use l0 loadbalancer create to create a new Load Balancer: l0 loadbalancer create --port port environmentName loadBalancerName deployName port is the port configuration for the listener of the Load Balancer. Valid pattern is hostPort:containerPort/protocol . Multiple ports can be specified using --port port1 --port port2 ... . hostPort - The port that the load balancer will listen for traffic on. containerPort - The port that the load balancer will forward traffic to. protocol - The protocol to use when forwarding traffic (acceptable values: TCP, SSL, HTTP, and HTTPS). Note The value of loadbalancerName in the above command must be unique to the Environment. Use l0 service create to create a new Service using the Load Balancer you just created: l0 service create --loadbalancer loadBalancerName environmentName serviceName deployName Note The value of serviceName in the above command must be unique to the Environment. Implement a method of routing traffic between the old and new Services, such as HAProxy or Consul .","title":"Method 3: Create a new Deploy, Loadbalancer and Service"},{"location":"setup/destroy/","text":"Destroying a Layer0 Instance # This section provides procedures for destroying (deleting) a Layer0 instance. Part 1: Clean Up Your Layer0 Environments # In order to destroy a Layer0 instance, you must first delete all environments in the instance. List all environments with: l0 environment list For each environment listed in the previous step, with the exception of the environment named api , issue the following command (replacing <environment_name> with the name of the environment to delete): l0 environment delete --wait <environment_name> Part 2: Destroy the Layer0 Instance # Once all environments have been deleted, the Layer0 instance can be deleted using the l0-setup tool. Run the following command (replacing <instance_name> with the name of the Layer0 instance): l0-setup destroy <instance_name> The destroy command is idempotent; if it fails, it is safe to re-attempt multiple times. Note If the operation continues to fail, it is likely there are resources that were created outside of Layer0 that have dependencies on the resources l0-setup is attempting to destroy. You will need to manually remove these dependencies in order to get the destroy command to complete successfully.","title":"Destroy"},{"location":"setup/destroy/#destroying-a-layer0-instance","text":"This section provides procedures for destroying (deleting) a Layer0 instance.","title":"Destroying a Layer0 Instance"},{"location":"setup/destroy/#part-1-clean-up-your-layer0-environments","text":"In order to destroy a Layer0 instance, you must first delete all environments in the instance. List all environments with: l0 environment list For each environment listed in the previous step, with the exception of the environment named api , issue the following command (replacing <environment_name> with the name of the environment to delete): l0 environment delete --wait <environment_name>","title":"Part 1: Clean Up Your Layer0 Environments"},{"location":"setup/destroy/#part-2-destroy-the-layer0-instance","text":"Once all environments have been deleted, the Layer0 instance can be deleted using the l0-setup tool. Run the following command (replacing <instance_name> with the name of the Layer0 instance): l0-setup destroy <instance_name> The destroy command is idempotent; if it fails, it is safe to re-attempt multiple times. Note If the operation continues to fail, it is likely there are resources that were created outside of Layer0 that have dependencies on the resources l0-setup is attempting to destroy. You will need to manually remove these dependencies in order to get the destroy command to complete successfully.","title":"Part 2: Destroy the Layer0 Instance"},{"location":"setup/install/","text":"Create a new Layer0 Instance # Prerequisites # Before you can install and configure Layer0, you must obtain the following: Access to an AWS account An EC2 Key Pair This key pair allows you to access the EC2 instances running your Services using SSH. If you have already created a key pair, you can use it for this process. Otherwise, follow the AWS documentation to create a new key pair. Make a note of the name that you selected when creating the key pair. Terraform v0.11+ We use Terraform to create the resources that Layer0 needs. If you're unfamiliar with Terraform, you may want to check out our introduction . If you're ready to install Terraform, there are instructions in the Terraform documentation . Part 1: Download and extract Layer0 # In the Downloads section of the home page , select the appropriate installation file for your operating system. Extract the zip file to a directory on your computer. (Optional) Place the l0 and l0-setup binaries into your system path. For more information about adding directories to your system path, see the following resources: (Windows): How to Edit Your System PATH for Easy Command Line Access in Windows (Linux/macOS): Adding a Directory to the Path Part 2: Create an Access Key # This step will create an Identity & Access Management (IAM) access key for your AWS account. You will use the credentials created in this section when creating, updating, or removing Layer0 instances. To create an Access Key: In a web browser, login to the AWS Console . Click the Services dropdown menu in the upper left portion of the console page, then type IAM in the text box that appears at the top of the page after you click Services . As you type IAM, a search result will appear below the text box. Click on the IAM service result that appears below the text box. In the left panel, click Groups , and then confirm that you have a group called Administrators . Is the Administrators group missing in your AWS account? If the Administrators group does not already exist, complete the following steps: Click Create New Group . Name the new group Administrators , and then click Next Step . Check the AdministratorAccess policy to attach the Administrator policy to your new group. Click Next Step , and then click Create Group . In the left panel, click Users . Click the New User button and enter a unique user name you will use for Layer0. This user name can be used for multiple Layer0 installations. Check the box next to Programmatic access , and then click the Next: Permissions button. Make sure the Add user to group button is highlighted. Find and check the box next to the group Administrators . Click Next: Review button to continue. This will make your newly created user an administrator for your AWS account, so be sure to keep your security credentials safe! Review your choices and then click the Create user button. Once your user account has been created, click the Download .csv button to save your access and secret key to a CSV file. Part 3: Create a new Layer0 Instance # Now that you have downloaded Layer0 and configured your AWS account, you can create your Layer0 instance. From a command prompt, run the following (replacing <instance_name> with a name for your Layer0 instance): l0-setup init <instance_name> This command will prompt you for many different inputs. Enter the required values for AWS Access Key , AWS Secret Key , and AWS SSH Key as they come up. All remaining inputs are optional and can be set to their default by pressing enter. ... AWS Access Key: The access_key input variable is used to provision the AWS resources required for Layer0. This corresponds to the Access Key ID portion of an AWS Access Key. It is recommended this key has the 'AdministratorAccess' policy. Note that Layer0 will only use this key for 'l0-setup' commands associated with this Layer0 instance; the Layer0 API will use its own key with limited permissions to provision AWS resources. [current: <none>] Please enter a value and press 'enter'. Input: ABC123xzy AWS Secret Key: The secret_key input variable is used to provision the AWS resources required for Layer0. This corresponds to the Secret Access Key portion of an AWS Access Key. It is recommended this key has the 'AdministratorAccess' policy. Note that Layer0 will only use this key for 'l0-setup' commands associated with this Layer0 instance; the Layer0 API will use its own key with limited permissions to provision AWS resources. [current: <none>] Please enter a value and press 'enter'. Input: ZXY987cba AWS SSH Key Pair: The ssh_key_pair input variable specifies the name of the ssh key pair to include in EC2 instances provisioned by Layer0. This key pair must already exist in the AWS account. The names of existing key pairs can be found in the EC2 dashboard. Note that changing this value will not effect instances that have already been provisioned. [current: <none>] Please enter a value and press 'enter'. Input: mySSHKey ... Once the init command has successfully completed, you're ready to actually create the resources needed to use Layer0. Run the following command (again, replace <instance_name> with the name you've chosen for your Layer0 instance): l0-setup apply <instance_name> The first time you run the apply command, it may take around 5 minutes to complete. This command is idempotent; it is safe to run multiple times if it fails the first. At the end of the apply command, your Layer0 instance's configuration and state will be automatically backed up to an S3 bucket. You can manually back up your configuration at any time using the push command. It's a good idea to run this command regularly ( l0-setup push <instance_name> ) to ensure that your configuration is backed up. These files can be downloaded at any time using the pull command ( l0-setup pull <instance_name> ). Using a Private Docker Registry The procedures in this section are optional, but are highly recommended for production use. If you require authentication to a private Docker registry, you will need a Docker configuration file present on your machine with access to private repositories (typically located at ~/.docker/config.json ). If you don't have a config file yet, you can generate one by running docker login [registry-address] . A configuration file will be generated at ~/.docker/config.json . To add this authentication to your Layer0 instance, run: l0-setup init --docker-path=<path/to/config.json> <instance_name> This will reconfigure your Layer0 configuration and add a rendered file into your Layer0 instance's directory at ~/.layer0/<instance_name>/dockercfg.json . You can modify a Layer0 instance's dockercfg.json file and re-run the apply command ( l0-setup apply <instance_name> ) to make changes to your authentication. Note: Any EC2 instances created prior to changing your dockercfg.json file will need to be manually terminated since they only grab the authentication file during instance creation. Terminated EC2 instances will be automatically re-created by autoscaling. Using an Existing VPC The procedures in this section must be followed precisely to properly install Layer0 into an existing VPC By default, l0-setup creates a new VPC to place resources. However, l0-setup can place resources in an existing VPC if the VPC meets all of the following conditions: Has access to the public internet (through a NAT instance or gateway) Has at least 1 public and 1 private subnet The public and private subnets have the tag Tier: Public or Tier: Private , respectively. For information on how to tag AWS resources, please visit the AWS documentation . Once you are sure the existing VPC satisfies these requirements, run the init command, placing the VPC ID when prompted: l0-setup init <instance_name> ... VPC ID (optional): The vpc_id input variable specifies an existing AWS VPC to provision the AWS resources required for Layer0. If no input is specified, a new VPC will be created for you. Existing VPCs must satisfy the following constraints: - Have access to the public internet (through a NAT instance or gateway) - Have at least 1 public and 1 private subnet - Each subnet must be tagged with [\"Tier\": \"Private\"] or [\"Tier\": \"Public\"] Note that changing this value will destroy and recreate any existing resources. [current: ] Please enter a new value, or press 'enter' to keep the current value. Input: vpc123 Once the command has completed, it is safe to run apply to provision the resources. Part 4: Connect to a Layer0 Instance # Once the apply command has run successfully, you can configure the environment variables needed to connect to the Layer0 API using the endpoint command. l0-setup endpoint --insecure <instance_name> export LAYER0_API_ENDPOINT=\"https://l0-instance_name-api-123456.us-west-2.elb.amazonaws.com\" export LAYER0_AUTH_TOKEN=\"abcDEFG123\" export LAYER0_SKIP_SSL_VERIFY=\"1\" export LAYER0_SKIP_VERSION_VERIFY=\"1\" Danger The --insecure flag shows configurations that bypass SSL and version verifications. This is required as the Layer0 API created uses a self-signed SSL certificate by default. These settings are not recommended for production use! The endpoint command supports a --syntax option, which can be used to turn configuration into a single line: Bash (default) - eval \"$(l0-setup endpoint --insecure <instance_name>)\" Powershell - l0-setup endpoint --insecure --syntax=powershell <instance_name> | Out-String | Invoke-Expression","title":"Install"},{"location":"setup/install/#create-a-new-layer0-instance","text":"","title":"Create a new Layer0 Instance"},{"location":"setup/install/#prerequisites","text":"Before you can install and configure Layer0, you must obtain the following: Access to an AWS account An EC2 Key Pair This key pair allows you to access the EC2 instances running your Services using SSH. If you have already created a key pair, you can use it for this process. Otherwise, follow the AWS documentation to create a new key pair. Make a note of the name that you selected when creating the key pair. Terraform v0.11+ We use Terraform to create the resources that Layer0 needs. If you're unfamiliar with Terraform, you may want to check out our introduction . If you're ready to install Terraform, there are instructions in the Terraform documentation .","title":"Prerequisites"},{"location":"setup/install/#part-1-download-and-extract-layer0","text":"In the Downloads section of the home page , select the appropriate installation file for your operating system. Extract the zip file to a directory on your computer. (Optional) Place the l0 and l0-setup binaries into your system path. For more information about adding directories to your system path, see the following resources: (Windows): How to Edit Your System PATH for Easy Command Line Access in Windows (Linux/macOS): Adding a Directory to the Path","title":"Part 1: Download and extract Layer0"},{"location":"setup/install/#part-2-create-an-access-key","text":"This step will create an Identity & Access Management (IAM) access key for your AWS account. You will use the credentials created in this section when creating, updating, or removing Layer0 instances. To create an Access Key: In a web browser, login to the AWS Console . Click the Services dropdown menu in the upper left portion of the console page, then type IAM in the text box that appears at the top of the page after you click Services . As you type IAM, a search result will appear below the text box. Click on the IAM service result that appears below the text box. In the left panel, click Groups , and then confirm that you have a group called Administrators . Is the Administrators group missing in your AWS account? If the Administrators group does not already exist, complete the following steps: Click Create New Group . Name the new group Administrators , and then click Next Step . Check the AdministratorAccess policy to attach the Administrator policy to your new group. Click Next Step , and then click Create Group . In the left panel, click Users . Click the New User button and enter a unique user name you will use for Layer0. This user name can be used for multiple Layer0 installations. Check the box next to Programmatic access , and then click the Next: Permissions button. Make sure the Add user to group button is highlighted. Find and check the box next to the group Administrators . Click Next: Review button to continue. This will make your newly created user an administrator for your AWS account, so be sure to keep your security credentials safe! Review your choices and then click the Create user button. Once your user account has been created, click the Download .csv button to save your access and secret key to a CSV file.","title":"Part 2: Create an Access Key"},{"location":"setup/install/#part-3-create-a-new-layer0-instance","text":"Now that you have downloaded Layer0 and configured your AWS account, you can create your Layer0 instance. From a command prompt, run the following (replacing <instance_name> with a name for your Layer0 instance): l0-setup init <instance_name> This command will prompt you for many different inputs. Enter the required values for AWS Access Key , AWS Secret Key , and AWS SSH Key as they come up. All remaining inputs are optional and can be set to their default by pressing enter. ... AWS Access Key: The access_key input variable is used to provision the AWS resources required for Layer0. This corresponds to the Access Key ID portion of an AWS Access Key. It is recommended this key has the 'AdministratorAccess' policy. Note that Layer0 will only use this key for 'l0-setup' commands associated with this Layer0 instance; the Layer0 API will use its own key with limited permissions to provision AWS resources. [current: <none>] Please enter a value and press 'enter'. Input: ABC123xzy AWS Secret Key: The secret_key input variable is used to provision the AWS resources required for Layer0. This corresponds to the Secret Access Key portion of an AWS Access Key. It is recommended this key has the 'AdministratorAccess' policy. Note that Layer0 will only use this key for 'l0-setup' commands associated with this Layer0 instance; the Layer0 API will use its own key with limited permissions to provision AWS resources. [current: <none>] Please enter a value and press 'enter'. Input: ZXY987cba AWS SSH Key Pair: The ssh_key_pair input variable specifies the name of the ssh key pair to include in EC2 instances provisioned by Layer0. This key pair must already exist in the AWS account. The names of existing key pairs can be found in the EC2 dashboard. Note that changing this value will not effect instances that have already been provisioned. [current: <none>] Please enter a value and press 'enter'. Input: mySSHKey ... Once the init command has successfully completed, you're ready to actually create the resources needed to use Layer0. Run the following command (again, replace <instance_name> with the name you've chosen for your Layer0 instance): l0-setup apply <instance_name> The first time you run the apply command, it may take around 5 minutes to complete. This command is idempotent; it is safe to run multiple times if it fails the first. At the end of the apply command, your Layer0 instance's configuration and state will be automatically backed up to an S3 bucket. You can manually back up your configuration at any time using the push command. It's a good idea to run this command regularly ( l0-setup push <instance_name> ) to ensure that your configuration is backed up. These files can be downloaded at any time using the pull command ( l0-setup pull <instance_name> ). Using a Private Docker Registry The procedures in this section are optional, but are highly recommended for production use. If you require authentication to a private Docker registry, you will need a Docker configuration file present on your machine with access to private repositories (typically located at ~/.docker/config.json ). If you don't have a config file yet, you can generate one by running docker login [registry-address] . A configuration file will be generated at ~/.docker/config.json . To add this authentication to your Layer0 instance, run: l0-setup init --docker-path=<path/to/config.json> <instance_name> This will reconfigure your Layer0 configuration and add a rendered file into your Layer0 instance's directory at ~/.layer0/<instance_name>/dockercfg.json . You can modify a Layer0 instance's dockercfg.json file and re-run the apply command ( l0-setup apply <instance_name> ) to make changes to your authentication. Note: Any EC2 instances created prior to changing your dockercfg.json file will need to be manually terminated since they only grab the authentication file during instance creation. Terminated EC2 instances will be automatically re-created by autoscaling. Using an Existing VPC The procedures in this section must be followed precisely to properly install Layer0 into an existing VPC By default, l0-setup creates a new VPC to place resources. However, l0-setup can place resources in an existing VPC if the VPC meets all of the following conditions: Has access to the public internet (through a NAT instance or gateway) Has at least 1 public and 1 private subnet The public and private subnets have the tag Tier: Public or Tier: Private , respectively. For information on how to tag AWS resources, please visit the AWS documentation . Once you are sure the existing VPC satisfies these requirements, run the init command, placing the VPC ID when prompted: l0-setup init <instance_name> ... VPC ID (optional): The vpc_id input variable specifies an existing AWS VPC to provision the AWS resources required for Layer0. If no input is specified, a new VPC will be created for you. Existing VPCs must satisfy the following constraints: - Have access to the public internet (through a NAT instance or gateway) - Have at least 1 public and 1 private subnet - Each subnet must be tagged with [\"Tier\": \"Private\"] or [\"Tier\": \"Public\"] Note that changing this value will destroy and recreate any existing resources. [current: ] Please enter a new value, or press 'enter' to keep the current value. Input: vpc123 Once the command has completed, it is safe to run apply to provision the resources.","title":"Part 3: Create a new Layer0 Instance"},{"location":"setup/install/#part-4-connect-to-a-layer0-instance","text":"Once the apply command has run successfully, you can configure the environment variables needed to connect to the Layer0 API using the endpoint command. l0-setup endpoint --insecure <instance_name> export LAYER0_API_ENDPOINT=\"https://l0-instance_name-api-123456.us-west-2.elb.amazonaws.com\" export LAYER0_AUTH_TOKEN=\"abcDEFG123\" export LAYER0_SKIP_SSL_VERIFY=\"1\" export LAYER0_SKIP_VERSION_VERIFY=\"1\" Danger The --insecure flag shows configurations that bypass SSL and version verifications. This is required as the Layer0 API created uses a self-signed SSL certificate by default. These settings are not recommended for production use! The endpoint command supports a --syntax option, which can be used to turn configuration into a single line: Bash (default) - eval \"$(l0-setup endpoint --insecure <instance_name>)\" Powershell - l0-setup endpoint --insecure --syntax=powershell <instance_name> | Out-String | Invoke-Expression","title":"Part 4: Connect to a Layer0 Instance"},{"location":"setup/upgrade/","text":"Upgrade a Layer0 Instance # This section provides procedures for upgrading your Layer0 installation to the latest version. This assumes you are using Layer0 version v0.10.0 or later. Warning Layer0 does not support updating MAJOR or MINOR versions in place unless explicitly stated otherwise. Users will either need to create a new Layer0 instance and migrate to it or destroy and re-create their Layer0 instance in these circumstances. Run the upgrade command, replacing <instance_name> and <version> with the name of the Layer0 instance and new version, respectively: l0-setup upgrade <instance_name> <version> This will prompt you about the updated source and version inputs changing. If you are not satisfied with the changes, exit the application during the prompts. For full control on changing inputs, use the set command. Example Usage l0-setup upgrade mylayer0 v0.10.1 This will update the 'version' input From: [v0.10.0] To: [v0.10.1] Press 'enter' to accept this change: This will update the 'source' input From: [github.com/quintilesims/layer0//setup/module?ref=v0.10.0] To: [github.com/quintilesims/layer0//setup/module?ref=v0.10.1] Press 'enter' to accept this change: ... Everything looks good! You are now ready to run 'l0-setup apply mylayer0' As stated by the command output, run the apply command to apply the changes to the Layer0 instance. If any errors occur, please contact the Layer0 team.","title":"Upgrade"},{"location":"setup/upgrade/#upgrade-a-layer0-instance","text":"This section provides procedures for upgrading your Layer0 installation to the latest version. This assumes you are using Layer0 version v0.10.0 or later. Warning Layer0 does not support updating MAJOR or MINOR versions in place unless explicitly stated otherwise. Users will either need to create a new Layer0 instance and migrate to it or destroy and re-create their Layer0 instance in these circumstances. Run the upgrade command, replacing <instance_name> and <version> with the name of the Layer0 instance and new version, respectively: l0-setup upgrade <instance_name> <version> This will prompt you about the updated source and version inputs changing. If you are not satisfied with the changes, exit the application during the prompts. For full control on changing inputs, use the set command. Example Usage l0-setup upgrade mylayer0 v0.10.1 This will update the 'version' input From: [v0.10.0] To: [v0.10.1] Press 'enter' to accept this change: This will update the 'source' input From: [github.com/quintilesims/layer0//setup/module?ref=v0.10.0] To: [github.com/quintilesims/layer0//setup/module?ref=v0.10.1] Press 'enter' to accept this change: ... Everything looks good! You are now ready to run 'l0-setup apply mylayer0' As stated by the command output, run the apply command to apply the changes to the Layer0 instance. If any errors occur, please contact the Layer0 team.","title":"Upgrade a Layer0 Instance"},{"location":"troubleshooting/commonissues/","text":"Common issues and their solutions # \"Connection refused\" error when executing Layer0 commands # When executing commands using the Layer0 CLI, you may see the following error message: Get http://localhost:9090/command/: dial tcp 127.0.0.1:9090: connection refused Where command is the Layer0 command you are trying to execute. This error indicates that your Layer0 environment variables have not been set for the current session. See the \"Connect to a Layer0 Instance\" section of the Layer0 installation guide for instructions for setting up your environment variables. \"Invalid Dockerrun.aws.json\" error when creating a deploy # Byte Order Marks (BOM) in Dockerrun file # If your Dockerrun.aws.json file contains a Byte Order Marker, you may receive an \"Invalid Dockerrun.aws.json\" error when creating a deploy. If you create or edit the Dockerrun file using Visual Studio, and you have not modified the file encoding settings in Visual Studio, you are likely to encounter this error. To remove the BOM: At the command line, type the following to remove the BOM: (Linux/OS X) tail -c +4 DockerrunFile > DockerrunFileNew Replace DockerrunFile with the path to your Dockerrun file, and DockerrunFileNew with a new name for the Dockerrun file without the BOM. Alternatively, you can use the dos2unix file converter to remove the BOM from your Dockerrun files. Dos2unix is available for Windows, Linux and Mac OS. To remove the BOM using dos2unix: At the command line, type the following: dos2unix --remove-bom -n DockerrunFile DockerrunFileNew Replace DockerrunFile with the path to your Dockerrun file, and DockerrunFileNew with a new name for the Dockerrun file without the BOM. \"AWS Error: the key pair ' ' does not exist (code 'ValidationError')\" with l0-setup # This occurs when you pass an invalid EC2 keypair to l0-setup. To fix this, follow the instructions for creating an EC2 Key Pair . After you've created a new EC2 Key Pair, use l0-setup init to reconfigure your instance: l0-setup init --aws-ssh-key-pair keypair","title":"Common Issues"},{"location":"troubleshooting/commonissues/#common-issues-and-their-solutions","text":"","title":"Common issues and their solutions"},{"location":"troubleshooting/commonissues/#connection-refused-error-when-executing-layer0-commands","text":"When executing commands using the Layer0 CLI, you may see the following error message: Get http://localhost:9090/command/: dial tcp 127.0.0.1:9090: connection refused Where command is the Layer0 command you are trying to execute. This error indicates that your Layer0 environment variables have not been set for the current session. See the \"Connect to a Layer0 Instance\" section of the Layer0 installation guide for instructions for setting up your environment variables.","title":"\"Connection refused\" error when executing Layer0 commands"},{"location":"troubleshooting/commonissues/#invalid-dockerrunawsjson-error-when-creating-a-deploy","text":"","title":"\"Invalid Dockerrun.aws.json\" error when creating a deploy"},{"location":"troubleshooting/commonissues/#byte-order-marks-bom-in-dockerrun-file","text":"If your Dockerrun.aws.json file contains a Byte Order Marker, you may receive an \"Invalid Dockerrun.aws.json\" error when creating a deploy. If you create or edit the Dockerrun file using Visual Studio, and you have not modified the file encoding settings in Visual Studio, you are likely to encounter this error. To remove the BOM: At the command line, type the following to remove the BOM: (Linux/OS X) tail -c +4 DockerrunFile > DockerrunFileNew Replace DockerrunFile with the path to your Dockerrun file, and DockerrunFileNew with a new name for the Dockerrun file without the BOM. Alternatively, you can use the dos2unix file converter to remove the BOM from your Dockerrun files. Dos2unix is available for Windows, Linux and Mac OS. To remove the BOM using dos2unix: At the command line, type the following: dos2unix --remove-bom -n DockerrunFile DockerrunFileNew Replace DockerrunFile with the path to your Dockerrun file, and DockerrunFileNew with a new name for the Dockerrun file without the BOM.","title":"Byte Order Marks (BOM) in Dockerrun file"},{"location":"troubleshooting/commonissues/#aws-error-the-key-pair-does-not-exist-code-validationerror-with-l0-setup","text":"This occurs when you pass an invalid EC2 keypair to l0-setup. To fix this, follow the instructions for creating an EC2 Key Pair . After you've created a new EC2 Key Pair, use l0-setup init to reconfigure your instance: l0-setup init --aws-ssh-key-pair keypair","title":"\"AWS Error: the key pair '' does not exist (code 'ValidationError')\" with l0-setup"},{"location":"troubleshooting/ssh/","text":"Secure Shell (SSH) # You can use Secure Shell (SSH) to access your Layer0 environment(s). By default, Layer0 Setup asks for an EC2 key pair when creating a new Layer0. This key pair is associated with all machines that host your Layer0 Services. This means you can use SSH to log into the underlying Docker host to perform tasks such as troubleshooting failing containers or viewing logs. For information about creating an EC2 key pair, see Install and Configure Layer0 . Warning This section is recommended for development debugging only. It is not recommended for production environments. To SSH into a Service # In a console window, add port 2222:22/tcp to your Service's load balancer: l0 loadbalancer addport <name> 2222:22/tcp SSH into your Service by supplying the load balancer url and key pair file name. ssh -i <key pair path and file name> ec2-user@<load balancer url> -p 2222 If required, Use Docker to access a specific container with Bash. docker exec -it <container id> /bin/bash Remarks # You can get the load balancer url from the Load Balancers section of your Layer0 AWS console. Use the l0 loadbalancer dropport subcommand to remove a port configuration from an existing Layer0 load balancer. You cannot change the key pair after a Layer0 has been created. If you lose your key pair or need to generate a new one, you will need to create a new Layer0. If your Service is behind a private load balancer, or none at all, you can either re-create your Service behind a public load balancer, use an existing public load balancer as a \"jump\" point, or create a new Layer0 Service behind a public load balancer to serve as a \"jump\" point.","title":"Secure Shell (SSH)"},{"location":"troubleshooting/ssh/#secure-shell-ssh","text":"You can use Secure Shell (SSH) to access your Layer0 environment(s). By default, Layer0 Setup asks for an EC2 key pair when creating a new Layer0. This key pair is associated with all machines that host your Layer0 Services. This means you can use SSH to log into the underlying Docker host to perform tasks such as troubleshooting failing containers or viewing logs. For information about creating an EC2 key pair, see Install and Configure Layer0 . Warning This section is recommended for development debugging only. It is not recommended for production environments.","title":"Secure Shell (SSH)"},{"location":"troubleshooting/ssh/#to-ssh-into-a-service","text":"In a console window, add port 2222:22/tcp to your Service's load balancer: l0 loadbalancer addport <name> 2222:22/tcp SSH into your Service by supplying the load balancer url and key pair file name. ssh -i <key pair path and file name> ec2-user@<load balancer url> -p 2222 If required, Use Docker to access a specific container with Bash. docker exec -it <container id> /bin/bash","title":"To SSH into a Service"},{"location":"troubleshooting/ssh/#remarks","text":"You can get the load balancer url from the Load Balancers section of your Layer0 AWS console. Use the l0 loadbalancer dropport subcommand to remove a port configuration from an existing Layer0 load balancer. You cannot change the key pair after a Layer0 has been created. If you lose your key pair or need to generate a new one, you will need to create a new Layer0. If your Service is behind a private load balancer, or none at all, you can either re-create your Service behind a public load balancer, use an existing public load balancer as a \"jump\" point, or create a new Layer0 Service behind a public load balancer to serve as a \"jump\" point.","title":"Remarks"}]}