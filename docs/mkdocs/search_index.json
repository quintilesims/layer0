{
    "docs": [
        {
            "location": "/", 
            "text": "Build, Manage, and Deploy Your Application\n#\n\n\nMeet Layer0\n#\n\n\nLayer0 is a framework that helps you deploy web applications to the cloud with minimal fuss. Using a simple command line interface (CLI), you can manage the entire life cycle of your application without having to focus on infrastructure.\n\n\nReady to learn more about Layer0? See our \nintroduction page\n to learn about some important concepts. When you're ready to get started, take a look at the \ninstallation page\n for information about setting up Layer0.\n\n\nDownload\n#\n\n\n\n\n\n\n\n\nDownload \nv0.9.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmacOS\n\n\nLinux\n\n\nWindows\n\n\n\n\n\n\n\n\nContact Us\n#\n\n\nIf you have questions about Layer0, email the development team at \nxfra@us.imshealth.com\n.", 
            "title": "Home"
        }, 
        {
            "location": "/#build-manage-and-deploy-your-application", 
            "text": "", 
            "title": "Build, Manage, and Deploy Your Application"
        }, 
        {
            "location": "/#meet-layer0", 
            "text": "Layer0 is a framework that helps you deploy web applications to the cloud with minimal fuss. Using a simple command line interface (CLI), you can manage the entire life cycle of your application without having to focus on infrastructure.  Ready to learn more about Layer0? See our  introduction page  to learn about some important concepts. When you're ready to get started, take a look at the  installation page  for information about setting up Layer0.", 
            "title": "Meet Layer0"
        }, 
        {
            "location": "/#download", 
            "text": "Download  v0.9.0             macOS  Linux  Windows", 
            "title": "Download"
        }, 
        {
            "location": "/#contact-us", 
            "text": "If you have questions about Layer0, email the development team at  xfra@us.imshealth.com .", 
            "title": "Contact Us"
        }, 
        {
            "location": "/releases/", 
            "text": "Version\n\n\nmacOS\n\n\nLinux\n\n\nWindows\n\n\n\n\n\n\n\n\n\n\nv0.9.0\n\n\nmacOS\n\n\nLinux\n\n\nWindows\n\n\n\n\n\n\nv0.8.4\n\n\nmacOS\n\n\nLinux\n\n\nWindows", 
            "title": "Releases"
        }, 
        {
            "location": "/intro/", 
            "text": "Layer0 Introduction\n#\n\n\nIn recent years, the process of deploying applications has seen incredible innovation. However, this innovation has taken a somewhat simple task and made it into something quite \ncomplicated\n. Cloud providers, load balancing, virtual servers, IP subnets, and a continuing list of technological considerations are not only required to be understood, but their creation and management must be automated for a modern application to be successful at scale.\n\n\nThe burden of understanding a complicated and ever-growing infrastructure is a large aspect of what Layer0 is trying to fix. We've already done the leg work for huge swathes of your backend infrastructure, and we've made it easy to tear down and start over again, too. Meanwhile, you can develop locally using \nDocker\n and be assured that your application will properly translate to the cloud when you're ready to deploy.\n\n\nLayer0 requires a solid understanding of Docker to get the most out of it. We highly recommend starting with \nDocker's Understanding the Architecture\n to learn more about using Docker locally and in the cloud. We also recommend the \nTwelve-Factor App\n primer, which is a critical resource for understanding how to build a microservice.\n\n\n\n\nLayer0 Concepts\n#\n\n\nThe following concepts are core Layer0 abstractions for the technologies and features we use \nbehind the scenes\n. These terms will be used throughout our guides, so having a general understanding of them is helpful.\n\n\nCertificates\n#\n\n\nSSL certificates obtained from a valid \nCertificate Authority (CA)\n. You can use these certificates to secure your HTTPS services by applying them to your Layer0 load balancers.\n\n\nDeploys\n#\n\n\nECS Task Definitions\n. These configuration files detail how to deploy your application. We have several \nsample applications\n available that show what these files look like --- they're called \nDockerrun.aws.json\n within each sample app.\n\n\nTasks\n#\n\n\nManual one-off commands that don't necessarily make sense to keep running, or to restart when they finish. These run using Amazon's \nRunTask\n action (more info \nhere\n), and are \"ideally suited for processes such as batch jobs that perform work and then stop.\"\n\n\nLoad Balancers\n#\n\n\nPowerful tools that give you the basic building blocks for high-availability, scaling, and HTTPS. We currently use Amazon's \nElastic Load Balancing\n, and it pays to understand the basics of this service when working with Layer0.\n\n\nServices\n#\n\n\nYour running Layer0 applications. We also use the term \nservice\n for tools such as Consul, for which we provide a pre-built \nsample implementation\n using Layer0.\n\n\nEnvironments\n#\n\n\nLogical groupings of services. Typically, you would make a single environment for each tier of your application, such as \ndev\n, \nstaging\n, and \nprod\n.", 
            "title": "Introduction"
        }, 
        {
            "location": "/intro/#layer0-introduction", 
            "text": "In recent years, the process of deploying applications has seen incredible innovation. However, this innovation has taken a somewhat simple task and made it into something quite  complicated . Cloud providers, load balancing, virtual servers, IP subnets, and a continuing list of technological considerations are not only required to be understood, but their creation and management must be automated for a modern application to be successful at scale.  The burden of understanding a complicated and ever-growing infrastructure is a large aspect of what Layer0 is trying to fix. We've already done the leg work for huge swathes of your backend infrastructure, and we've made it easy to tear down and start over again, too. Meanwhile, you can develop locally using  Docker  and be assured that your application will properly translate to the cloud when you're ready to deploy.  Layer0 requires a solid understanding of Docker to get the most out of it. We highly recommend starting with  Docker's Understanding the Architecture  to learn more about using Docker locally and in the cloud. We also recommend the  Twelve-Factor App  primer, which is a critical resource for understanding how to build a microservice.", 
            "title": "Layer0 Introduction"
        }, 
        {
            "location": "/intro/#layer0-concepts", 
            "text": "The following concepts are core Layer0 abstractions for the technologies and features we use  behind the scenes . These terms will be used throughout our guides, so having a general understanding of them is helpful.", 
            "title": "Layer0 Concepts"
        }, 
        {
            "location": "/intro/#certificates", 
            "text": "SSL certificates obtained from a valid  Certificate Authority (CA) . You can use these certificates to secure your HTTPS services by applying them to your Layer0 load balancers.", 
            "title": "Certificates"
        }, 
        {
            "location": "/intro/#deploys", 
            "text": "ECS Task Definitions . These configuration files detail how to deploy your application. We have several  sample applications  available that show what these files look like --- they're called  Dockerrun.aws.json  within each sample app.", 
            "title": "Deploys"
        }, 
        {
            "location": "/intro/#tasks", 
            "text": "Manual one-off commands that don't necessarily make sense to keep running, or to restart when they finish. These run using Amazon's  RunTask  action (more info  here ), and are \"ideally suited for processes such as batch jobs that perform work and then stop.\"", 
            "title": "Tasks"
        }, 
        {
            "location": "/intro/#load-balancers", 
            "text": "Powerful tools that give you the basic building blocks for high-availability, scaling, and HTTPS. We currently use Amazon's  Elastic Load Balancing , and it pays to understand the basics of this service when working with Layer0.", 
            "title": "Load Balancers"
        }, 
        {
            "location": "/intro/#services", 
            "text": "Your running Layer0 applications. We also use the term  service  for tools such as Consul, for which we provide a pre-built  sample implementation  using Layer0.", 
            "title": "Services"
        }, 
        {
            "location": "/intro/#environments", 
            "text": "Logical groupings of services. Typically, you would make a single environment for each tier of your application, such as  dev ,  staging , and  prod .", 
            "title": "Environments"
        }, 
        {
            "location": "/setup/install/", 
            "text": "Install and Configure Layer0\n#\n\n\nPrerequisites\n#\n\n\nBefore you can install and configure Layer0, you must obtain the following:\n\n\n\n\nAn AWS account.\n\n\nAn EC2 Key Pair.\n This key pair allows you to access the EC2 instances running your Services using SSH. If you have already created a key pair, you can use it for this process. Otherwise, follow the \ninstructions at aws.amazon.com\n to create a new key pair. Make a note of the name that you selected when creating the key pair.\n\n\n\n\nPart 1: Download and extract Layer0\n#\n\n\n\n\nIn the \nDownloads section of the home page\n, select the appropriate installation file for your operating system. Extract the zip file to a directory on your computer.\n\n\nAdd both the directory that contains the \nl0\n application, as well as the entire l0-setup directory, to your system path. The l0-setup directory contains files that are necessary for the \nl0-setup\n application to work properly, so you must add the entire directory to your path.\nFor more information about adding directories to your system path, see the following resources:\n\n\n(Windows): \nHow to Edit Your System PATH for Easy Command Line Access in Windows\n\n\n(Linux/macOS): \nAdding a Directory to the Path\n\n\n\n\n\n\n\n\nPart 2: Create an Access Key\n#\n\n\nThis step will create an Identity \n Access Management (IAM) access key from your AWS account. You will use the credentials created in this section when installing, updating, or removing Layer0 resources.\n\n\nTo create an Access Key:\n\n\n\n\n\n\nIn a web browser, login to the \nAWS Console\n.\n\n\n\n\n\n\nUnder \nSecurity and Identity\n, click \nIdentity and Access Management\n.\n\n\n\n\n\n\nClick \nGroups\n, and then click \nAdministrators\n. \nNote\nIf the \nAdministrators\n group does not already exist, complete the following steps: \nClick \nCreate New Group\n. Name the new group \"Administrators\", and then click \nNext Step\n.\nClick \nAdministratorAccess\n to attach the Administrator policy to your new group.\nClick \nNext Step\n, and then click \nCreate Group\n.\n\n\n\n\n\n\nClick \nUsers\n.\n\n\n\n\n\n\nClick \nCreate New Users\n and enter a unique user name you will use for Layer0. This user name can be used for multiple Layer0 installations. Check the box next to \nGenerate an Access Key for each user\n, and then click \nCreate\n.\n\n\n\n\n\n\nOnce your user account has been created, click \nDownload Credentials\n to save your access key to a CSV file.\n\n\n\n\n\n\nIn the Users list, click the user account you just created. Under \nUser Actions\n, click \nAdd User to Groups\n.\n\n\n\n\n\n\nSelect the group \nAdministrators\n and click \nAdd to Groups\n. This will make your newly created user an administrator for your AWS account, so be sure to keep your security credentials safe!\n\n\n\n\n\n\nPart 3: Configure your Layer0\n#\n\n\nNow that you have downloaded Layer0 and configured your AWS instance, you can create your Layer0.\n\n\nTo configure Layer0:\n\n\n\n\n\n\nAt the command prompt, navigate to the \nl0-setup\n subdirectory in the folder in which you extracted the Layer0 files.\n\n\n\n\n\n\nType the following command, replacing \n[prefix]\n with a unique name for your Layer0: \nl0-setup apply [prefix]\n \n\n\nUsing a private Docker registry\n\n\n\n\nThe procedures in this section are optional, but are highly recommended for production use.\n\n\n\n\nIf you require authentication to a private Docker registry, you will need to populate a file named \ndockercfg\n inside your Layer0 instance directory, located at \n~/layer0/instances/[prefix]/dockercfg\n. When you run \nl0-setup\n for the first time, use the \n--dockercfg\n flag, like so:\n\n\n\n\nl0-setup apply --dockercfg [path/to/config/file] [prefix]\n\n\n\n\nIf you don't have a config file yet, you can generate one by running \ndocker login [registry-address]\n. A configuration file will be generated at \n~/.docker/config.json\n for Docker versions 1.7 or higher, or at \n~/.dockercfg\n for Docker versions below 1.7.\n\n\n\n\nYou can modify an instance's \ndockercfg\n file (the one located at \n~/layer0/instances/[instance]/dockercfg\n) and re-run \nl0-setup apply\n to make changes to your authentication. Note that any EC2 instances created prior to changing your \ndockercfg\n file will need to be manually terminated since they only grab the \ndockercfg\n authentication file once during instance creation. Terminated EC2 instances will be automatically re-created by autoscaling.\n\n\n\n\n\n\n\n\nWhen prompted, enter the following information:\n\n\n\n\nAWS Access Key ID\n: The access key ID contained in the credential file that you downloaded in step 6 of the previous section.\n\n\nAWS Secret Access Key\n: The secret access key contained in the credential file that you downloaded in step 6 of the previous section.\n\n\nKey Pair\n: The name of the key pair that you created in the Prerequisites section.\nThe first time you run the \napply\n command, it may take around 15 minutes to complete. If the \napply\n command fails to complete successfully, it is safe to run it again until it succeeds.\n\n\n\n\n\n\n\n\nPart 4: Configure the environment variables\n#\n\n\nOnce the \napply\n command has run successfully, you can configure the Layer0 environment variables using the \nendpoint\n command.\n\n\nTo view the environment variables for your Layer0 and apply them to your shell, type the following command, replacing \n[prefix]\n with the name of the Layer0 prefix you created in Part 3:\n\n\n\n\n(Windows PowerShell): \nl0-setup endpoint --insecure --syntax=powershell [prefix] | Out-String | Invoke-Expression\n\n\n(Linux/macOS): \neval \"$(l0-setup endpoint --insecure [prefix])\"\n\n\n\n\n(Optional) Part 6: Using a custom certificate\n#\n\n\n\n\nNote\n\n\nThe procedures in this section are optional, but are highly recommended for production use.\n\n\n\n\nLayer0 uses a self-signed certificate to run the API.\nIn a production setting, we recommend using a certificate from a trusted CA.\n\n\nTo use a custom certificate:\n\n\n\n    \nIn a text editor, open the file in your Layer0 directory named elb.tf.template.\nThe full path will be `~/layer0/instances/\n/elb.tf.template`.\nUpdate the following line the file:\n\n\n...\n    ssl_certificate_id = \"[ARN of your SSL cert]\"\n...\n\n\n    \nSave the modified elb.tf.template file, and then run the following command: \nl0-setup apply [prefix]", 
            "title": "Install"
        }, 
        {
            "location": "/setup/install/#install-and-configure-layer0", 
            "text": "", 
            "title": "Install and Configure Layer0"
        }, 
        {
            "location": "/setup/install/#prerequisites", 
            "text": "Before you can install and configure Layer0, you must obtain the following:   An AWS account.  An EC2 Key Pair.  This key pair allows you to access the EC2 instances running your Services using SSH. If you have already created a key pair, you can use it for this process. Otherwise, follow the  instructions at aws.amazon.com  to create a new key pair. Make a note of the name that you selected when creating the key pair.", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/setup/install/#part-1-download-and-extract-layer0", 
            "text": "In the  Downloads section of the home page , select the appropriate installation file for your operating system. Extract the zip file to a directory on your computer.  Add both the directory that contains the  l0  application, as well as the entire l0-setup directory, to your system path. The l0-setup directory contains files that are necessary for the  l0-setup  application to work properly, so you must add the entire directory to your path. For more information about adding directories to your system path, see the following resources:  (Windows):  How to Edit Your System PATH for Easy Command Line Access in Windows  (Linux/macOS):  Adding a Directory to the Path", 
            "title": "Part 1: Download and extract Layer0"
        }, 
        {
            "location": "/setup/install/#part-2-create-an-access-key", 
            "text": "This step will create an Identity   Access Management (IAM) access key from your AWS account. You will use the credentials created in this section when installing, updating, or removing Layer0 resources.  To create an Access Key:    In a web browser, login to the  AWS Console .    Under  Security and Identity , click  Identity and Access Management .    Click  Groups , and then click  Administrators .  Note If the  Administrators  group does not already exist, complete the following steps:  Click  Create New Group . Name the new group \"Administrators\", and then click  Next Step . Click  AdministratorAccess  to attach the Administrator policy to your new group. Click  Next Step , and then click  Create Group .    Click  Users .    Click  Create New Users  and enter a unique user name you will use for Layer0. This user name can be used for multiple Layer0 installations. Check the box next to  Generate an Access Key for each user , and then click  Create .    Once your user account has been created, click  Download Credentials  to save your access key to a CSV file.    In the Users list, click the user account you just created. Under  User Actions , click  Add User to Groups .    Select the group  Administrators  and click  Add to Groups . This will make your newly created user an administrator for your AWS account, so be sure to keep your security credentials safe!", 
            "title": "Part 2: Create an Access Key"
        }, 
        {
            "location": "/setup/install/#part-3-configure-your-layer0", 
            "text": "Now that you have downloaded Layer0 and configured your AWS instance, you can create your Layer0.  To configure Layer0:    At the command prompt, navigate to the  l0-setup  subdirectory in the folder in which you extracted the Layer0 files.    Type the following command, replacing  [prefix]  with a unique name for your Layer0:  l0-setup apply [prefix]    Using a private Docker registry   The procedures in this section are optional, but are highly recommended for production use.   If you require authentication to a private Docker registry, you will need to populate a file named  dockercfg  inside your Layer0 instance directory, located at  ~/layer0/instances/[prefix]/dockercfg . When you run  l0-setup  for the first time, use the  --dockercfg  flag, like so:   l0-setup apply --dockercfg [path/to/config/file] [prefix]   If you don't have a config file yet, you can generate one by running  docker login [registry-address] . A configuration file will be generated at  ~/.docker/config.json  for Docker versions 1.7 or higher, or at  ~/.dockercfg  for Docker versions below 1.7.   You can modify an instance's  dockercfg  file (the one located at  ~/layer0/instances/[instance]/dockercfg ) and re-run  l0-setup apply  to make changes to your authentication. Note that any EC2 instances created prior to changing your  dockercfg  file will need to be manually terminated since they only grab the  dockercfg  authentication file once during instance creation. Terminated EC2 instances will be automatically re-created by autoscaling.     When prompted, enter the following information:   AWS Access Key ID : The access key ID contained in the credential file that you downloaded in step 6 of the previous section.  AWS Secret Access Key : The secret access key contained in the credential file that you downloaded in step 6 of the previous section.  Key Pair : The name of the key pair that you created in the Prerequisites section.\nThe first time you run the  apply  command, it may take around 15 minutes to complete. If the  apply  command fails to complete successfully, it is safe to run it again until it succeeds.", 
            "title": "Part 3: Configure your Layer0"
        }, 
        {
            "location": "/setup/install/#part-4-configure-the-environment-variables", 
            "text": "Once the  apply  command has run successfully, you can configure the Layer0 environment variables using the  endpoint  command.  To view the environment variables for your Layer0 and apply them to your shell, type the following command, replacing  [prefix]  with the name of the Layer0 prefix you created in Part 3:   (Windows PowerShell):  l0-setup endpoint --insecure --syntax=powershell [prefix] | Out-String | Invoke-Expression  (Linux/macOS):  eval \"$(l0-setup endpoint --insecure [prefix])\"", 
            "title": "Part 4: Configure the environment variables"
        }, 
        {
            "location": "/setup/install/#optional-part-6-using-a-custom-certificate", 
            "text": "Note  The procedures in this section are optional, but are highly recommended for production use.   Layer0 uses a self-signed certificate to run the API.\nIn a production setting, we recommend using a certificate from a trusted CA.  To use a custom certificate:  \n     In a text editor, open the file in your Layer0 directory named elb.tf.template.\nThe full path will be `~/layer0/instances/ /elb.tf.template`.\nUpdate the following line the file: \n...\n    ssl_certificate_id = \"[ARN of your SSL cert]\"\n... \n     Save the modified elb.tf.template file, and then run the following command:  l0-setup apply [prefix]", 
            "title": "(Optional) Part 6: Using a custom certificate"
        }, 
        {
            "location": "/setup/upgrade/", 
            "text": "Upgrade Layer0\n#\n\n\nThis section provides procedures for upgrading your Layer0 installation to the latest version.\n\n\n\n\n\n\nIn the \nDownloads section of the home page\n, select the appropriate installation file for your operating system. Extract the zip file to a directory on your computer, and then move the \nl0\n and \nl0-setup\n files to a folder in your system path, replacing any previous versions of these files.\n\n\n\n\n\n\nYou will need your existing access keys when l0-setup prompts you for AWS credentials. Type the following commands to find them:\n\n\n\n\nl0-setup terraform [prefix] output access_key\n\n\nl0-setup terraform [prefix] output secret_key\n\n\n\n\n\n\n\n\nType the following command to verify that you are working with the correct version of Layer0:\n\n\n\n\nl0-setup --version\n\n\n\n\nThe output of this command should display the version number of the most recent version of Layer0. If it does, proceed to the next step; if not, ensure that you copied the latest versions of \nl0\n and \nl0-setup\n to the appropriate directories in your system path.\n\n\n\n\n\n\nType the following command to restore the state files for your Layer0, replacing \n[prefix]\n with the name of your Layer0 prefix:\n\n\n\n\nl0-setup restore [prefix]\n\n\n\n\n\n\n\n\nType the following command to update your api image tag:\n\n\n\n\nl0-setup plan [prefix] -var api_docker_image_tag=[version]\n\n\n\n\n\n\n\n\nType the following command to update your runner image tag:\n\n\n\n\nl0-setup plan [prefix] -var runner_version_tag=[version]\n\n\n\n\n\n\n\n\nType the following command to apply the upgrade:\n\n\n\n\nl0-setup apply [prefix]", 
            "title": "Upgrade"
        }, 
        {
            "location": "/setup/upgrade/#upgrade-layer0", 
            "text": "This section provides procedures for upgrading your Layer0 installation to the latest version.    In the  Downloads section of the home page , select the appropriate installation file for your operating system. Extract the zip file to a directory on your computer, and then move the  l0  and  l0-setup  files to a folder in your system path, replacing any previous versions of these files.    You will need your existing access keys when l0-setup prompts you for AWS credentials. Type the following commands to find them:   l0-setup terraform [prefix] output access_key  l0-setup terraform [prefix] output secret_key     Type the following command to verify that you are working with the correct version of Layer0:   l0-setup --version   The output of this command should display the version number of the most recent version of Layer0. If it does, proceed to the next step; if not, ensure that you copied the latest versions of  l0  and  l0-setup  to the appropriate directories in your system path.    Type the following command to restore the state files for your Layer0, replacing  [prefix]  with the name of your Layer0 prefix:   l0-setup restore [prefix]     Type the following command to update your api image tag:   l0-setup plan [prefix] -var api_docker_image_tag=[version]     Type the following command to update your runner image tag:   l0-setup plan [prefix] -var runner_version_tag=[version]     Type the following command to apply the upgrade:   l0-setup apply [prefix]", 
            "title": "Upgrade Layer0"
        }, 
        {
            "location": "/setup/destroy/", 
            "text": "Destroying a Layer0 instance\n#\n\n\nDuring testing or migration, you may find that you need to delete a non-functional or outdated instance of Layer0. This section provides procedures for destroying (deleting) a Layer0 instance.\n\n\nPart 1: Clean Up Your Layer0 Environments\n#\n\n\nIn order to destroy a Layer0 instance, you must first delete all environments in the instance.\n\n\nTo delete Layer0 environments:\n\n\n\n  \nAt the command prompt, type the following command to see a list of environments in your Layer0 instance:\n    \n\n      \nl0 environment list\n\n    \n\n\nFor each environment listed in the previous step, with the exception of the environments that begin with \"api\", issue the following command (where \n[environment_name]\n is the name of the environment you want to delete):\n    \n\n      \nl0 environment delete [environment_name] --wait\n\n    \n\n\nRepeat this step until all of the environments (except the \"api\" environments) have been deleted. When you have finished deleting the environments in your Layer0 instance, proceed to Part 2.\n  \n\n\n\n\nPart 2: Destroy the Layer0 instance\n#\n\n\nOnce you have prepared your Layer0 instance for deletion, you can use the \nl0-setup destroy\n command to destroy the instance.\n\n\nTo destroy a Layer0 instance:\n\n\n\n  \nAt the command prompt, type the following command, replacing \n[prefix]\n with the prefix you created when you created your Layer0 instance:\n    \n\n      \nl0-setup destroy [prefix]\n\n    \n\n  \n\n\n\n\n\n  \nNote\n\n  \nThe \nl0-setup destroy\n operation is idempotent (that is, it has no additional effects if you execute it multiple times with the same parameters). Therefore, if the \ndestroy\n operation fails, you may be able to make it complete by running it again. If the \ndestroy\n operation continues to fail after running it again, please contact the Xfra team at \nxfra@us.imshealth.com\n.", 
            "title": "Destroy"
        }, 
        {
            "location": "/setup/destroy/#destroying-a-layer0-instance", 
            "text": "During testing or migration, you may find that you need to delete a non-functional or outdated instance of Layer0. This section provides procedures for destroying (deleting) a Layer0 instance.", 
            "title": "Destroying a Layer0 instance"
        }, 
        {
            "location": "/setup/destroy/#part-1-clean-up-your-layer0-environments", 
            "text": "In order to destroy a Layer0 instance, you must first delete all environments in the instance.  To delete Layer0 environments:  \n   At the command prompt, type the following command to see a list of environments in your Layer0 instance:\n     \n       l0 environment list \n      For each environment listed in the previous step, with the exception of the environments that begin with \"api\", issue the following command (where  [environment_name]  is the name of the environment you want to delete):\n     \n       l0 environment delete [environment_name] --wait \n      Repeat this step until all of the environments (except the \"api\" environments) have been deleted. When you have finished deleting the environments in your Layer0 instance, proceed to Part 2.", 
            "title": "Part 1: Clean Up Your Layer0 Environments"
        }, 
        {
            "location": "/setup/destroy/#part-2-destroy-the-layer0-instance", 
            "text": "Once you have prepared your Layer0 instance for deletion, you can use the  l0-setup destroy  command to destroy the instance.  To destroy a Layer0 instance:  \n   At the command prompt, type the following command, replacing  [prefix]  with the prefix you created when you created your Layer0 instance:\n     \n       l0-setup destroy [prefix] \n     \n     \n   Note \n   The  l0-setup destroy  operation is idempotent (that is, it has no additional effects if you execute it multiple times with the same parameters). Therefore, if the  destroy  operation fails, you may be able to make it complete by running it again. If the  destroy  operation continues to fail after running it again, please contact the Xfra team at  xfra@us.imshealth.com .", 
            "title": "Part 2: Destroy the Layer0 instance"
        }, 
        {
            "location": "/guides/guestbook/", 
            "text": "Deployment guide: Guestbook sample application\n#\n\n\nIn this example, you will learn how different Layer0 commands work together to deploy web applications to the cloud. The sample application in this guide is a guestbook \n a web application that acts as a simple message board.\n\n\nBefore you start\n#\n\n\nIn order to complete the procedures in this section, you must install and configure Layer0 v0.8.4 or later. If you have not already configured Layer0, see the \ninstallation guide\n. If you are running an older version of Layer0, see the \nupgrade instructions\n.\n\n\nOnce Layer0 is configured on your computer, download the \nGuestbook Task Definition\n; save the resulting file as \nDockerrun.aws.json\n. The instructions in this section assume that this file is located in your current working directory when you run the Layer0 commands; if you place the file elsewhere, you will need to provide the complete path to the file when you reference it in \nPart 4\n.\n\n\nFor an easier deploy, \ninstall the Layer0 Terraform Plugin\n. The Layer0 Terraform Plugin makes Layer0 deployment information available to Terraform configurations (.tf files).\n\n\n\n\nDeploy with Terraform (easier)\n#\n\n\nUse the Layer0 Terraform Plugin.\n\n\nPart 1: Download the configuration files\n#\n\n\n\n\nDockerrun.aws.json\n\n\nterraform.tfvars\n\n\nlayer0.tf\n\n\n\n\nPart 2: Terraform Apply\n#\n\n\nRun \nterraform apply\n to begin the process. Terraform will prompt you for configuration values that it does not have.\n\n\nTo begin deploying the application, run the following command:\n\n\n  \nterraform apply\n\n\n\n\nTo avoid entering these values manually each time you run terraform, you can set the terraform variables by editing the \nterraform.tfvars\n file.\n\n\nvar.endpoint\n  Enter a value: \nenter your Layer0 endpoint\n\n\nvar.token\n  Enter a value: \nenter your Layer0 token\n\n\nlayer0_environment.demo: Refreshing state...\n...\n...\n...\nlayer0_service.guestbook: Creation complete\n\nApply complete! Resources: 7 added, 0 changed, 0 destroyed.\n\nThe state of your infrastructure has been saved to the path\nbelow. This state is required to modify and destroy your\ninfrastructure, so keep it safe. To inspect the complete state\nuse the `terraform show` command.\n\nState path: terraform.tfstate\n\nOutputs:\n\nguestbook_url = \nhttp endpoint for the sample application\n\n\n\n\nIt may take a few minutes for the guestbook service to launch and the load balancer to become available. During that time you may get HTTP 503 errors when making HTTP requests against the load balancer URL.\n\n\nTerraform will set up the entire environment for you and then output a link to the application's load balancer.\n\n\nWhat's happening\n#\n\n\nTerraform provisions the AWS resources (an RDS instance, VPC and subnet configurations to connect the RDS instance to the Layer0 application), configures environment variables for the application, and deploys the application into a Layer0 environment.\n\n\nYou can use Terraform with Layer0 and AWS to create \"fire-and-forget\" deployments for your applications.\n\n\nWe use these files to set up a Layer0 envrionment with Terraform:\n\n\n\n\n\n\n\n\nFilename\n\n\nPurpose\n\n\n\n\n\n\n\n\n\n\nterraform.tfvars\n\n\nVariables specific to the environment and guestbook application\n\n\n\n\n\n\nDockerrun.aws.json\n\n\nTemplate for running the guestbook application in a Layer0 environment\n\n\n\n\n\n\nlayer0.tf\n\n\nProvision Layer0 resources\n\n\n\n\n\n\n\n\nTerraform figures out the appropriate order for creating each resource and handles the entire provisioning process.\n\n\nCleanup\n#\n\n\nWhen you're finished with the example run \nterraform destroy\n in the same directory to destroy the Layer0 environment and application.\n\n\n\n\nDeploy with Layer0 CLI\n#\n\n\nYou can also use the Layer0 CLI to provision each piece of this simple application.\n\n\nPart 1: Create the environment\n#\n\n\nThe first step in deploying an application in Layer0 is to create an environment.\nAn environment is a dedicated space in which one or more services can reside. In this example, you will create an environment named \ndemo-env\n.\n\n\nTo create the environment\n\n\n\n  \nAt the command prompt, run the following command to create a new environment named \ndemo-env\n:\n    \n\n      \nl0 environment create demo-env\n\n    \n\n  \nYou will see the following output:\n\nENVIRONMENT ID  ENVIRONMENT NAME  CLUSTER COUNT  INSTANCE SIZE\ndemo00e6aa9     demo-env          0              m3.medium\n\n  \n\n\n\n\nPart 2: Create the load balancer\n#\n\n\nIn order to expose a web application to the public internet, you must create a load balancer. A load balancer listens for web traffic at a specific address, and directs that traffic to a Layer0 service.\n\n\nBy default, Layer0 load balancers listen for web traffic on port 80 and forward it on port 80 using the TCP protocol. You can modify the way in which ports are forwarded, as well as the protocol used, using the \n--port\n option.\n\n\nIn this example, you will create a new load balancer called \nguestbook-lb\n in the environment named \ndemo-env\n. The load balancer will listen on port 80, and forward traffic to port 80 in the Docker container using the HTTP protocol.\n\n\nTo create the load balancer:\n\n\n\n  \nAt the command prompt, run the following command to create a load balancer named \nguestbook-lb\n that forwards traffic on port 80 using the HTTP protocol:\n    \n\n      \nl0 loadbalancer create --port 80:80/http demo-env guestbook-lb\n\n    \n\n  \nYou will see the following output:\n\nLOADBALANCER ID  LOADBALANCER NAME  ENVIRONMENT  SERVICES  PORTS       PUBLIC  URL\nguestbodb65a     guestbook-lb       demo-env               80:80/http  true\n\n\n\n\n\n\nThe following is an summary of the arguments passed in the above command:\n\n\n\n\nloadbalancer create\n: creates a new load balancer\n\n\n--port 80:80/http\n: instructs the load balancer to forward requests from port 80 on the server to port 80 in the Docker container using the HTTP protocol\n\n\ndemo-env\n: the name of the environment in which you are creating the load balancer\n\n\nguestbook-lb\n: a name for the load balancer itself\n\n\n\n\nPart 3: Deploy the Docker task definition\n#\n\n\nThe \ndeploy\n command is used to specify the Docker task definition that refers to a web application. In this section, you will create a new deploy called \nguestbook-dep\n that refers to the \nDockerrun.aws.json\n file you created earlier.\n\n\nTo deploy the Guestbook task definition:\n\n\n\n  \nAt the command prompt, run the following command:\n    \n\n      \nl0 deploy create Dockerrun.aws.json guestbook-dep\n\n    \n\n  \nYou will see the following output:\n\nDEPLOY ID        DEPLOY NAME    VERSION\nguestbook-dep.1  guestbook-dep  1\n\n  \n\n\n\n\nThe following is an summary of the arguments passed in the above command:\n\n\n\n\ndeploy create\n: creates a new deployment and allows you to specify a Docker task definition\n\n\nDockerrun.aws.json\n: the file name of the Docker task definition (use the full path of the file if it is not in your current working directory)\n\n\nguestbook-dep\n: a name for the deploy, which you will use later when you create the service\n\n\n\n\nThe Deploy Name and Version are combined to create a unique identifier for a deploy. If you create additional deploys named \nguestbook-dep\n,  they will be assigned different version numbers.\n\n\nPart 4: Create the service\n#\n\n\nThe final part of the deployment process involves using the \nservice create\n command to create a new service and associate it with the environment, load balancer and deployment you created in the previous sections. The service will execute the containers described in the deployment. In this example, you will create a new service called \nguestbook-svc\n.\n\n\nTo create the service:\n\n\n\n  \nAt the command prompt, run the following command:\n    \n\n      \nl0 service create --loadbalancer demo-env:guestbook-lb demo-env guestbook-svc guestbook-dep:latest\n\n    \n\n  \nYou will see the following output:\n\nSERVICE ID    SERVICE NAME   ENVIRONMENT  LOADBALANCER  DEPLOYMENTS       SCALE\nguestbo9364b  guestbook-svc  demo-env     guestbodb65a  guestbook-dep:1*  0/1\n\n  \n\n\n\n\nThe following is an summary of the arguments passed in the above command:\n\n\n\n\nservice create\n: creates a new service\n\n\n--loadbalancer demo-env:guestbook-lb\n: the fully-qualified name of the load balancer; in this case, the load balancer named \nguestbook-lb\n in the environment named \ndemo-env\n. It is not strictly necessary to use the fully qualified name of the load balancer, unless another load balancer with exactly the same name exists in a different environment.\n\n\ndemo-env\n: the name of the environment you created in \nPart 1\n\n\nguestbook-svc\n: a name for the service you are creating\n\n\nguestbook-dep\n: the name of the deploy that you created in \nPart 3\n\n\n\n\nCheck the status of the service\n#\n\n\nAfter you create a service, it may take several minutes for that service to completely finish deploying. You can check the status of a service using the \nservice get\n command.\n\n\nTo check the status:\n\n\n\n  \nAt the command prompt, type the following command to check the status of the \nguestbook-svc\n deploy:\n    \n\n      \nl0 service get demo-env:guestbook-svc\n\n    \n\n\n\n\nInitially, you will see an asterisk (*) next to the name of the \nguestbook-dep:1\n deploy; this indicates that the service is in a transitional state. In this phase, if you execute the \nservice get\n command again, you will see the following output:\n\n\nSERVICE ID    SERVICE NAME   ENVIRONMENT  LOADBALANCER  DEPLOYMENTS       SCALE\nguestbo9364b  guestbook-svc  demo-env     guestbodb65a  guestbook-dep:1*  0/1\n\n\n\n\nIn the next phase of the deployment, you will see \n(1)\n in the \nScale\n column; this indicates that 1 copy of the service is transitioning to an active state. In this phase, if you execute the \nservice get\n command again, you will see the following output:\n\n\nSERVICE ID    SERVICE NAME   ENVIRONMENT  LOADBALANCER  DEPLOYMENTS       SCALE\nguestbo9364b  guestbook-svc  demo-env     guestbodb65a  guestbook-dep:1   1/1\n\n\n\n\nGet the application URL\n#\n\n\nOnce the service has been completely deployed, you can obtain the URL for your application and launch it in a browser.\n\n\nTo test your web application:\n\n\n\n  \nAt the command prompt, type the following command:\n    \n\n      \nl0 loadbalancer get demo-env:guestbook-lb\n\n    \n\n  You will see the following output:\n  \nLOADBALANCER ID  LOADBALANCER NAME  ENVIRONMENT  SERVICE        PORTS       PUBLIC  URL\nguestbodb65a     guestbook-lb       demo-env     guestbook-svc  80:80/HTTP  true    \nurl\n\n  \nCopy the value shown in the \nURL\n column and paste it into a web browser. The guestbook application will appear.\n\n\n\n\nCleanup\n#\n\n\nWhen you're finished with the example, instruct Layer0 to delete the environment to terminate the application and the Layer0 the application was using.\n\n\n    \n\n      \nl0 environment delete demo-env", 
            "title": "Guestbook"
        }, 
        {
            "location": "/guides/guestbook/#deployment-guide-guestbook-sample-application", 
            "text": "In this example, you will learn how different Layer0 commands work together to deploy web applications to the cloud. The sample application in this guide is a guestbook   a web application that acts as a simple message board.", 
            "title": "Deployment guide: Guestbook sample application"
        }, 
        {
            "location": "/guides/guestbook/#before-you-start", 
            "text": "In order to complete the procedures in this section, you must install and configure Layer0 v0.8.4 or later. If you have not already configured Layer0, see the  installation guide . If you are running an older version of Layer0, see the  upgrade instructions .  Once Layer0 is configured on your computer, download the  Guestbook Task Definition ; save the resulting file as  Dockerrun.aws.json . The instructions in this section assume that this file is located in your current working directory when you run the Layer0 commands; if you place the file elsewhere, you will need to provide the complete path to the file when you reference it in  Part 4 .  For an easier deploy,  install the Layer0 Terraform Plugin . The Layer0 Terraform Plugin makes Layer0 deployment information available to Terraform configurations (.tf files).", 
            "title": "Before you start"
        }, 
        {
            "location": "/guides/guestbook/#deploy-with-terraform-easier", 
            "text": "Use the Layer0 Terraform Plugin.", 
            "title": "Deploy with Terraform (easier)"
        }, 
        {
            "location": "/guides/guestbook/#part-1-download-the-configuration-files", 
            "text": "Dockerrun.aws.json  terraform.tfvars  layer0.tf", 
            "title": "Part 1: Download the configuration files"
        }, 
        {
            "location": "/guides/guestbook/#part-2-terraform-apply", 
            "text": "Run  terraform apply  to begin the process. Terraform will prompt you for configuration values that it does not have.  To begin deploying the application, run the following command: \n   terraform apply   To avoid entering these values manually each time you run terraform, you can set the terraform variables by editing the  terraform.tfvars  file.  var.endpoint\n  Enter a value:  enter your Layer0 endpoint \n\nvar.token\n  Enter a value:  enter your Layer0 token \n\nlayer0_environment.demo: Refreshing state...\n...\n...\n...\nlayer0_service.guestbook: Creation complete\n\nApply complete! Resources: 7 added, 0 changed, 0 destroyed.\n\nThe state of your infrastructure has been saved to the path\nbelow. This state is required to modify and destroy your\ninfrastructure, so keep it safe. To inspect the complete state\nuse the `terraform show` command.\n\nState path: terraform.tfstate\n\nOutputs:\n\nguestbook_url =  http endpoint for the sample application  It may take a few minutes for the guestbook service to launch and the load balancer to become available. During that time you may get HTTP 503 errors when making HTTP requests against the load balancer URL.  Terraform will set up the entire environment for you and then output a link to the application's load balancer.", 
            "title": "Part 2: Terraform Apply"
        }, 
        {
            "location": "/guides/guestbook/#whats-happening", 
            "text": "Terraform provisions the AWS resources (an RDS instance, VPC and subnet configurations to connect the RDS instance to the Layer0 application), configures environment variables for the application, and deploys the application into a Layer0 environment.  You can use Terraform with Layer0 and AWS to create \"fire-and-forget\" deployments for your applications.  We use these files to set up a Layer0 envrionment with Terraform:     Filename  Purpose      terraform.tfvars  Variables specific to the environment and guestbook application    Dockerrun.aws.json  Template for running the guestbook application in a Layer0 environment    layer0.tf  Provision Layer0 resources     Terraform figures out the appropriate order for creating each resource and handles the entire provisioning process.", 
            "title": "What's happening"
        }, 
        {
            "location": "/guides/guestbook/#cleanup", 
            "text": "When you're finished with the example run  terraform destroy  in the same directory to destroy the Layer0 environment and application.", 
            "title": "Cleanup"
        }, 
        {
            "location": "/guides/guestbook/#deploy-with-layer0-cli", 
            "text": "You can also use the Layer0 CLI to provision each piece of this simple application.", 
            "title": "Deploy with Layer0 CLI"
        }, 
        {
            "location": "/guides/guestbook/#part-1-create-the-environment", 
            "text": "The first step in deploying an application in Layer0 is to create an environment.\nAn environment is a dedicated space in which one or more services can reside. In this example, you will create an environment named  demo-env .  To create the environment  \n   At the command prompt, run the following command to create a new environment named  demo-env :\n     \n       l0 environment create demo-env \n     \n   You will see the following output: ENVIRONMENT ID  ENVIRONMENT NAME  CLUSTER COUNT  INSTANCE SIZE\ndemo00e6aa9     demo-env          0              m3.medium", 
            "title": "Part 1: Create the environment"
        }, 
        {
            "location": "/guides/guestbook/#part-2-create-the-load-balancer", 
            "text": "In order to expose a web application to the public internet, you must create a load balancer. A load balancer listens for web traffic at a specific address, and directs that traffic to a Layer0 service.  By default, Layer0 load balancers listen for web traffic on port 80 and forward it on port 80 using the TCP protocol. You can modify the way in which ports are forwarded, as well as the protocol used, using the  --port  option.  In this example, you will create a new load balancer called  guestbook-lb  in the environment named  demo-env . The load balancer will listen on port 80, and forward traffic to port 80 in the Docker container using the HTTP protocol.  To create the load balancer:  \n   At the command prompt, run the following command to create a load balancer named  guestbook-lb  that forwards traffic on port 80 using the HTTP protocol:\n     \n       l0 loadbalancer create --port 80:80/http demo-env guestbook-lb \n     \n   You will see the following output: LOADBALANCER ID  LOADBALANCER NAME  ENVIRONMENT  SERVICES  PORTS       PUBLIC  URL\nguestbodb65a     guestbook-lb       demo-env               80:80/http  true    The following is an summary of the arguments passed in the above command:   loadbalancer create : creates a new load balancer  --port 80:80/http : instructs the load balancer to forward requests from port 80 on the server to port 80 in the Docker container using the HTTP protocol  demo-env : the name of the environment in which you are creating the load balancer  guestbook-lb : a name for the load balancer itself", 
            "title": "Part 2: Create the load balancer"
        }, 
        {
            "location": "/guides/guestbook/#part-3-deploy-the-docker-task-definition", 
            "text": "The  deploy  command is used to specify the Docker task definition that refers to a web application. In this section, you will create a new deploy called  guestbook-dep  that refers to the  Dockerrun.aws.json  file you created earlier.  To deploy the Guestbook task definition:  \n   At the command prompt, run the following command:\n     \n       l0 deploy create Dockerrun.aws.json guestbook-dep \n     \n   You will see the following output: DEPLOY ID        DEPLOY NAME    VERSION\nguestbook-dep.1  guestbook-dep  1 \n     The following is an summary of the arguments passed in the above command:   deploy create : creates a new deployment and allows you to specify a Docker task definition  Dockerrun.aws.json : the file name of the Docker task definition (use the full path of the file if it is not in your current working directory)  guestbook-dep : a name for the deploy, which you will use later when you create the service   The Deploy Name and Version are combined to create a unique identifier for a deploy. If you create additional deploys named  guestbook-dep ,  they will be assigned different version numbers.", 
            "title": "Part 3: Deploy the Docker task definition"
        }, 
        {
            "location": "/guides/guestbook/#part-4-create-the-service", 
            "text": "The final part of the deployment process involves using the  service create  command to create a new service and associate it with the environment, load balancer and deployment you created in the previous sections. The service will execute the containers described in the deployment. In this example, you will create a new service called  guestbook-svc .  To create the service:  \n   At the command prompt, run the following command:\n     \n       l0 service create --loadbalancer demo-env:guestbook-lb demo-env guestbook-svc guestbook-dep:latest \n     \n   You will see the following output: SERVICE ID    SERVICE NAME   ENVIRONMENT  LOADBALANCER  DEPLOYMENTS       SCALE\nguestbo9364b  guestbook-svc  demo-env     guestbodb65a  guestbook-dep:1*  0/1 \n     The following is an summary of the arguments passed in the above command:   service create : creates a new service  --loadbalancer demo-env:guestbook-lb : the fully-qualified name of the load balancer; in this case, the load balancer named  guestbook-lb  in the environment named  demo-env . It is not strictly necessary to use the fully qualified name of the load balancer, unless another load balancer with exactly the same name exists in a different environment.  demo-env : the name of the environment you created in  Part 1  guestbook-svc : a name for the service you are creating  guestbook-dep : the name of the deploy that you created in  Part 3", 
            "title": "Part 4: Create the service"
        }, 
        {
            "location": "/guides/guestbook/#check-the-status-of-the-service", 
            "text": "After you create a service, it may take several minutes for that service to completely finish deploying. You can check the status of a service using the  service get  command.  To check the status:  \n   At the command prompt, type the following command to check the status of the  guestbook-svc  deploy:\n     \n       l0 service get demo-env:guestbook-svc \n       Initially, you will see an asterisk (*) next to the name of the  guestbook-dep:1  deploy; this indicates that the service is in a transitional state. In this phase, if you execute the  service get  command again, you will see the following output:  SERVICE ID    SERVICE NAME   ENVIRONMENT  LOADBALANCER  DEPLOYMENTS       SCALE\nguestbo9364b  guestbook-svc  demo-env     guestbodb65a  guestbook-dep:1*  0/1  In the next phase of the deployment, you will see  (1)  in the  Scale  column; this indicates that 1 copy of the service is transitioning to an active state. In this phase, if you execute the  service get  command again, you will see the following output:  SERVICE ID    SERVICE NAME   ENVIRONMENT  LOADBALANCER  DEPLOYMENTS       SCALE\nguestbo9364b  guestbook-svc  demo-env     guestbodb65a  guestbook-dep:1   1/1", 
            "title": "Check the status of the service"
        }, 
        {
            "location": "/guides/guestbook/#get-the-application-url", 
            "text": "Once the service has been completely deployed, you can obtain the URL for your application and launch it in a browser.  To test your web application:  \n   At the command prompt, type the following command:\n     \n       l0 loadbalancer get demo-env:guestbook-lb \n     \n  You will see the following output:\n   LOADBALANCER ID  LOADBALANCER NAME  ENVIRONMENT  SERVICE        PORTS       PUBLIC  URL\nguestbodb65a     guestbook-lb       demo-env     guestbook-svc  80:80/HTTP  true     url \n   Copy the value shown in the  URL  column and paste it into a web browser. The guestbook application will appear.", 
            "title": "Get the application URL"
        }, 
        {
            "location": "/guides/guestbook/#cleanup_1", 
            "text": "When you're finished with the example, instruct Layer0 to delete the environment to terminate the application and the Layer0 the application was using. \n     \n       l0 environment delete demo-env", 
            "title": "Cleanup"
        }, 
        {
            "location": "/guides/guestbook_rds/", 
            "text": "Deployment guide: Guestbook with RDS Database\n#\n\n\nThe Guestbook application that you deployed in the \nGuestbook deployment guide\n was a very simple application that stored its data in memory (also known as a \"stateful\" application). If you were to re-deploy the Guestbook service, all of the data previously entered into the application would be lost permanently.\n\n\nA stateless application, on the other hand, does not record data generated in one session for use in subsequent sessions. In order to prevent data loss, web applications that you deploy using Layer0 should be stateless.\n\n\nThis guide will show you how to make a stateless Guestbook application that stores data in an Amazon Relational Database Service (RDS) database.\n\n\n\n\nBefore you start\n#\n\n\nIn order to complete the procedures in this section, you must install and configure Layer0. If you have not already configured Layer0, see the \ninstallation guide\n.\n\n\nInstall the Layer0 Terraform Plugin\n. The Layer0 Terraform Plugin makes Layer0 deployment information (like VPCs and subnets) available to Terraform configurations (.tf files).\n\n\nStep 1: Download the configuration files\n#\n\n\n\n\nDockerrun.aws.json\n\n\nterraform.tfvars\n\n\nlayer0.tf\n\n\nrds.tf\n\n\n\n\nStep 2: Terraform Apply\n#\n\n\nRun \nterraform apply\n to begin the process. Terraform will prompt you for configuration values that it does not have.\n\n\nTo begin deploying the application, run the following command:\n\n\n  \nterraform apply\n\n\n\n\nTo avoid entering these values manually each time you run terraform, you can set the terraform variables by editing the \nterraform.tfvars\n file.\n\n\nvar.access_key\n  Enter a value: [your AWS access key]\n\nvar.endpoint\n  Enter a value: [your Layer0 endpoint]\n\nvar.secret_key\n  Enter a value: [your AWS secret key]\n\nvar.token\n  Enter a value: [your Layer0 token]\n\nlayer0_environment.demo: Refreshing state...\n...\n...\n...\nlayer0_service.guestbook: Creation complete\n\nApply complete! Resources: 7 added, 0 changed, 0 destroyed.\n\nThe state of your infrastructure has been saved to the path\nbelow. This state is required to modify and destroy your\ninfrastructure, so keep it safe. To inspect the complete state\nuse the `terraform show` command.\n\nState path: terraform.tfstate\n\nOutputs:\n\nguestbook_url = \nhttp endpoint for the sample application\n\n\n\n\nIt may take a few minutes for the guestbook service to launch and the load balancer to become available. During that time you may get HTTP 503 errors when making HTTP requests against the load balancer URL.\n\n\nTerraform will set up the entire environment for you and then output a link to the application's load balancer.\n\n\nWhat's happening\n#\n\n\nTerraform provisions the AWS resources (an RDS instance, VPC and subnet configurations to connect the RDS instance to the Layer0 application), configures environment variables for the application, and deploys the application into a Layer0 environment.\n\n\nYou can use Terraform with Layer0 and AWS to create fire and forget deployments for your applications.\n\n\nWe use these files to set up a Layer0 envrionment with Terraform\n\n\n\n\n\n\n\n\nFilename\n\n\nPurpose\n\n\n\n\n\n\n\n\n\n\nterraform.tfvars\n\n\nVariables specific to the environment and guestbook application\n\n\n\n\n\n\nDockerrun.aws.json\n\n\nTemplate for running the guestbook application in a Layer0 environment\n\n\n\n\n\n\nlayer0.tf\n\n\nProvision Layer0 resources and populate variables in \nDockerrun.aws.json\n\n\n\n\n\n\nrds.tf\n\n\nCreate the RDS instance and link it to the Layer0 environment's network\n\n\n\n\n\n\n\n\nTerraform figures out the appropriate order for creating each resource and handles the entire provisioning process.\n\n\nCleanup\n#\n\n\nWhen you're finished with the example run \nterraform destroy\n in the same directory to destroy the AWS resources, Layer0 environment, and application.", 
            "title": "Guestbook with RDS"
        }, 
        {
            "location": "/guides/guestbook_rds/#deployment-guide-guestbook-with-rds-database", 
            "text": "The Guestbook application that you deployed in the  Guestbook deployment guide  was a very simple application that stored its data in memory (also known as a \"stateful\" application). If you were to re-deploy the Guestbook service, all of the data previously entered into the application would be lost permanently.  A stateless application, on the other hand, does not record data generated in one session for use in subsequent sessions. In order to prevent data loss, web applications that you deploy using Layer0 should be stateless.  This guide will show you how to make a stateless Guestbook application that stores data in an Amazon Relational Database Service (RDS) database.", 
            "title": "Deployment guide: Guestbook with RDS Database"
        }, 
        {
            "location": "/guides/guestbook_rds/#before-you-start", 
            "text": "In order to complete the procedures in this section, you must install and configure Layer0. If you have not already configured Layer0, see the  installation guide .  Install the Layer0 Terraform Plugin . The Layer0 Terraform Plugin makes Layer0 deployment information (like VPCs and subnets) available to Terraform configurations (.tf files).", 
            "title": "Before you start"
        }, 
        {
            "location": "/guides/guestbook_rds/#step-1-download-the-configuration-files", 
            "text": "Dockerrun.aws.json  terraform.tfvars  layer0.tf  rds.tf", 
            "title": "Step 1: Download the configuration files"
        }, 
        {
            "location": "/guides/guestbook_rds/#step-2-terraform-apply", 
            "text": "Run  terraform apply  to begin the process. Terraform will prompt you for configuration values that it does not have.  To begin deploying the application, run the following command: \n   terraform apply   To avoid entering these values manually each time you run terraform, you can set the terraform variables by editing the  terraform.tfvars  file.  var.access_key\n  Enter a value: [your AWS access key]\n\nvar.endpoint\n  Enter a value: [your Layer0 endpoint]\n\nvar.secret_key\n  Enter a value: [your AWS secret key]\n\nvar.token\n  Enter a value: [your Layer0 token]\n\nlayer0_environment.demo: Refreshing state...\n...\n...\n...\nlayer0_service.guestbook: Creation complete\n\nApply complete! Resources: 7 added, 0 changed, 0 destroyed.\n\nThe state of your infrastructure has been saved to the path\nbelow. This state is required to modify and destroy your\ninfrastructure, so keep it safe. To inspect the complete state\nuse the `terraform show` command.\n\nState path: terraform.tfstate\n\nOutputs:\n\nguestbook_url =  http endpoint for the sample application  It may take a few minutes for the guestbook service to launch and the load balancer to become available. During that time you may get HTTP 503 errors when making HTTP requests against the load balancer URL.  Terraform will set up the entire environment for you and then output a link to the application's load balancer.", 
            "title": "Step 2: Terraform Apply"
        }, 
        {
            "location": "/guides/guestbook_rds/#whats-happening", 
            "text": "Terraform provisions the AWS resources (an RDS instance, VPC and subnet configurations to connect the RDS instance to the Layer0 application), configures environment variables for the application, and deploys the application into a Layer0 environment.  You can use Terraform with Layer0 and AWS to create fire and forget deployments for your applications.  We use these files to set up a Layer0 envrionment with Terraform     Filename  Purpose      terraform.tfvars  Variables specific to the environment and guestbook application    Dockerrun.aws.json  Template for running the guestbook application in a Layer0 environment    layer0.tf  Provision Layer0 resources and populate variables in  Dockerrun.aws.json    rds.tf  Create the RDS instance and link it to the Layer0 environment's network     Terraform figures out the appropriate order for creating each resource and handles the entire provisioning process.", 
            "title": "What's happening"
        }, 
        {
            "location": "/guides/guestbook_rds/#cleanup", 
            "text": "When you're finished with the example run  terraform destroy  in the same directory to destroy the AWS resources, Layer0 environment, and application.", 
            "title": "Cleanup"
        }, 
        {
            "location": "/guides/consul/", 
            "text": "Deployment guide: Consul service\n#\n\n\nConsul is a tool for configuring services in your infrastructure. It includes several features, including health monitoring, service discovery and key/value storage.\n\n\nThis guide provides step-by-step instructions for deploying Consul in a Layer0 instance. These procedures build upon the \nGuestbook\n and \nGuestbook with RDS\n deployment guides; you must complete the procedures in those guides before you can complete the procedures in this guide.\n\n\nBefore you start\n#\n\n\nIn order to complete the procedures in this section, you must install and configure Layer0 v0.8.4 or later. If you have not already configured Layer0, see the \ninstallation guide\n. If you are running an older version of Layer0, see the \nupgrade instructions\n.\n\n\nThis guide expands upon the \nGuestbook with RDS Database deployment guide\n. The procedures in this guide assume that you completed the Guestbook with RDS deployment guide and all of its prerequisites.\n\n\nPart 1: Create a load balancer\n#\n\n\nConsul should run behind a private load balancer in the \ndemo\n environment with ports 8500 and 8301 exposed.\n\n\nTo create the load balancer:\n\n\n\n  \nAt the command line, type the following command to create a load balancer named \nconsullb\n in the \ndemo\n environment with port 8500 exposed:\n    \n\n      \nl0 loadbalancer create --private --port 8500:8500/tcp demo consullb\n\n    \nYou will see the following output:\n\nLOADBALANCER ID  LOADBALANCER NAME  ENVIRONMENT  SERVICES  PORTS          PUBLIC  URL\n1consullb        consullb           demo                   8500:8500/tcp  false\n\n\n  \n\n  \nAt the command line, type the following command to add port 8301 to the \nconsullb\n load balancer:\n    \n\n      \nl0 loadbalancer addport demo:consullb 8301:8301/tcp\n\n    \n\n  You will see the following output:\n\nLOADBALANCER ID  LOADBALANCER NAME  ENVIRONMENT  SERVICES  PORTS          PUBLIC  URL\n1consullb        consullb           demo                   8500:8500/tcp  false   \n(url)\n\n                                                           8301:8301/tcp\n\n\n\n\nCopy the URL listed in the \nURL\n column; you will need this URL in the next section.\n\n\n\n\nPart 2: Configure the deploy\n#\n\n\n\n  \nDownload the \nConsul Task Definition\n and save it to your computer as Consul.Dockerrun.aws.json.\n\n  \nOpen Consul.Dockerrun.aws.json in a text editor. Toward the bottom of the file, you will see the following:\n\n\"environment\": [\n    {\n        \"name\": \"EXTERNAL_URL\",\n        \"value\": \"\nurl\n\"\n     }\n]\n\n\n\n\nReplace \nurl\n with the URL you copied in step 2 of the previous section, and then save the file.\n\n  \nAt the command line, type the following command to create a new deploy called \nconsul\n:\n    \n\n      \nl0 deploy create Consul.Dockerrun.aws.json consul\n\n    \n\n  You will see the following output:\n\nDEPLOY ID  DEPLOY NAME  VERSION\n1consul:1  consul       1\n\n\n  \n\n\n\n\nPart 3: Create the service\n#\n\n\nNow that you've created an environment, load balancer, and deploy, you can create a service to bring these elements together.\n\n\nTo create the service:\n\n\n\n  \nAt the command line, type the following command to create a new service called \nconsul\n:\n    \n\n      \nl0 service create --loadbalancer demo:consullb demo consulsvc consul:latest\n\n    \n\n    You will see the following output:\n\nSERVICE ID  SERVICE NAME  ENVIRONMENT  LOADBALANCER  DEPLOYS      SCALE\n1consul     consulsvc     demo         consullb      consul:1     0/1\n\n  \n\n  \nWait several minutes for the service to be provisioned. You can check the status of the service creation by running the following command:\n  \n\n    \nl0 service get consulsvc\n\n  \n\n  When the service has finished provisioning, you will see the following output:\n\n\nSERVICE ID  SERVICE NAME  ENVIRONMENT  LOADBALANCER  DEPLOYS      SCALE\n1consul     consulsvc     demo         consullb      consul:1     1/1\n\n\n\n\n\n\n\n\nPart 4: Scale the Consul service\n#\n\n\nConsul is a scalable application. For added reliability, we recommend that you scale the Consul service to size 3.\n\n\nTo scale the service:\n\n\n\n  \nAt the command line, type the following to scale the consul service to size 3:\n    \n\n      \nl0 service scale demo:consulsvc 3\n\n    \n\n  \n\n\n  \nWait several minutes for the service to scale. You can check the status of the service by running the following command:\n  \n\n    \nl0 service get consul\n\n  \n\n  When the Service has finished scaling, you will see the following output:\n\nSERVICE ID  SERVICE NAME  ENVIRONMENT  LOADBALANCER  DEPLOYS      SCALE\n1consul     consulsvc     demo         consullb      consul:1     3/3\n\n\n  \n\n\n\n\n\nAdditional steps: Configure Layer0 services\n#\n\n\nIn order to for Layer0 services to use consul, the task definitions for those services must be configured. The next deployment guide in this series (\nGuestbook with Consul\n) contains an example of these configurations.", 
            "title": "Consul"
        }, 
        {
            "location": "/guides/consul/#deployment-guide-consul-service", 
            "text": "Consul is a tool for configuring services in your infrastructure. It includes several features, including health monitoring, service discovery and key/value storage.  This guide provides step-by-step instructions for deploying Consul in a Layer0 instance. These procedures build upon the  Guestbook  and  Guestbook with RDS  deployment guides; you must complete the procedures in those guides before you can complete the procedures in this guide.", 
            "title": "Deployment guide: Consul service"
        }, 
        {
            "location": "/guides/consul/#before-you-start", 
            "text": "In order to complete the procedures in this section, you must install and configure Layer0 v0.8.4 or later. If you have not already configured Layer0, see the  installation guide . If you are running an older version of Layer0, see the  upgrade instructions .  This guide expands upon the  Guestbook with RDS Database deployment guide . The procedures in this guide assume that you completed the Guestbook with RDS deployment guide and all of its prerequisites.", 
            "title": "Before you start"
        }, 
        {
            "location": "/guides/consul/#part-1-create-a-load-balancer", 
            "text": "Consul should run behind a private load balancer in the  demo  environment with ports 8500 and 8301 exposed.  To create the load balancer:  \n   At the command line, type the following command to create a load balancer named  consullb  in the  demo  environment with port 8500 exposed:\n     \n       l0 loadbalancer create --private --port 8500:8500/tcp demo consullb \n     You will see the following output: LOADBALANCER ID  LOADBALANCER NAME  ENVIRONMENT  SERVICES  PORTS          PUBLIC  URL\n1consullb        consullb           demo                   8500:8500/tcp  false \n   \n   At the command line, type the following command to add port 8301 to the  consullb  load balancer:\n     \n       l0 loadbalancer addport demo:consullb 8301:8301/tcp \n     \n  You will see the following output: LOADBALANCER ID  LOADBALANCER NAME  ENVIRONMENT  SERVICES  PORTS          PUBLIC  URL\n1consullb        consullb           demo                   8500:8500/tcp  false    (url) \n                                                           8301:8301/tcp  \nCopy the URL listed in the  URL  column; you will need this URL in the next section.", 
            "title": "Part 1: Create a load balancer"
        }, 
        {
            "location": "/guides/consul/#part-2-configure-the-deploy", 
            "text": "Download the  Consul Task Definition  and save it to your computer as Consul.Dockerrun.aws.json. \n   Open Consul.Dockerrun.aws.json in a text editor. Toward the bottom of the file, you will see the following: \"environment\": [\n    {\n        \"name\": \"EXTERNAL_URL\",\n        \"value\": \" url \"\n     }\n]  \nReplace  url  with the URL you copied in step 2 of the previous section, and then save the file. \n   At the command line, type the following command to create a new deploy called  consul :\n     \n       l0 deploy create Consul.Dockerrun.aws.json consul \n     \n  You will see the following output: DEPLOY ID  DEPLOY NAME  VERSION\n1consul:1  consul       1", 
            "title": "Part 2: Configure the deploy"
        }, 
        {
            "location": "/guides/consul/#part-3-create-the-service", 
            "text": "Now that you've created an environment, load balancer, and deploy, you can create a service to bring these elements together.  To create the service:  \n   At the command line, type the following command to create a new service called  consul :\n     \n       l0 service create --loadbalancer demo:consullb demo consulsvc consul:latest \n     \n    You will see the following output: SERVICE ID  SERVICE NAME  ENVIRONMENT  LOADBALANCER  DEPLOYS      SCALE\n1consul     consulsvc     demo         consullb      consul:1     0/1 \n   \n   Wait several minutes for the service to be provisioned. You can check the status of the service creation by running the following command:\n   \n     l0 service get consulsvc \n   \n  When the service has finished provisioning, you will see the following output:  SERVICE ID  SERVICE NAME  ENVIRONMENT  LOADBALANCER  DEPLOYS      SCALE\n1consul     consulsvc     demo         consullb      consul:1     1/1", 
            "title": "Part 3: Create the service"
        }, 
        {
            "location": "/guides/consul/#part-4-scale-the-consul-service", 
            "text": "Consul is a scalable application. For added reliability, we recommend that you scale the Consul service to size 3.  To scale the service:  \n   At the command line, type the following to scale the consul service to size 3:\n     \n       l0 service scale demo:consulsvc 3 \n     \n   \n\n   Wait several minutes for the service to scale. You can check the status of the service by running the following command:\n   \n     l0 service get consul \n   \n  When the Service has finished scaling, you will see the following output: SERVICE ID  SERVICE NAME  ENVIRONMENT  LOADBALANCER  DEPLOYS      SCALE\n1consul     consulsvc     demo         consullb      consul:1     3/3", 
            "title": "Part 4: Scale the Consul service"
        }, 
        {
            "location": "/guides/consul/#additional-steps-configure-layer0-services", 
            "text": "In order to for Layer0 services to use consul, the task definitions for those services must be configured. The next deployment guide in this series ( Guestbook with Consul ) contains an example of these configurations.", 
            "title": "Additional steps: Configure Layer0 services"
        }, 
        {
            "location": "/guides/guestbook_consul/", 
            "text": "Deployment guide: Guestbook with Consul\n#\n\n\nThis guide provides step-by-step instructions for deploying a Guestbook application that stores data in a\n\nRedis\n database. The Guestbook application uses \nConsul\n to dynamically discover the Redis service.\n\n\nBefore you start\n#\n\n\nThis guide assumes that you are running Layer0 version 0.7.2 or later, and that you have completed the\n\nGuestbook\n and \nConsul\n deployment guides.\n\n\nPart 1: Configure and deploy the Redis task definition\n#\n\n\nThe updated Guestbook application in this guide stores its data in a Redis database. Before you can deploy the updated Guestbook application, you must first configure and deploy the Redis task definition.\n\n\nTo configure and deploy the task definition:\n\n\n\n  \nDownload the \nRedis task\ndefinition\n and save it to your computer as Redis.dockerrun.aws.json.\n\n  \nAt the command prompt, type the following command:\n    \n\n      \nl0 loadbalancer get consul\n\n    \n\n    Copy the value in the \nURL\n column; you will need it in the next step.\n  \n\n  \nOpen Redis.dockerrun.aws.json in a text editor. Toward the end of the file, you will see the following:\n\n\"environment\": [\n    {\n        \"name\": \"EXTERNAL_URL\",\n        \"value\": \"\nurl\n\"\n    }\n]\n\n    In this section, replace \nurl\n with the URL you copied in the previous step.\n  \n\n  \nAt the command prompt, type the following command to create a new deploy named \nredis\n that uses the Redis.dockerrun.aws task definition:\n    \n\n      \nl0 deploy create Redis.Dockerrun.aws.json redis\n\n    \n\n  You will see the following output:\n\nDEPLOY ID  DEPLOY NAME  VERSION\n1redis:1   redis        1\n\n  \n\n\n\n\nPart 2: Create the redis service\n#\n\n\nNow that you have created the \nredis\n deploy, you can create a service to place it in.\n\n\nTo create a new service:\n\n\n\n  \nAt the command line, type the following command to create a service named \nredis\n running the deploy you created in the previous section:\n    \n\n      \n \nl0 service create demo redis redis:latest\n\n    \n\n    When you execute this command, the \nRegistrator\n service will read the environment variables for the l0-demo-redis container. These variables include the following:\n\n\nenvironment\n: [\n    {\n        \nname\n: \nSERVICE_NAME\n,\n        \nvalue\n: \ndb\n\n    },\n    {\n        \nname\n: \nSERVICE_TAGS\n,\n        \nvalue\n: \nguestbook\n\n    },\n    ...\n\n\n\n\nAfter reading these variables, Registrator will register a service named \ndb\n into your environment's Consul service.\nMembers of the Consul Cluster will be able to discover this service via DNS queries to guestbook.db.service.consul.\n  \n\n\n\n\nPart 3: Update the Guestbook service\n#\n\n\nTo configure the Guestbook application to use Consul for service discovery, you must first update the \nguestbook\n deploy.\n\n\nTo update the Guestbook service:\n\n\n\n  \nDownload the \nGuestbook with Consul Task definition\n and save it to your computer as GuestbookConsul.Dockerrun.aws.json.\n\n  \nAt the command line, type the following command: \nl0 loadbalancer get consullb\n. Copy the value in the \nURL\n column.\n\n  \nOpen GuestbookConsul.Dockerrun.aws.json in a text editor. Toward the bottom of the file, in the \nenvironment\n section, replace \nurl\n with the URL\nthat you copied in the previous step. Save the file.\n\n  \nAt the command line, type the following command to create a new version of the guestbook deploy using the updated Guestbook task definition:\n    \n\n      \nl0 deploy create GuestbookConsul.Dockerrun.aws.json guestbook\n\n    \n\n  You will see the following output:\n\n\nDEPLOY ID     DEPLOY NAME  VERSION\n1guestbook:3  guestbook    3\n\n\n  \n\n  \nAt the command line, type the following command to apply the updated \nguestbook\n deploy on the \nguestbook\n service:\n    \n\n      \nl0 deploy apply guestbook:latest demo:guestbook\n\n    \n\n  When you execute this command, the Guestbook application will automatically discover the Redis service by making a DNS query to guestbook.db.service.consul.\n\n  \nAt the command prompt, type the following command to find the URL of the \nguestbooklb\n load balancer:\n    \n\n      \nl0 loadbalancer get guestbooklb\n\n    \n\n  If the service has not finished deploying, you may see the following message:\n\"Could not connect to redis server, try refreshing.\"\n\nIf you see this message, wait a few minutes, and then refresh the page. You can also try using your web browser's Incognito or Private Browsing mode to ensure that the page is not cached.", 
            "title": "Guestbook with Consul"
        }, 
        {
            "location": "/guides/guestbook_consul/#deployment-guide-guestbook-with-consul", 
            "text": "This guide provides step-by-step instructions for deploying a Guestbook application that stores data in a Redis  database. The Guestbook application uses  Consul  to dynamically discover the Redis service.", 
            "title": "Deployment guide: Guestbook with Consul"
        }, 
        {
            "location": "/guides/guestbook_consul/#before-you-start", 
            "text": "This guide assumes that you are running Layer0 version 0.7.2 or later, and that you have completed the Guestbook  and  Consul  deployment guides.", 
            "title": "Before you start"
        }, 
        {
            "location": "/guides/guestbook_consul/#part-1-configure-and-deploy-the-redis-task-definition", 
            "text": "The updated Guestbook application in this guide stores its data in a Redis database. Before you can deploy the updated Guestbook application, you must first configure and deploy the Redis task definition.  To configure and deploy the task definition:  \n   Download the  Redis task\ndefinition  and save it to your computer as Redis.dockerrun.aws.json. \n   At the command prompt, type the following command:\n     \n       l0 loadbalancer get consul \n     \n    Copy the value in the  URL  column; you will need it in the next step.\n   \n   Open Redis.dockerrun.aws.json in a text editor. Toward the end of the file, you will see the following: \"environment\": [\n    {\n        \"name\": \"EXTERNAL_URL\",\n        \"value\": \" url \"\n    }\n] \n    In this section, replace  url  with the URL you copied in the previous step.\n   \n   At the command prompt, type the following command to create a new deploy named  redis  that uses the Redis.dockerrun.aws task definition:\n     \n       l0 deploy create Redis.Dockerrun.aws.json redis \n     \n  You will see the following output: DEPLOY ID  DEPLOY NAME  VERSION\n1redis:1   redis        1", 
            "title": "Part 1: Configure and deploy the Redis task definition"
        }, 
        {
            "location": "/guides/guestbook_consul/#part-2-create-the-redis-service", 
            "text": "Now that you have created the  redis  deploy, you can create a service to place it in.  To create a new service:  \n   At the command line, type the following command to create a service named  redis  running the deploy you created in the previous section:\n     \n         l0 service create demo redis redis:latest \n     \n    When you execute this command, the  Registrator  service will read the environment variables for the l0-demo-redis container. These variables include the following:  environment : [\n    {\n         name :  SERVICE_NAME ,\n         value :  db \n    },\n    {\n         name :  SERVICE_TAGS ,\n         value :  guestbook \n    },\n    ...  After reading these variables, Registrator will register a service named  db  into your environment's Consul service.\nMembers of the Consul Cluster will be able to discover this service via DNS queries to guestbook.db.service.consul.", 
            "title": "Part 2: Create the redis service"
        }, 
        {
            "location": "/guides/guestbook_consul/#part-3-update-the-guestbook-service", 
            "text": "To configure the Guestbook application to use Consul for service discovery, you must first update the  guestbook  deploy.  To update the Guestbook service:  \n   Download the  Guestbook with Consul Task definition  and save it to your computer as GuestbookConsul.Dockerrun.aws.json. \n   At the command line, type the following command:  l0 loadbalancer get consullb . Copy the value in the  URL  column. \n   Open GuestbookConsul.Dockerrun.aws.json in a text editor. Toward the bottom of the file, in the  environment  section, replace  url  with the URL\nthat you copied in the previous step. Save the file. \n   At the command line, type the following command to create a new version of the guestbook deploy using the updated Guestbook task definition:\n     \n       l0 deploy create GuestbookConsul.Dockerrun.aws.json guestbook \n     \n  You will see the following output: DEPLOY ID     DEPLOY NAME  VERSION\n1guestbook:3  guestbook    3 \n   \n   At the command line, type the following command to apply the updated  guestbook  deploy on the  guestbook  service:\n     \n       l0 deploy apply guestbook:latest demo:guestbook \n     \n  When you execute this command, the Guestbook application will automatically discover the Redis service by making a DNS query to guestbook.db.service.consul. \n   At the command prompt, type the following command to find the URL of the  guestbooklb  load balancer:\n     \n       l0 loadbalancer get guestbooklb \n     \n  If the service has not finished deploying, you may see the following message:\n\"Could not connect to redis server, try refreshing.\" \nIf you see this message, wait a few minutes, and then refresh the page. You can also try using your web browser's Incognito or Private Browsing mode to ensure that the page is not cached.", 
            "title": "Part 3: Update the Guestbook service"
        }, 
        {
            "location": "/guides/one_off_task/", 
            "text": "Deployment guide: Guestbook one-off task\n#\n\n\nIn this example, you will learn how to use layer0 to run a one-off task. In this case, it will be to run a task to restore the \nguestbook application\n from a backup.\n\n\n\n\nBefore you start\n#\n\n\nIn order to complete the procedures in this section, you must install and configure Layer0 v0.8.4 or later. If you have not already configured Layer0, see the \ninstallation guide\n. If you are running an older version of Layer0, see the \nupgrade instructions\n.\n\n\nThis guide expands upon the \nGuestbook deployment guide\n deployment guide. You must complete the procedures in that guide before you can complete the procedures listed here. After completing the procedures in the Guestbook guide, your Layer0 should contain a service named \"guestbooksvc\", running a deploy named \"guestbook\", behind a load balancer named \"guestbooklb\", all within an environment named \"demo\".\n\n\nPart 1: Prepare the task definition\n#\n\n\n\n\nDownload the \nGuestbook One-off Task Definition\n and save it to your computer as \nGuestbookRestore.Dockerrun.aws.json\n.\n\n\nEdit the \nGUESTBOOK_URL\n environment variable for the \nl0-guestbook-restore\n container to the url of the loadbalancer running your guestbook application. This url can be obtained by looking at the output of the command\n\n\n\n\nl0 loadbalancer get guestbooklb\n\n\n\n\nEdit the \nBACKUP_FILE_URL\n environment variable for the \nl0-guestbook-restore\n container to the url of the backup file that you wish to restore from. A sample backup file is provided at \nhttps://github.com/quintilesims/layer0-examples/raw/master/1offtask/backup.txt\n.\n\n\n\n\nPart 2: Create a deploy\n#\n\n\nNext, you will create a new deploy for the task.\n\n\nTo create a new deploy:\n\n\nAt the command prompt, run the following command:\n\n\nl0 deploy create GuestbookRestore.Dockerrun.aws.json guestbookrestore\n\n\nYou will see the following output:\n\n\nDEPLOY ID           DEPLOY NAME        VERSION\nguestbookrestore.1  guestbookrestore   1\n\n\n\n\nPart 3: Create the task\n#\n\n\nAt this point, you can use the \ntask create\n command to begin an instance of the task deployed above. This task requires two environment variables to be supplied to the \nl0-guestbook-restore\n container: \nGUESTBOOK_URL\n and \nBACKUP_FILE_URL\n. These were collected in Part 1 of this guide.\n\n\nTo run the task, use the following command:\n\n\nl0 task create demo guestbookrestore guestbookrestore\n\n\nYou will see the following output:\n\n\nTASK ID       TASK NAME         ENVIRONMENT  DEPLOY              SCALE\nguestbo851c9  guestbookrestore  demo         guestbookrestore:2  0/1 (1)\n\n\n\n\nPart 4: Wait for the task to complete\n#\n\n\nCheck the logs for the task\n#\n\n\nTo see the logs for this task, and evaluate progress, use the command:\n\n\nl0 task logs guestbookrestore\n\n\nOnce it has completed, check your guestbook url, and note that the entries have been replaced with the contents of your backup file.", 
            "title": "One-off Task"
        }, 
        {
            "location": "/guides/one_off_task/#deployment-guide-guestbook-one-off-task", 
            "text": "In this example, you will learn how to use layer0 to run a one-off task. In this case, it will be to run a task to restore the  guestbook application  from a backup.", 
            "title": "Deployment guide: Guestbook one-off task"
        }, 
        {
            "location": "/guides/one_off_task/#before-you-start", 
            "text": "In order to complete the procedures in this section, you must install and configure Layer0 v0.8.4 or later. If you have not already configured Layer0, see the  installation guide . If you are running an older version of Layer0, see the  upgrade instructions .  This guide expands upon the  Guestbook deployment guide  deployment guide. You must complete the procedures in that guide before you can complete the procedures listed here. After completing the procedures in the Guestbook guide, your Layer0 should contain a service named \"guestbooksvc\", running a deploy named \"guestbook\", behind a load balancer named \"guestbooklb\", all within an environment named \"demo\".", 
            "title": "Before you start"
        }, 
        {
            "location": "/guides/one_off_task/#part-1-prepare-the-task-definition", 
            "text": "Download the  Guestbook One-off Task Definition  and save it to your computer as  GuestbookRestore.Dockerrun.aws.json .  Edit the  GUESTBOOK_URL  environment variable for the  l0-guestbook-restore  container to the url of the loadbalancer running your guestbook application. This url can be obtained by looking at the output of the command   l0 loadbalancer get guestbooklb   Edit the  BACKUP_FILE_URL  environment variable for the  l0-guestbook-restore  container to the url of the backup file that you wish to restore from. A sample backup file is provided at  https://github.com/quintilesims/layer0-examples/raw/master/1offtask/backup.txt .", 
            "title": "Part 1: Prepare the task definition"
        }, 
        {
            "location": "/guides/one_off_task/#part-2-create-a-deploy", 
            "text": "Next, you will create a new deploy for the task.  To create a new deploy:  At the command prompt, run the following command:  l0 deploy create GuestbookRestore.Dockerrun.aws.json guestbookrestore  You will see the following output:  DEPLOY ID           DEPLOY NAME        VERSION\nguestbookrestore.1  guestbookrestore   1", 
            "title": "Part 2: Create a deploy"
        }, 
        {
            "location": "/guides/one_off_task/#part-3-create-the-task", 
            "text": "At this point, you can use the  task create  command to begin an instance of the task deployed above. This task requires two environment variables to be supplied to the  l0-guestbook-restore  container:  GUESTBOOK_URL  and  BACKUP_FILE_URL . These were collected in Part 1 of this guide.  To run the task, use the following command:  l0 task create demo guestbookrestore guestbookrestore  You will see the following output:  TASK ID       TASK NAME         ENVIRONMENT  DEPLOY              SCALE\nguestbo851c9  guestbookrestore  demo         guestbookrestore:2  0/1 (1)", 
            "title": "Part 3: Create the task"
        }, 
        {
            "location": "/guides/one_off_task/#part-4-wait-for-the-task-to-complete", 
            "text": "", 
            "title": "Part 4: Wait for the task to complete"
        }, 
        {
            "location": "/guides/one_off_task/#check-the-logs-for-the-task", 
            "text": "To see the logs for this task, and evaluate progress, use the command:  l0 task logs guestbookrestore  Once it has completed, check your guestbook url, and note that the entries have been replaced with the contents of your backup file.", 
            "title": "Check the logs for the task"
        }, 
        {
            "location": "/reference/cli/", 
            "text": "Layer0 CLI Reference\n#\n\n\nGlobal options\n#\n\n\nThe \nl0\n application is designed to be used with one of several subcommands: \nadmin\n, \ndeploy\n, \nenvironment\n, \njob\n, \nloadbalancer\n, \nservice\n, and \ntask\n. These subcommands are detailed in the sections below. There are, however, some global parameters that you may specify when using \nl0\n.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0\n [\nglobalOptions\n] \ncommand\n \nsubcommand\n [\noptions\n] [\nparameters\n]\n\n  \n\n\n\n\nOptional arguments\n#\n\n\n\n  \n\n    \n--output {text|json}\n\n    \nSpecify the format of Layer0 outputs. By default, Layer0 outputs unformatted text; by issuing the \n--output json\n option, you can force \nl0\n to output JSON-formatted text.\n\n  \n\n  \n\n    \n--version\n\n    \nDisplay the version number of the \nl0\n application.\n\n  \n\n\n\n\n\n\nAdmin\n#\n\n\nThe \nadmin\n command is used to manage the Layer0 API server. This command is used with the following subcommands: \ndebug\n, \nsql\n, and \nversion\n.\n\n\nadmin debug\n#\n\n\nUse the \ndebug\n subcommand to view the running version of your Layer0 API server and CLI.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 admin debug\n\n  \n\n\n\n\nadmin sql\n#\n\n\nUse the \nsql\n subcommand to initialize the Layer0 API database.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 admin sql\n\n  \n\n\n\n\nAdditional information\n#\n\n\n\n  \n\n    \nThe \nsql\n subcommand is automatically executed during the Layer0 installation process; we recommend that you do not use this subcommand unless specifically directed to do so.\n\n  \n\n\n\n\nadmin version\n#\n\n\nUse the \nversion\n subcommand to display the current version of the Layer0 API.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 admin version\n\n  \n\n\n\n\n\n\nDeploy\n#\n\n\ndeploy create\n#\n\n\nUse the \ncreate\n subcommand to upload a Docker task definition into Layer0. This command is used with the following subcommands: \ncreate\n, \ndelete\n, \nget\n and \nlist\n.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 deploy create\n \ndockerPath\n \ndeployName\n\n  \n\n\n\n\nRequired parameters\n#\n\n\n\n  \n\n    \ndockerPath\n\n    \nThe path to the Docker task definition that you want to upload.\n\n  \n\n  \n\n    \ndeployName\n\n    \nA name for the deploy.\n\n  \n\n\n\n\nAdditional information\n#\n\n\n\n  \n\n    \nIf \ndeployName\n exactly matches the name of an existing Layer0 deploy, then the version number of that deploy will increase by 1, and the task definition you specified will replace the task definition specified in the previous version.\n\n  \n \n\n  \n\n    \nIf you use Visual Studio to modify or create your Dockerrun file, you may see an \"Invalid Dockerrun.aws.json\" error. This error is caused by the default encoding used by Visual Studio. See the \n\"Common issues\" page\n for steps to resolve this issue.\n\n  \n  \n\n  \n\n    \n\nDeploys created through Layer0 are rendered with a \nlogConfiguration\n section for each container.\nIf a \nlogConfiguration\n section already exists, no changes are made to the section.\nThe additional section enables logs from each container to be sent to the the Layer0 log group.\nThis is where logs are looked up during \nl0 \nentity\n logs\n commands.\nThe added \nlogConfiguration\n section uses the following template:\n\n\nlogConfiguration\n: {\n    \nlogDriver\n: \nawslogs\n,\n        \noptions\n: {\n            \nawslogs-group\n: \nl0-\nprefix\n,\n            \nawslogs-region\n: \nregion\n,\n            \nawslogs-stream-prefix\n: \nl0\n\n        }\n    }\n}\n\n\n\n\n\n  \n\n\n\n\ndeploy delete\n#\n\n\nUse the \ndelete\n subcommand to delete a version of a Layer0 deploy.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 deploy delete\n \ndeployID\n\n  \n\n\n\n\nRequired parameters\n#\n\n\n\n  \n\n    \ndeployID\n\n    \nThe unique identifier of the version of the deploy that you want to delete. You can obtain a list of deployIDs for a given deploy by executing the following command: \nl0 deploy get\n \ndeployName\n\n  \n\n\n\n\ndeploy get\n#\n\n\nUse the \nget\n subcommand to view information about an existing Layer0 deploy.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 deploy get\n \ndeployName\n\n  \n\n\n\n\nRequired parameters\n#\n\n\n\n  \n\n    \ndeployName\n\n    \nThe name of the Layer0 deploy for which you want to view additional information.\n\n  \n\n\n\n\nAdditional information\n#\n\n\n\n  \n\n    \nThe \nget\n subcommand supports wildcard matching: \nl0 deploy get dep*\n would return all deploys beginning with \ndep\n.\n\n  \n\n\n\n\ndeploy list\n#\n\n\nUse the \nlist\n subcommand to view a list of deploys in your instance of Layer0.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 deploy list\n\n  \n\n\n\n\n\n\nEnvironment\n#\n\n\nLayer0 environments allow you to isolate services and load balancers for specific applications.\nThe \nenvironment\n command is used to manage Layer0 environments. This command is used with the following subcommands: \ncreate\n, \ndelete\n, \nget\n, \nlist\n, and \nsetmincount\n.\n\n\nenvironment create\n#\n\n\nUse the \ncreate\n subcommand to create an additional Layer0 environment (\nenvironmentName\n).\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 environment create\n [--size] [--min-count] [--user-data] [--os] [--ami] \nenvironmentName\n \n\n  \n\n\n\n\nRequired parameters\n#\n\n\n\n  \n\n    \nenvironmentName\n\n    \nA name for the environment.\n\n  \n\n\n\n\nOptional arguments\n#\n\n\n\n  \n\n    \n--size\n\n    \nThe size of the EC2 instances to create in your environment (default: m3.medium).\n\n  \n\n    \n\n    \n--min-count\n\n    \nThe minimum number of EC2 instances allowed in the environment's autoscaling group (default: 0).\n\n  \n\n  \n\n    \n--user-data\n\n    \nThe user data template to use for the environment's autoscaling group.\n\n  \n\n  \n\n    \n--os\n\n    \nThe operating system used in the environment. Options are \"linux\" or \"windows\" (default: linux).\n        More information on windows environments is documented below\n\n  \n\n  \n\n    \n--ami\n\n    \nA custom AMI ID to use in the environment. If not specified, Layer0 will use its default AMI ID for the specified operating system.\n\n  \n\n\n\n\nThe user data template can be used to add custom configuration to your Layer0 environment.\nLayer0 uses \nGo Templates\n to render user data.\nCurrently, two variables are passed into the template: \nECSEnvironmentID\n and \nS3Bucket\n.\nPlease review the \nECS Tutorial\n\nto better understand how to write a user data template, and use at your own risk!\n\n\nLinux Environments\n: The default Layer0 user data template is:\n\n\n#!/bin/bash\necho ECS_CLUSTER={{ .ECSEnvironmentID }} \n /etc/ecs/ecs.config\necho ECS_ENGINE_AUTH_TYPE=dockercfg \n /etc/ecs/ecs.config\nyum install -y aws-cli awslogs jq\naws s3 cp s3://{{ .S3Bucket }}/bootstrap/dockercfg dockercfg\ncfg=$(cat dockercfg)\necho ECS_ENGINE_AUTH_DATA=$cfg \n /etc/ecs/ecs.config\ndocker pull amazon/amazon-ecs-agent:latest\nstart ecs\n\n\n\n\nWindows Environments\n: The default Layer0 user data template is:\n\n\npowershell\n\n# Set agent env variables for the Machine context (durable)\n$clusterName = \n{{ .ECSEnvironmentID }}\n\nWrite-Host Cluster name set as: $clusterName -foreground green\n\n[Environment]::SetEnvironmentVariable(\nECS_CLUSTER\n, $clusterName, \nMachine\n)\n[Environment]::SetEnvironmentVariable(\nECS_ENABLE_TASK_IAM_ROLE\n, \nfalse\n, \nMachine\n)\n$agentVersion = 'v1.14.0-1.windows.1'\n$agentZipUri = \nhttps://s3.amazonaws.com/amazon-ecs-agent/ecs-agent-windows-$agentVersion.zip\n\n$agentZipMD5Uri = \n$agentZipUri.md5\n\n\n# Configure docker auth\nRead-S3Object -BucketName {{ .S3Bucket }} -Key bootstrap/dockercfg -File dockercfg.json\n$dockercfgContent = [IO.File]::ReadAllText(\ndockercfg.json\n)\n[Environment]::SetEnvironmentVariable(\nECS_ENGINE_AUTH_DATA\n, $dockercfgContent, \nMachine\n)\n[Environment]::SetEnvironmentVariable(\nECS_ENGINE_AUTH_TYPE\n, \ndockercfg\n, \nMachine\n)\n\n### --- Nothing user configurable after this point ---\n$ecsExeDir = \n$env:ProgramFiles\\Amazon\\ECS\n\n$zipFile = \n$env:TEMP\\ecs-agent.zip\n\n$md5File = \n$env:TEMP\\ecs-agent.zip.md5\n\n\n### Get the files from S3\nInvoke-RestMethod -OutFile $zipFile -Uri $agentZipUri\nInvoke-RestMethod -OutFile $md5File -Uri $agentZipMD5Uri\n\n## MD5 Checksum\n$expectedMD5 = (Get-Content $md5File)\n$md5 = New-Object -TypeName System.Security.Cryptography.MD5CryptoServiceProvider\n$actualMD5 = [System.BitConverter]::ToString($md5.ComputeHash([System.IO.File]::ReadAllBytes($zipFile))).replace('-', '')\nif($expectedMD5 -ne $actualMD5) {\n    echo \nDownload doesn't match hash.\n\n    echo \nExpected: $expectedMD5 - Got: $actualMD5\n\n    exit 1\n}\n\n## Put the executables in the executable directory.\nExpand-Archive -Path $zipFile -DestinationPath $ecsExeDir -Force\n\n## Start the agent script in the background.\n$jobname = \nECS-Agent-Init\n\n$script =  \ncd '$ecsExeDir'; .\\amazon-ecs-agent.ps1\n\n$repeat = (New-TimeSpan -Minutes 1)\n$jobpath = $env:LOCALAPPDATA + \n\\Microsoft\\Windows\\PowerShell\\ScheduledJobs\\$jobname\\ScheduledJobDefinition.xml\n\n\nif($(Test-Path -Path $jobpath)) {\n  echo \nJob definition already present\n\n  exit 0\n}\n\n$scriptblock = [scriptblock]::Create(\n$script\n)\n$trigger = New-JobTrigger -At (Get-Date).Date -RepeatIndefinitely -RepetitionInterval $repeat -Once\n$options = New-ScheduledJobOption -RunElevated -ContinueIfGoingOnBattery -StartIfOnBattery\nRegister-ScheduledJob -Name $jobname -ScriptBlock $scriptblock -Trigger $trigger -ScheduledJobOption $options -RunNow\nAdd-JobTrigger -Name $jobname -Trigger (New-JobTrigger -AtStartup -RandomDelay 00:1:00)\n\n/powershell\n\n\npersist\ntrue\n/persist\n\n\n\n\n\n\nWindows Environments\nWindows containers are still in beta. \n\n\n\n\n\n\nYou can view the documented caveats with ECS \nhere\n.\nWhen creating Windows environments in Layer0, the root volume sizes for instances are 200GiB to accommodate the large size of the containers.\n\nIt can take as long as 45 minutes for a new windows container to come online. \n\n\nenvironment delete\n#\n\n\nUse the \ndelete\n subcommand to delete an existing Layer0 environment.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 environment delete\n [--wait] \nenvironmentName\n\n  \n\n\n\n\nRequired parameters\n#\n\n\n\n  \n\n    \nenvironmentName\n\n    \nThe name of the Layer0 environment that you want to delete.\n\n  \n\n\n\n\nOptional arguments\n#\n\n\n\n  \n\n    \n--wait\n\n    \nWait until the deletion is complete before exiting.\n\n  \n\n\n\n\nAdditional information\n#\n\n\n\n  \n\n    \nThis operation performs several tasks asynchronously. When run without the \n--wait\n option, this operation will most likely exit before all of these tasks are complete; when run with the \n--wait\n option, this operation will only exit once these tasks have completed.\n\n  \n\n\n\n\nenvironment get\n#\n\n\nUse the \nget\n subcommand to display information about an existing Layer0 environment.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 environment get\n \nenvironmentName\n\n  \n\n\n\n\nRequired parameters\n#\n\n\n\n  \n\n    \nenvironmentName\n\n    \nThe name of the Layer0 environment for which you want to view additional information.\n\n  \n\n\n\n\nAdditional information\n#\n\n\n\n  \n\n    \nThe \nget\n subcommand supports wildcard matching: \nl0 environment get test*\n would return all environments beginning with \ntest\n.\n\n  \n\n\n\n\nenvironment list\n#\n\n\nUse the \nlist\n subcommand to display a list of environments in your instance of Layer0.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 environment list\n\n  \n\n\n\n\nenvironment setmincount\n#\n\n\nUse the \nsetmincount\n subcommand to set the minimum number of EC2 instances allowed the environment's autoscaling group.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 enviroment setmincount\n \nenvironmentName\n \ncount\n\n  \n\n\n\n\nRequired parameters\n#\n\n\n\n  \n\n    \nenvironmentName\n\n    \nThe name of the Layer0 environment that you want to delete.\n\n  \n\n  \n\n    \ncount\n\n    \nThe minimum number of instances allowed in the environment's autoscaling group.\n\n  \n\n\n\n\nenvironment link\n#\n\n\nUse the \nlink\n subcommand to link two environments together. \nWhen environments are linked, services inside the environments are allowed to communicate with each other as if they were in the same environment. \nThis link is bidirectional. \nThis command is idempotent; it will succeed even if the two specified environments are already linked.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 environment link\n \nsourceEnvironmentName\n \ndestEnvironmentName\n\n  \n\n\n\n\nRequired parameters\n#\n\n\n\n  \n\n    \nsourceEnvironmentName\n\n    \nThe name of the first environment to link.\n\n  \n\n  \n\n    \ndestEnvironmentName\n\n    \nThe name of the second environment to link. \n\n  \n\n\n\n\nenvironment unlink\n#\n\n\nUse the \nunlink\n subcommand to remove the link between two environments.\nThis command is idempotent; it will succeed even if the link does not exist.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 environment unlink\n \nsourceEnvironmentName\n \ndestEnvironmentName\n\n  \n\n\n\n\nRequired parameters\n#\n\n\n\n  \n\n    \nsourceEnvironmentName\n\n    \nThe name of the first environment to unlink.\n\n  \n\n  \n\n    \ndestEnvironmentName\n\n    \nThe name of the second environment to unlink. \n\n  \n\n\n\n\n\n\nJob\n#\n\n\nA Job is a long-running unit of work performed on behalf of the Layer0 API.\nJobs are executed as Layer0 tasks that run in the \napi\n Environment.\nThe \njob\n command is used with the following subcommands: \nlogs\n, \ndelete\n, \nget\n, and \nlist\n.\n\n\njob logs\n#\n\n\nUse the \nlogs\n subcommand to display the logs from a Layer0 job that is currently running.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 job logs\n [--tail=\nN\n ] \njobName\n\n  \n\n\n\n\nRequired parameters\n#\n\n\n\n  \n\n    \njobName\n\n    \nThe name of the Layer0 job for which you want to view logs.\n\n  \n\n\n\n\njob logs\n#\n\n\nUse the \nlogs\n subcommand to display the logs from a Layer0 job that is currently running.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 job logs\n [--tail=\nN\n ] \njobName\n\n  \n\n\n\n\nRequired parameters\n#\n\n\n\n  \n\n    \njobName\n\n    \nThe name of the Layer0 job for which you want to view logs.\n\n  \n\n\n\n\nOptional arguments\n#\n\n\n\n  \n\n    \n--tail=\nN\n\n    \nDisplay only the last \nN\n lines of the log.\n\n  \n\n\n\n\njob delete\n#\n\n\nUse the \ndelete\n subcommand to delete an existing job.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 job delete\n \njobName\n\n  \n\n\n\n\nRequired parameters\n#\n\n\n\n  \n\n    \njobName\n\n    \nThe name of the job that you want to delete.\n\n  \n\n\n\n\njob get\n#\n\n\nUse the \nget\n subcommand to display information about an existing Layer0 job.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 job get\n \njobName\n\n  \n\n\n\n\nRequired parameters\n#\n\n\n\n  \n\n    \njobName\n\n    \nThe name of an existing Layer0 job.\n\n  \n\n\n\n\nAdditional information\n#\n\n\n\n  \n\n    \nThe \nget\n subcommand supports wildcard matching: \nl0 job get 2a55*\n would return all jobs beginning with \n2a55\n.\n\n  \n\n\n\n\njob list\n#\n\n\nUse the \nlist\n subcommand to display information about all of the existing jobs in an instance of Layer0.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 job list\n\n  \n\n\n\n\n\n\nLoadbalancer\n#\n\n\nA load balancer is a component of a Layer0 environment. Load balancers listen for traffic on certain ports, and then forward that traffic to Layer0 \nservices\n. The \nloadbalancer\n command is used with the following subcommands: \ncreate\n, \ndelete\n, \naddport\n, \ndropport\n, \nget\n, \nlist\n, and \nhealthcheck\n.\n\n\nloadbalancer create\n#\n\n\nUse the \ncreate\n subcommand to create a new load balancer.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 loadbalancer create\n [--port \nport\n --port \nport\n ...] [--certificate \ncertificateName\n] [--private] [healthcheck-flags]\nenvironmentName loadBalancerName\n\n  \n\n\n\n\nRequired parameters\n#\n\n\n\n  \n\n    \nenvironmentName\n\n    \nThe name of the existing Layer0 environment in which you want to create the load balancer.\n\n  \n\n  \n\n    \nloadBalancerName\n\n    \nA name for the load balancer.\n\n  \n\n\n\n\nOptional arguments\n#\n\n\n\n  \n\n    \n\n      --port \nhostPort:containerPort/protocol\n\n    \n\n    \n\n      \nThe port configuration for the load balancer. \nhostPort\n is the port on which the load balancer will listen for traffic; \ncontainerPort\n is the port that traffic will be forwarded to. You can specify multiple ports using \n--port xxx --port yyy\n. If this option is not specified, Layer0 will use the following configuration: 80:80/tcp\n\n    \n\n  \n\n  \n\n    \n\n      --certificate \ncertificateName\n\n    \n\n    \n\n      \nThe name of an existing Layer0 certificate. You must include this option if you are using an HTTPS port configuration.\n\n    \n\n  \n\n  \n\n    \n\n      --private\n    \n\n    \n\n      \nWhen you use this option, the load balancer will only be accessible from within the Layer0 environment.\n\n    \n\n  \n\n  \n\n    \n\n      --healthcheck-target \ntarget\n\n    \n\n    \n\n      \nThe target of the check. Valid pattern is \nPROTOCOL:PORT/PATH\n \n(default: \n\"TCP:80\"\n)\n\n      \n\n      If PROTOCOL is \nHTTP\n or \nHTTPS\n, both PORT and PATH are required\n      \n\n      - \nexample: \nHTTP:80/admin/healthcheck\n\n      \n\n      If PROTOCOL is \nTCP\n or \nSSL\n, PORT is required and PATH is not supported\n      \n\n      - \nexample: \nTCP:80\n\n    \n\n  \n\n  \n\n    \n\n      --healthcheck-interval \ninterval\n\n    \n\n    \n\n      \nThe interval between checks \n(default: \n30\n)\n.\n\n    \n\n  \n\n  \n\n    \n\n      --healthcheck-timeout \ntimeout\n\n    \n\n    \n\n      \nThe length of time before the check times out \n(default: \n5\n)\n.\n\n    \n\n  \n\n  \n\n    \n\n      --healthcheck-healthy-threshold \nhealthyThreshold\n\n    \n\n    \n\n      \nThe number of checks before the instance is declared healthy \n(default: \n2\n)\n.\n\n    \n\n  \n\n  \n\n    \n\n      --healthcheck-unhealthy-threshold \nunhealthyThreshold\n\n    \n\n    \n\n      \nThe number of checks before the instance is declared unhealthy \n(default: \n2\n)\n.\n\n    \n\n  \n\n\n\n\nloadbalancer delete\n#\n\n\nUse the \ndelete\n subcommand to delete an existing load balancer.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 loadbalancer delete\n [--wait] \nloadBalancerName\n\n  \n\n\n\n\nRequired parameters\n#\n\n\n\n  \n\n    \nloadBalancerName\n\n    \nThe name of the load balancer that you want to delete.\n\n  \n\n\n\n\nOptional arguments\n#\n\n\n\n  \n\n    \n--wait\n\n    \nWait until the deletion is complete before exiting.\n\n  \n\n\n\n\nAdditional information\n#\n\n\n\n  \n\n    \nIn order to delete a load balancer that is already attached to a service, you must first delete the service that uses the load balancer.\n\n  \n\n  \n\n    \nThis operation performs several tasks asynchronously. When run without the \n--wait\n option, this operation will most likely exit before all of these tasks are complete; when run with the \n--wait\n option, this operation will only exit once these tasks have completed.\n\n  \n\n\n\n\nloadbalancer addport\n#\n\n\nUse the \naddport\n subcommand to add a new port configuration to an existing Layer0 load balancer.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 loadbalancer addport\n \nloadBalancerName hostPort:containerPort/protocol\n [--certificate \ncertificateName\n]\n\n  \n\n\n\n\nRequired parameters\n#\n\n\n\n  \n\n    \nloadBalancerName\n\n    \nThe name of an existing Layer0 load balancer in which you want to add the port configuration.\n\n  \n\n  \n\n    \nhostPort\n\n    \nThe port that the load balancer will listen on.\n\n  \n\n  \n\n    \ncontainerPort\n\n    \nThe port that the load balancer will forward traffic to.\n\n  \n\n  \n\n    \nprotocol\n\n    \nThe protocol to use when forwarding traffic (acceptable values: tcp, ssl, http, and https).\n\n  \n\n\n\n\nOptional arguments\n#\n\n\n\n  \n\n    \n--certificate \ncertificateName\n\n    \nThe name of an existing Layer0 certificate. You must include this option if you are using an HTTPS port configuration.\n\n  \n\n\n\n\nAdditional information\n#\n\n\n\n  \n\n    \nThe port configuration you specify must not already be in use by the load balancer you specify.\n\n  \n\n\n\n\nloadbalancer dropport\n#\n\n\nUse the \ndropport\n subcommand to remove a port configuration from an existing Layer0 load balancer.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 loadbalancer dropport\n \nloadBalancerName\n \nhostPort\n\n  \n\n\n\n\nRequired parameters\n#\n\n\n\n  \n\n    \nloadBalancerName\n\n    \nThe name of an existing Layer0 load balancer in which you want to remove the port configuration.\n\n  \n\n  \n\n    \nhostPort\n\n    \nThe host port to remove from the load balancer.\n\n  \n\n\n\n\nloadbalancer get\n#\n\n\nUse the \nget\n subcommand to display information about an existing Layer0 load balancer.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 loadbalancer get\n \nenvironmentName:loadBalancerName\n\n  \n\n\n\n\nRequired parameters\n#\n\n\n\n  \n\n    \nenvironmentName\n\n    \nThe name of an existing Layer0 environment.\n\n  \n\n  \n\n    \nloadBalancerName\n\n    \nThe name of an existing Layer0 load balancer.\n\n  \n\n\n\n\nAdditional information\n#\n\n\n\n  \n\n    \nThe \nget\n subcommand supports wildcard matching: \nl0 loadbalancer get entrypoint*\n would return all jobs beginning with \nentrypoint\n.\n\n  \n\n\n\n\nloadbalancer list\n#\n\n\nUse the \nlist\n subcommand to display information about all of the existing load balancers in an instance of Layer0.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 loadbalancer list\n\n  \n\n\n\n\nloadbalancer healthcheck\n#\n\n\nUse the \nhealthcheck\n subcommand to display information about or update the configuration of a load balancer's health check.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 loadbalancer healthcheck\n [healthcheck-flags] \nloadbalancerName\n\n  \n\n\n\n\nOptional arguments\n#\n\n\n\n  \n\n    \n\n      --set-target \ntarget\n\n    \n\n    \n\n      \nThe target of the check. Valid pattern is \nPROTOCOL:PORT/PATH\n, where PROTOCOL values are:\n      \n\n      \nHTTP\n or \nHTTPS\n: both PORT and PATH are required\n      \n\n      - \nexample: \nHTTP:80/admin/healthcheck\n\n      \n\n      \nTCP\n or \nSSL\n: PORT is required, PATH is not supported\n      \n\n      - \nexample: \nTCP:80\n\n    \n\n  \n\n  \n\n    \n\n      --set-interval \ninterval\n\n    \n\n    \n\n      \nThe interval between checks.\n\n    \n\n  \n\n  \n\n    \n\n      --set-timeout \ntimeout\n\n    \n\n    \n\n      \nThe length of time before the check times out.\n\n    \n\n  \n\n  \n\n    \n\n      --set-healthy-threshold \nhealthyThreshold\n\n    \n\n    \n\n      \nThe number of checks before the instance is declared healthy.\n\n    \n\n  \n\n  \n\n    \n\n      --set-unhealthy-threshold \nunhealthyThreshold\n\n    \n\n    \n\n      \nThe number of checks before the instance is declared unhealthy.\n\n    \n\n  \n\n\n\n\nAdditional information\n#\n\n\n\n  \n\n    \nCalling the subcommand without flags will display the current configuration of the load balancer's health check. Setting any of the flags will update the corresponding field in the health check, and all omitted flags will leave the corresponding fields unchanged.\n\n  \n\n\n\n\n\n\n\nService\n#\n\n\nA service is a component of a Layer0 environment. The purpose of a service is to execute a Docker image specified in a \ndeploy\n. In order to create a service, you must first create an \nenvironment\n and a \ndeploy\n; in most cases, you should also create a \nload balancer\n before creating the service.\n\n\nThe \nservice\n command is used with the following subcommands: \ncreate\n, \ndelete\n, \nget\n, \nupdate\n, \nlist\n, \nlogs\n, and \nscale\n.\n\n\nservice create\n#\n\n\nUse the \ncreate\n subcommand to create a Layer0 service.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 service create\n [--loadbalancer \nenvironmentName:loadBalancerName\n ] [--no-logs] \nenvironmentName serviceName deployName:deployVersion\n\n  \n\n\n\n\nRequired parameters\n#\n\n\n\n  \n\n    \nserviceName\n\n    \nA name for the service that you are creating.\n\n  \n\n  \n\n    \nenvironmentName\n\n    \nThe name of an existing Layer0 environment.\n\n  \n\n  \n\n    \ndeployName\n\n    \nThe name of a Layer0 deploy that exists in the environment \nenvironmentName\n.\n\n  \n\n  \n\n    \ndeployVersion\n\n    \nThe version number of the Layer0 deploy that you want to deploy. If you do not specify a version number, the latest version of the deploy will be used.\n\n  \n\n\n\n\nOptional arguments\n#\n\n\n\n  \n\n    \n--loadbalancer \nenvironmentName:loadBalancerName\n\n    \nPlace the new service behind an existing load balancer named \nloadBalancerName\n in the environment named \nenvironmentName\n.\n\n  \n\n  \n\n    \n--no-logs\n\n    \nDisable cloudwatch logging for the service\n\n  \n\n\n\n\nservice update\n#\n\n\nUse the \nupdate\n subcommand to apply an existing Layer0 Deploy to an existing Layer0 service.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 service update\n [--no-logs] \nenvironmentName:serviceName deployName:deployVersion\n\n  \n\n\n\n\nRequired parameters\n#\n\n\n\n \n\n    \nenvironmentName\n\n    \nThe name of the Layer0 environment in which the service resides.\n\n  \n\n  \n\n    \nserviceName\n\n    \nThe name of an existing Layer0 service into which you want to apply the deploy.\n\n  \n\n  \n\n    \ndeployName\n\n    \nThe name of the Layer0 deploy that you want to apply to the service.\n\n  \n\n  \n\n    \ndeployVersion\n\n    \nThe version of the Layer0 deploy that you want to apply to the service. If you do not specify a version number, the latest version of the deploy will be applied.\n\n  \n\n  \n\n    \n--no-logs\n\n    \nDisable cloudwatch logging for the service\n\n  \n\n\n\n\nAdditional information\n#\n\n\n\n  \n\n    \nIf your service uses a load balancer, when you update the task definition for the service, the container name and container port that were specified when the service was created must remain the same in the task definition. In other words, if your service has a load balancer, you cannot apply any deploy you want to that service. If you are varying the container name or exposed ports, you must create a new service instead.\n\n  \n\n\n\n\n\nservice delete\n#\n\n\nUse the \ndelete\n subcommand to delete an existing Layer0 service.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 service delete\n [--wait] \nenvironmentName:serviceName\n\n  \n\n\n\n\nRequired parameters\n#\n\n\n\n  \n\n    \nenvironmentName\n\n    \nThe name of the Layer0 environment that contains the service you want to delete.\n\n  \n\n  \n\n    \nserviceName\n\n    \nThe name of the Layer0 service that you want to delete.\n\n  \n\n\n\n\nOptional arguments\n#\n\n\n\n  \n\n    \n--wait\n\n    \nWait until the deletion is complete before exiting.\n\n  \n\n\n\n\nAdditional information\n#\n\n\n\n  \n\n    \nThis operation performs several tasks asynchronously. When run without the \n--wait\n option, this operation will most likely exit before all of these tasks are complete; when run with the \n--wait\n option, this operation will only exit once these tasks have completed.\n\n  \n\n\n\n\nservice get\n#\n\n\nUse the \nget\n subcommand to display information about an existing Layer0 service.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 service get\n \nenvironmentName:serviceName\n\n  \n\n\n\n\nRequired parameters\n#\n\n\n\n  \n\n    \nenvironmentName\n\n    \nThe name of an existing Layer0 environment.\n\n  \n\n  \n\n    \nserviceName\n\n    \nThe name of an existing Layer0 service.\n\n  \n\n\n\n\nservice list\n#\n\n\nUse the \nlist\n subcommand to list all of the existing services in your Layer0 instance.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 service list\n\n  \n\n\n\n\nservice logs\n#\n\n\nUse the \nlogs\n subcommand to display the logs from a Layer0 service that is currently running.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 service logs\n [--tail=\nN\n ] \nserviceName\n\n  \n\n\n\n\nRequired parameters\n#\n\n\n\n  \n\n    \nserviceName\n\n    \nThe name of the Layer0 service for which you want to view logs.\n\n  \n\n\n\n\nOptional arguments\n#\n\n\n\n  \n\n    \n--tail=\nN\n\n    \nDisplay only the last \nN\n lines of the log.\n\n  \n\n\n\n\nservice scale\n#\n\n\nUse the \nscale\n subcommand to specify how many copies of an existing Layer0 service should run.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 service scale\n \nenvironmentName:serviceName N\n\n  \n\n\n\n\nRequired parameters\n#\n\n\n\n  \n\n    \nenvironmentName\n\n    \nThe name of the Layer0 environment that contains the service that you want to scale.\n\n  \n\n  \n\n    \nserviceName\n\n    \nThe name of the Layer0 service that you want to scale up.\n\n  \n\n  \n\n    \nN\n\n    \nThe number of copies of the specified service that should be run.\n\n  \n\n\n\n\n\n\nTask\n#\n\n\nA Layer0 task is a component of an environment. A task executes the contents of a Docker image, as specified in a deploy. A task differs from a service in that a task does not restart after exiting. Additionally, ports are not exposed when using a task.\n\n\nThe \ntask\n command is used with the following subcommands: \ncreate\n, \ndelete\n, \nget\n, \nlist\n, and \nlogs\n.\n\n\ntask create\n#\n\n\nUse the \ncreate\n subcommand to create a Layer0 task.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 task create\n [--no-logs] [--copies \ncopies\n] \nenvironmentName taskName deployName\n\n  \n\n\n\n\nRequired parameters\n#\n\n\n\n  \n\n    \nenvironmentName\n\n    \nThe name of the existing Layer0 environment in which you want to create the task.\n\n  \n\n  \n\n    \ntaskName\n\n    \nA name for the task.\n\n  \n\n  \n\n    \ndeployName\n\n    \nThe name of an existing Layer0 deploy that the task should use.\n\n  \n\n\n\n\nOptional arguments\n#\n\n\n\n  \n\n    \n--copies\n\n    \nThe number of copies of the task to run (default: 1)\n\n  \n\n  \n\n    \n--no-logs\n\n    \nDisable cloudwatch logging for the service\n\n  \n\n\n\n\ntask delete\n#\n\n\nUse the \ndelete\n subcommand to delete an existing Layer0 task.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 task delete\n [\nenvironmentName\n:]\ntaskName\n\n  \n\n\n\n\nRequired parameters\n#\n\n\n\n  \n\n    \ntaskName\n\n    \nThe name of the Layer0 task that you want to delete.\n\n  \n\n\n\n\nOptional parameters\n#\n\n\n\n  \n\n    \n[\nenvironmentName\n:]\n\n    \nThe name of the Layer0 environment that contains the task. This parameter is only necessary if multiple environments contain tasks with exactly the same name.\n\n  \n\n\n\n\nAdditional information\n#\n\n\n\n  \n\n    \nUntil the record has been purged, the API may indicate that the task is still running. Task records are typically purged within an hour.\n\n  \n\n\n\n\ntask get\n#\n\n\nUse the \nget\n subcommand to display information about an existing Layer0 task (\ntaskName\n).\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 task get\n [\nenvironmentName\n:]\ntaskName\n\n  \n\n\n\n\nRequired parameters\n#\n\n\n\n  \n\n    \ntaskName\n\n    \nThe name of a Layer0 task for which you want to see information.\n\n  \n\n\n\n\nAdditional information\n#\n\n\n\n  \n\n    \nThe value of \ntaskName\n does not need to exactly match the name of an existing task. If multiple results are found that match the pattern you specified in \ntaskName\n, then information about all matching tasks will be returned.\n\n  \n\n\n\n\ntask list\n#\n\n\nUse the \ntask\n subcommand to display a list of running tasks in your Layer0.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 task list\n\n  \n\n\n\n\ntask logs\n#\n\n\nUse the \nlogs\n subcommand to display logs for a running Layer0 task.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 task logs\n [--tail=\nN\n ] \ntaskName\n\n  \n\n\n\n\nRequired parameters\n#\n\n\n\n  \n\n    \ntaskName\n\n    \nThe name of an existing Layer0 task.\n\n  \n\n\n\n\nAdditional information\n#\n\n\n\n  \n\n    \nThe value of \ntaskName\n does not need to exactly match the name of an existing task. If multiple results are found that match the pattern you specified in \ntaskName\n, then information about all matching tasks will be returned.\n\n  \n\n\n\n\ntask list\n#\n\n\nUse the \ntask\n subcommand to display a list of running tasks in your Layer0.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 task list\n\n  \n\n\n\n=======", 
            "title": "Layer0 CLI"
        }, 
        {
            "location": "/reference/cli/#layer0-cli-reference", 
            "text": "", 
            "title": "Layer0 CLI Reference"
        }, 
        {
            "location": "/reference/cli/#global-options", 
            "text": "The  l0  application is designed to be used with one of several subcommands:  admin ,  deploy ,  environment ,  job ,  loadbalancer ,  service , and  task . These subcommands are detailed in the sections below. There are, however, some global parameters that you may specify when using  l0 .", 
            "title": "Global options"
        }, 
        {
            "location": "/reference/cli/#usage", 
            "text": "l0  [ globalOptions ]  command   subcommand  [ options ] [ parameters ]", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#optional-arguments", 
            "text": "--output {text|json} \n     Specify the format of Layer0 outputs. By default, Layer0 outputs unformatted text; by issuing the  --output json  option, you can force  l0  to output JSON-formatted text. \n   \n   \n     --version \n     Display the version number of the  l0  application.", 
            "title": "Optional arguments"
        }, 
        {
            "location": "/reference/cli/#admin", 
            "text": "The  admin  command is used to manage the Layer0 API server. This command is used with the following subcommands:  debug ,  sql , and  version .", 
            "title": "Admin"
        }, 
        {
            "location": "/reference/cli/#admin-debug", 
            "text": "Use the  debug  subcommand to view the running version of your Layer0 API server and CLI.", 
            "title": "admin debug"
        }, 
        {
            "location": "/reference/cli/#usage_1", 
            "text": "l0 admin debug", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#admin-sql", 
            "text": "Use the  sql  subcommand to initialize the Layer0 API database.", 
            "title": "admin sql"
        }, 
        {
            "location": "/reference/cli/#usage_2", 
            "text": "l0 admin sql", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#additional-information", 
            "text": "The  sql  subcommand is automatically executed during the Layer0 installation process; we recommend that you do not use this subcommand unless specifically directed to do so.", 
            "title": "Additional information"
        }, 
        {
            "location": "/reference/cli/#admin-version", 
            "text": "Use the  version  subcommand to display the current version of the Layer0 API.", 
            "title": "admin version"
        }, 
        {
            "location": "/reference/cli/#usage_3", 
            "text": "l0 admin version", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#deploy", 
            "text": "", 
            "title": "Deploy"
        }, 
        {
            "location": "/reference/cli/#deploy-create", 
            "text": "Use the  create  subcommand to upload a Docker task definition into Layer0. This command is used with the following subcommands:  create ,  delete ,  get  and  list .", 
            "title": "deploy create"
        }, 
        {
            "location": "/reference/cli/#usage_4", 
            "text": "l0 deploy create   dockerPath   deployName", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#required-parameters", 
            "text": "dockerPath \n     The path to the Docker task definition that you want to upload. \n   \n   \n     deployName \n     A name for the deploy.", 
            "title": "Required parameters"
        }, 
        {
            "location": "/reference/cli/#additional-information_1", 
            "text": "If  deployName  exactly matches the name of an existing Layer0 deploy, then the version number of that deploy will increase by 1, and the task definition you specified will replace the task definition specified in the previous version. \n     \n   \n     If you use Visual Studio to modify or create your Dockerrun file, you may see an \"Invalid Dockerrun.aws.json\" error. This error is caused by the default encoding used by Visual Studio. See the  \"Common issues\" page  for steps to resolve this issue. \n      \n   \n     \nDeploys created through Layer0 are rendered with a  logConfiguration  section for each container.\nIf a  logConfiguration  section already exists, no changes are made to the section.\nThe additional section enables logs from each container to be sent to the the Layer0 log group.\nThis is where logs are looked up during  l0  entity  logs  commands.\nThe added  logConfiguration  section uses the following template:  logConfiguration : {\n     logDriver :  awslogs ,\n         options : {\n             awslogs-group :  l0- prefix ,\n             awslogs-region :  region ,\n             awslogs-stream-prefix :  l0 \n        }\n    }\n}", 
            "title": "Additional information"
        }, 
        {
            "location": "/reference/cli/#deploy-delete", 
            "text": "Use the  delete  subcommand to delete a version of a Layer0 deploy.", 
            "title": "deploy delete"
        }, 
        {
            "location": "/reference/cli/#usage_5", 
            "text": "l0 deploy delete   deployID", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#required-parameters_1", 
            "text": "deployID \n     The unique identifier of the version of the deploy that you want to delete. You can obtain a list of deployIDs for a given deploy by executing the following command:  l0 deploy get   deployName", 
            "title": "Required parameters"
        }, 
        {
            "location": "/reference/cli/#deploy-get", 
            "text": "Use the  get  subcommand to view information about an existing Layer0 deploy.", 
            "title": "deploy get"
        }, 
        {
            "location": "/reference/cli/#usage_6", 
            "text": "l0 deploy get   deployName", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#required-parameters_2", 
            "text": "deployName \n     The name of the Layer0 deploy for which you want to view additional information.", 
            "title": "Required parameters"
        }, 
        {
            "location": "/reference/cli/#additional-information_2", 
            "text": "The  get  subcommand supports wildcard matching:  l0 deploy get dep*  would return all deploys beginning with  dep .", 
            "title": "Additional information"
        }, 
        {
            "location": "/reference/cli/#deploy-list", 
            "text": "Use the  list  subcommand to view a list of deploys in your instance of Layer0.", 
            "title": "deploy list"
        }, 
        {
            "location": "/reference/cli/#usage_7", 
            "text": "l0 deploy list", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#environment", 
            "text": "Layer0 environments allow you to isolate services and load balancers for specific applications.\nThe  environment  command is used to manage Layer0 environments. This command is used with the following subcommands:  create ,  delete ,  get ,  list , and  setmincount .", 
            "title": "Environment"
        }, 
        {
            "location": "/reference/cli/#environment-create", 
            "text": "Use the  create  subcommand to create an additional Layer0 environment ( environmentName ).", 
            "title": "environment create"
        }, 
        {
            "location": "/reference/cli/#usage_8", 
            "text": "l0 environment create  [--size] [--min-count] [--user-data] [--os] [--ami]  environmentName", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#required-parameters_3", 
            "text": "environmentName \n     A name for the environment.", 
            "title": "Required parameters"
        }, 
        {
            "location": "/reference/cli/#optional-arguments_1", 
            "text": "--size \n     The size of the EC2 instances to create in your environment (default: m3.medium). \n   \n     \n     --min-count \n     The minimum number of EC2 instances allowed in the environment's autoscaling group (default: 0). \n   \n   \n     --user-data \n     The user data template to use for the environment's autoscaling group. \n   \n   \n     --os \n     The operating system used in the environment. Options are \"linux\" or \"windows\" (default: linux).\n        More information on windows environments is documented below \n   \n   \n     --ami \n     A custom AMI ID to use in the environment. If not specified, Layer0 will use its default AMI ID for the specified operating system. \n     The user data template can be used to add custom configuration to your Layer0 environment.\nLayer0 uses  Go Templates  to render user data.\nCurrently, two variables are passed into the template:  ECSEnvironmentID  and  S3Bucket .\nPlease review the  ECS Tutorial \nto better understand how to write a user data template, and use at your own risk!  Linux Environments : The default Layer0 user data template is:  #!/bin/bash\necho ECS_CLUSTER={{ .ECSEnvironmentID }}   /etc/ecs/ecs.config\necho ECS_ENGINE_AUTH_TYPE=dockercfg   /etc/ecs/ecs.config\nyum install -y aws-cli awslogs jq\naws s3 cp s3://{{ .S3Bucket }}/bootstrap/dockercfg dockercfg\ncfg=$(cat dockercfg)\necho ECS_ENGINE_AUTH_DATA=$cfg   /etc/ecs/ecs.config\ndocker pull amazon/amazon-ecs-agent:latest\nstart ecs  Windows Environments : The default Layer0 user data template is:  powershell \n# Set agent env variables for the Machine context (durable)\n$clusterName =  {{ .ECSEnvironmentID }} \nWrite-Host Cluster name set as: $clusterName -foreground green\n\n[Environment]::SetEnvironmentVariable( ECS_CLUSTER , $clusterName,  Machine )\n[Environment]::SetEnvironmentVariable( ECS_ENABLE_TASK_IAM_ROLE ,  false ,  Machine )\n$agentVersion = 'v1.14.0-1.windows.1'\n$agentZipUri =  https://s3.amazonaws.com/amazon-ecs-agent/ecs-agent-windows-$agentVersion.zip \n$agentZipMD5Uri =  $agentZipUri.md5 \n\n# Configure docker auth\nRead-S3Object -BucketName {{ .S3Bucket }} -Key bootstrap/dockercfg -File dockercfg.json\n$dockercfgContent = [IO.File]::ReadAllText( dockercfg.json )\n[Environment]::SetEnvironmentVariable( ECS_ENGINE_AUTH_DATA , $dockercfgContent,  Machine )\n[Environment]::SetEnvironmentVariable( ECS_ENGINE_AUTH_TYPE ,  dockercfg ,  Machine )\n\n### --- Nothing user configurable after this point ---\n$ecsExeDir =  $env:ProgramFiles\\Amazon\\ECS \n$zipFile =  $env:TEMP\\ecs-agent.zip \n$md5File =  $env:TEMP\\ecs-agent.zip.md5 \n\n### Get the files from S3\nInvoke-RestMethod -OutFile $zipFile -Uri $agentZipUri\nInvoke-RestMethod -OutFile $md5File -Uri $agentZipMD5Uri\n\n## MD5 Checksum\n$expectedMD5 = (Get-Content $md5File)\n$md5 = New-Object -TypeName System.Security.Cryptography.MD5CryptoServiceProvider\n$actualMD5 = [System.BitConverter]::ToString($md5.ComputeHash([System.IO.File]::ReadAllBytes($zipFile))).replace('-', '')\nif($expectedMD5 -ne $actualMD5) {\n    echo  Download doesn't match hash. \n    echo  Expected: $expectedMD5 - Got: $actualMD5 \n    exit 1\n}\n\n## Put the executables in the executable directory.\nExpand-Archive -Path $zipFile -DestinationPath $ecsExeDir -Force\n\n## Start the agent script in the background.\n$jobname =  ECS-Agent-Init \n$script =   cd '$ecsExeDir'; .\\amazon-ecs-agent.ps1 \n$repeat = (New-TimeSpan -Minutes 1)\n$jobpath = $env:LOCALAPPDATA +  \\Microsoft\\Windows\\PowerShell\\ScheduledJobs\\$jobname\\ScheduledJobDefinition.xml \n\nif($(Test-Path -Path $jobpath)) {\n  echo  Job definition already present \n  exit 0\n}\n\n$scriptblock = [scriptblock]::Create( $script )\n$trigger = New-JobTrigger -At (Get-Date).Date -RepeatIndefinitely -RepetitionInterval $repeat -Once\n$options = New-ScheduledJobOption -RunElevated -ContinueIfGoingOnBattery -StartIfOnBattery\nRegister-ScheduledJob -Name $jobname -ScriptBlock $scriptblock -Trigger $trigger -ScheduledJobOption $options -RunNow\nAdd-JobTrigger -Name $jobname -Trigger (New-JobTrigger -AtStartup -RandomDelay 00:1:00) /powershell  persist true /persist   Windows Environments Windows containers are still in beta.     You can view the documented caveats with ECS  here .\nWhen creating Windows environments in Layer0, the root volume sizes for instances are 200GiB to accommodate the large size of the containers. \nIt can take as long as 45 minutes for a new windows container to come online.", 
            "title": "Optional arguments"
        }, 
        {
            "location": "/reference/cli/#environment-delete", 
            "text": "Use the  delete  subcommand to delete an existing Layer0 environment.", 
            "title": "environment delete"
        }, 
        {
            "location": "/reference/cli/#usage_9", 
            "text": "l0 environment delete  [--wait]  environmentName", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#required-parameters_4", 
            "text": "environmentName \n     The name of the Layer0 environment that you want to delete.", 
            "title": "Required parameters"
        }, 
        {
            "location": "/reference/cli/#optional-arguments_2", 
            "text": "--wait \n     Wait until the deletion is complete before exiting.", 
            "title": "Optional arguments"
        }, 
        {
            "location": "/reference/cli/#additional-information_3", 
            "text": "This operation performs several tasks asynchronously. When run without the  --wait  option, this operation will most likely exit before all of these tasks are complete; when run with the  --wait  option, this operation will only exit once these tasks have completed.", 
            "title": "Additional information"
        }, 
        {
            "location": "/reference/cli/#environment-get", 
            "text": "Use the  get  subcommand to display information about an existing Layer0 environment.", 
            "title": "environment get"
        }, 
        {
            "location": "/reference/cli/#usage_10", 
            "text": "l0 environment get   environmentName", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#required-parameters_5", 
            "text": "environmentName \n     The name of the Layer0 environment for which you want to view additional information.", 
            "title": "Required parameters"
        }, 
        {
            "location": "/reference/cli/#additional-information_4", 
            "text": "The  get  subcommand supports wildcard matching:  l0 environment get test*  would return all environments beginning with  test .", 
            "title": "Additional information"
        }, 
        {
            "location": "/reference/cli/#environment-list", 
            "text": "Use the  list  subcommand to display a list of environments in your instance of Layer0.", 
            "title": "environment list"
        }, 
        {
            "location": "/reference/cli/#usage_11", 
            "text": "l0 environment list", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#environment-setmincount", 
            "text": "Use the  setmincount  subcommand to set the minimum number of EC2 instances allowed the environment's autoscaling group.", 
            "title": "environment setmincount"
        }, 
        {
            "location": "/reference/cli/#usage_12", 
            "text": "l0 enviroment setmincount   environmentName   count", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#required-parameters_6", 
            "text": "environmentName \n     The name of the Layer0 environment that you want to delete. \n   \n   \n     count \n     The minimum number of instances allowed in the environment's autoscaling group.", 
            "title": "Required parameters"
        }, 
        {
            "location": "/reference/cli/#environment-link", 
            "text": "Use the  link  subcommand to link two environments together. \nWhen environments are linked, services inside the environments are allowed to communicate with each other as if they were in the same environment. \nThis link is bidirectional. \nThis command is idempotent; it will succeed even if the two specified environments are already linked.", 
            "title": "environment link"
        }, 
        {
            "location": "/reference/cli/#usage_13", 
            "text": "l0 environment link   sourceEnvironmentName   destEnvironmentName", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#required-parameters_7", 
            "text": "sourceEnvironmentName \n     The name of the first environment to link. \n   \n   \n     destEnvironmentName \n     The name of the second environment to link.", 
            "title": "Required parameters"
        }, 
        {
            "location": "/reference/cli/#environment-unlink", 
            "text": "Use the  unlink  subcommand to remove the link between two environments.\nThis command is idempotent; it will succeed even if the link does not exist.", 
            "title": "environment unlink"
        }, 
        {
            "location": "/reference/cli/#usage_14", 
            "text": "l0 environment unlink   sourceEnvironmentName   destEnvironmentName", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#required-parameters_8", 
            "text": "sourceEnvironmentName \n     The name of the first environment to unlink. \n   \n   \n     destEnvironmentName \n     The name of the second environment to unlink.", 
            "title": "Required parameters"
        }, 
        {
            "location": "/reference/cli/#job", 
            "text": "A Job is a long-running unit of work performed on behalf of the Layer0 API.\nJobs are executed as Layer0 tasks that run in the  api  Environment.\nThe  job  command is used with the following subcommands:  logs ,  delete ,  get , and  list .", 
            "title": "Job"
        }, 
        {
            "location": "/reference/cli/#job-logs", 
            "text": "Use the  logs  subcommand to display the logs from a Layer0 job that is currently running.", 
            "title": "job logs"
        }, 
        {
            "location": "/reference/cli/#usage_15", 
            "text": "l0 job logs  [--tail= N  ]  jobName", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#required-parameters_9", 
            "text": "jobName \n     The name of the Layer0 job for which you want to view logs.", 
            "title": "Required parameters"
        }, 
        {
            "location": "/reference/cli/#job-logs_1", 
            "text": "Use the  logs  subcommand to display the logs from a Layer0 job that is currently running.", 
            "title": "job logs"
        }, 
        {
            "location": "/reference/cli/#usage_16", 
            "text": "l0 job logs  [--tail= N  ]  jobName", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#required-parameters_10", 
            "text": "jobName \n     The name of the Layer0 job for which you want to view logs.", 
            "title": "Required parameters"
        }, 
        {
            "location": "/reference/cli/#optional-arguments_3", 
            "text": "--tail= N \n     Display only the last  N  lines of the log.", 
            "title": "Optional arguments"
        }, 
        {
            "location": "/reference/cli/#job-delete", 
            "text": "Use the  delete  subcommand to delete an existing job.", 
            "title": "job delete"
        }, 
        {
            "location": "/reference/cli/#usage_17", 
            "text": "l0 job delete   jobName", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#required-parameters_11", 
            "text": "jobName \n     The name of the job that you want to delete.", 
            "title": "Required parameters"
        }, 
        {
            "location": "/reference/cli/#job-get", 
            "text": "Use the  get  subcommand to display information about an existing Layer0 job.", 
            "title": "job get"
        }, 
        {
            "location": "/reference/cli/#usage_18", 
            "text": "l0 job get   jobName", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#required-parameters_12", 
            "text": "jobName \n     The name of an existing Layer0 job.", 
            "title": "Required parameters"
        }, 
        {
            "location": "/reference/cli/#additional-information_5", 
            "text": "The  get  subcommand supports wildcard matching:  l0 job get 2a55*  would return all jobs beginning with  2a55 .", 
            "title": "Additional information"
        }, 
        {
            "location": "/reference/cli/#job-list", 
            "text": "Use the  list  subcommand to display information about all of the existing jobs in an instance of Layer0.", 
            "title": "job list"
        }, 
        {
            "location": "/reference/cli/#usage_19", 
            "text": "l0 job list", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#loadbalancer", 
            "text": "A load balancer is a component of a Layer0 environment. Load balancers listen for traffic on certain ports, and then forward that traffic to Layer0  services . The  loadbalancer  command is used with the following subcommands:  create ,  delete ,  addport ,  dropport ,  get ,  list , and  healthcheck .", 
            "title": "Loadbalancer"
        }, 
        {
            "location": "/reference/cli/#loadbalancer-create", 
            "text": "Use the  create  subcommand to create a new load balancer.", 
            "title": "loadbalancer create"
        }, 
        {
            "location": "/reference/cli/#usage_20", 
            "text": "l0 loadbalancer create  [--port  port  --port  port  ...] [--certificate  certificateName ] [--private] [healthcheck-flags] environmentName loadBalancerName", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#required-parameters_13", 
            "text": "environmentName \n     The name of the existing Layer0 environment in which you want to create the load balancer. \n   \n   \n     loadBalancerName \n     A name for the load balancer.", 
            "title": "Required parameters"
        }, 
        {
            "location": "/reference/cli/#optional-arguments_4", 
            "text": "--port  hostPort:containerPort/protocol \n     \n     \n       The port configuration for the load balancer.  hostPort  is the port on which the load balancer will listen for traffic;  containerPort  is the port that traffic will be forwarded to. You can specify multiple ports using  --port xxx --port yyy . If this option is not specified, Layer0 will use the following configuration: 80:80/tcp \n     \n   \n   \n     \n      --certificate  certificateName \n     \n     \n       The name of an existing Layer0 certificate. You must include this option if you are using an HTTPS port configuration. \n     \n   \n   \n     \n      --private\n     \n     \n       When you use this option, the load balancer will only be accessible from within the Layer0 environment. \n     \n   \n   \n     \n      --healthcheck-target  target \n     \n     \n       The target of the check. Valid pattern is  PROTOCOL:PORT/PATH   (default:  \"TCP:80\" ) \n       \n      If PROTOCOL is  HTTP  or  HTTPS , both PORT and PATH are required\n       \n      -  example:  HTTP:80/admin/healthcheck \n       \n      If PROTOCOL is  TCP  or  SSL , PORT is required and PATH is not supported\n       \n      -  example:  TCP:80 \n     \n   \n   \n     \n      --healthcheck-interval  interval \n     \n     \n       The interval between checks  (default:  30 ) . \n     \n   \n   \n     \n      --healthcheck-timeout  timeout \n     \n     \n       The length of time before the check times out  (default:  5 ) . \n     \n   \n   \n     \n      --healthcheck-healthy-threshold  healthyThreshold \n     \n     \n       The number of checks before the instance is declared healthy  (default:  2 ) . \n     \n   \n   \n     \n      --healthcheck-unhealthy-threshold  unhealthyThreshold \n     \n     \n       The number of checks before the instance is declared unhealthy  (default:  2 ) .", 
            "title": "Optional arguments"
        }, 
        {
            "location": "/reference/cli/#loadbalancer-delete", 
            "text": "Use the  delete  subcommand to delete an existing load balancer.", 
            "title": "loadbalancer delete"
        }, 
        {
            "location": "/reference/cli/#usage_21", 
            "text": "l0 loadbalancer delete  [--wait]  loadBalancerName", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#required-parameters_14", 
            "text": "loadBalancerName \n     The name of the load balancer that you want to delete.", 
            "title": "Required parameters"
        }, 
        {
            "location": "/reference/cli/#optional-arguments_5", 
            "text": "--wait \n     Wait until the deletion is complete before exiting.", 
            "title": "Optional arguments"
        }, 
        {
            "location": "/reference/cli/#additional-information_6", 
            "text": "In order to delete a load balancer that is already attached to a service, you must first delete the service that uses the load balancer. \n   \n   \n     This operation performs several tasks asynchronously. When run without the  --wait  option, this operation will most likely exit before all of these tasks are complete; when run with the  --wait  option, this operation will only exit once these tasks have completed.", 
            "title": "Additional information"
        }, 
        {
            "location": "/reference/cli/#loadbalancer-addport", 
            "text": "Use the  addport  subcommand to add a new port configuration to an existing Layer0 load balancer.", 
            "title": "loadbalancer addport"
        }, 
        {
            "location": "/reference/cli/#usage_22", 
            "text": "l0 loadbalancer addport   loadBalancerName hostPort:containerPort/protocol  [--certificate  certificateName ]", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#required-parameters_15", 
            "text": "loadBalancerName \n     The name of an existing Layer0 load balancer in which you want to add the port configuration. \n   \n   \n     hostPort \n     The port that the load balancer will listen on. \n   \n   \n     containerPort \n     The port that the load balancer will forward traffic to. \n   \n   \n     protocol \n     The protocol to use when forwarding traffic (acceptable values: tcp, ssl, http, and https).", 
            "title": "Required parameters"
        }, 
        {
            "location": "/reference/cli/#optional-arguments_6", 
            "text": "--certificate  certificateName \n     The name of an existing Layer0 certificate. You must include this option if you are using an HTTPS port configuration.", 
            "title": "Optional arguments"
        }, 
        {
            "location": "/reference/cli/#additional-information_7", 
            "text": "The port configuration you specify must not already be in use by the load balancer you specify.", 
            "title": "Additional information"
        }, 
        {
            "location": "/reference/cli/#loadbalancer-dropport", 
            "text": "Use the  dropport  subcommand to remove a port configuration from an existing Layer0 load balancer.", 
            "title": "loadbalancer dropport"
        }, 
        {
            "location": "/reference/cli/#usage_23", 
            "text": "l0 loadbalancer dropport   loadBalancerName   hostPort", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#required-parameters_16", 
            "text": "loadBalancerName \n     The name of an existing Layer0 load balancer in which you want to remove the port configuration. \n   \n   \n     hostPort \n     The host port to remove from the load balancer.", 
            "title": "Required parameters"
        }, 
        {
            "location": "/reference/cli/#loadbalancer-get", 
            "text": "Use the  get  subcommand to display information about an existing Layer0 load balancer.", 
            "title": "loadbalancer get"
        }, 
        {
            "location": "/reference/cli/#usage_24", 
            "text": "l0 loadbalancer get   environmentName:loadBalancerName", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#required-parameters_17", 
            "text": "environmentName \n     The name of an existing Layer0 environment. \n   \n   \n     loadBalancerName \n     The name of an existing Layer0 load balancer.", 
            "title": "Required parameters"
        }, 
        {
            "location": "/reference/cli/#additional-information_8", 
            "text": "The  get  subcommand supports wildcard matching:  l0 loadbalancer get entrypoint*  would return all jobs beginning with  entrypoint .", 
            "title": "Additional information"
        }, 
        {
            "location": "/reference/cli/#loadbalancer-list", 
            "text": "Use the  list  subcommand to display information about all of the existing load balancers in an instance of Layer0.", 
            "title": "loadbalancer list"
        }, 
        {
            "location": "/reference/cli/#usage_25", 
            "text": "l0 loadbalancer list", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#loadbalancer-healthcheck", 
            "text": "Use the  healthcheck  subcommand to display information about or update the configuration of a load balancer's health check.", 
            "title": "loadbalancer healthcheck"
        }, 
        {
            "location": "/reference/cli/#usage_26", 
            "text": "l0 loadbalancer healthcheck  [healthcheck-flags]  loadbalancerName", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#optional-arguments_7", 
            "text": "--set-target  target \n     \n     \n       The target of the check. Valid pattern is  PROTOCOL:PORT/PATH , where PROTOCOL values are:\n       \n       HTTP  or  HTTPS : both PORT and PATH are required\n       \n      -  example:  HTTP:80/admin/healthcheck \n       \n       TCP  or  SSL : PORT is required, PATH is not supported\n       \n      -  example:  TCP:80 \n     \n   \n   \n     \n      --set-interval  interval \n     \n     \n       The interval between checks. \n     \n   \n   \n     \n      --set-timeout  timeout \n     \n     \n       The length of time before the check times out. \n     \n   \n   \n     \n      --set-healthy-threshold  healthyThreshold \n     \n     \n       The number of checks before the instance is declared healthy. \n     \n   \n   \n     \n      --set-unhealthy-threshold  unhealthyThreshold \n     \n     \n       The number of checks before the instance is declared unhealthy.", 
            "title": "Optional arguments"
        }, 
        {
            "location": "/reference/cli/#additional-information_9", 
            "text": "Calling the subcommand without flags will display the current configuration of the load balancer's health check. Setting any of the flags will update the corresponding field in the health check, and all omitted flags will leave the corresponding fields unchanged.", 
            "title": "Additional information"
        }, 
        {
            "location": "/reference/cli/#service", 
            "text": "A service is a component of a Layer0 environment. The purpose of a service is to execute a Docker image specified in a  deploy . In order to create a service, you must first create an  environment  and a  deploy ; in most cases, you should also create a  load balancer  before creating the service.  The  service  command is used with the following subcommands:  create ,  delete ,  get ,  update ,  list ,  logs , and  scale .", 
            "title": "Service"
        }, 
        {
            "location": "/reference/cli/#service-create", 
            "text": "Use the  create  subcommand to create a Layer0 service.", 
            "title": "service create"
        }, 
        {
            "location": "/reference/cli/#usage_27", 
            "text": "l0 service create  [--loadbalancer  environmentName:loadBalancerName  ] [--no-logs]  environmentName serviceName deployName:deployVersion", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#required-parameters_18", 
            "text": "serviceName \n     A name for the service that you are creating. \n   \n   \n     environmentName \n     The name of an existing Layer0 environment. \n   \n   \n     deployName \n     The name of a Layer0 deploy that exists in the environment  environmentName . \n   \n   \n     deployVersion \n     The version number of the Layer0 deploy that you want to deploy. If you do not specify a version number, the latest version of the deploy will be used.", 
            "title": "Required parameters"
        }, 
        {
            "location": "/reference/cli/#optional-arguments_8", 
            "text": "--loadbalancer  environmentName:loadBalancerName \n     Place the new service behind an existing load balancer named  loadBalancerName  in the environment named  environmentName . \n   \n   \n     --no-logs \n     Disable cloudwatch logging for the service", 
            "title": "Optional arguments"
        }, 
        {
            "location": "/reference/cli/#service-update", 
            "text": "Use the  update  subcommand to apply an existing Layer0 Deploy to an existing Layer0 service.", 
            "title": "service update"
        }, 
        {
            "location": "/reference/cli/#usage_28", 
            "text": "l0 service update  [--no-logs]  environmentName:serviceName deployName:deployVersion", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#required-parameters_19", 
            "text": "environmentName \n     The name of the Layer0 environment in which the service resides. \n   \n   \n     serviceName \n     The name of an existing Layer0 service into which you want to apply the deploy. \n   \n   \n     deployName \n     The name of the Layer0 deploy that you want to apply to the service. \n   \n   \n     deployVersion \n     The version of the Layer0 deploy that you want to apply to the service. If you do not specify a version number, the latest version of the deploy will be applied. \n   \n   \n     --no-logs \n     Disable cloudwatch logging for the service", 
            "title": "Required parameters"
        }, 
        {
            "location": "/reference/cli/#additional-information_10", 
            "text": "If your service uses a load balancer, when you update the task definition for the service, the container name and container port that were specified when the service was created must remain the same in the task definition. In other words, if your service has a load balancer, you cannot apply any deploy you want to that service. If you are varying the container name or exposed ports, you must create a new service instead.", 
            "title": "Additional information"
        }, 
        {
            "location": "/reference/cli/#service-delete", 
            "text": "Use the  delete  subcommand to delete an existing Layer0 service.", 
            "title": "service delete"
        }, 
        {
            "location": "/reference/cli/#usage_29", 
            "text": "l0 service delete  [--wait]  environmentName:serviceName", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#required-parameters_20", 
            "text": "environmentName \n     The name of the Layer0 environment that contains the service you want to delete. \n   \n   \n     serviceName \n     The name of the Layer0 service that you want to delete.", 
            "title": "Required parameters"
        }, 
        {
            "location": "/reference/cli/#optional-arguments_9", 
            "text": "--wait \n     Wait until the deletion is complete before exiting.", 
            "title": "Optional arguments"
        }, 
        {
            "location": "/reference/cli/#additional-information_11", 
            "text": "This operation performs several tasks asynchronously. When run without the  --wait  option, this operation will most likely exit before all of these tasks are complete; when run with the  --wait  option, this operation will only exit once these tasks have completed.", 
            "title": "Additional information"
        }, 
        {
            "location": "/reference/cli/#service-get", 
            "text": "Use the  get  subcommand to display information about an existing Layer0 service.", 
            "title": "service get"
        }, 
        {
            "location": "/reference/cli/#usage_30", 
            "text": "l0 service get   environmentName:serviceName", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#required-parameters_21", 
            "text": "environmentName \n     The name of an existing Layer0 environment. \n   \n   \n     serviceName \n     The name of an existing Layer0 service.", 
            "title": "Required parameters"
        }, 
        {
            "location": "/reference/cli/#service-list", 
            "text": "Use the  list  subcommand to list all of the existing services in your Layer0 instance.", 
            "title": "service list"
        }, 
        {
            "location": "/reference/cli/#usage_31", 
            "text": "l0 service list", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#service-logs", 
            "text": "Use the  logs  subcommand to display the logs from a Layer0 service that is currently running.", 
            "title": "service logs"
        }, 
        {
            "location": "/reference/cli/#usage_32", 
            "text": "l0 service logs  [--tail= N  ]  serviceName", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#required-parameters_22", 
            "text": "serviceName \n     The name of the Layer0 service for which you want to view logs.", 
            "title": "Required parameters"
        }, 
        {
            "location": "/reference/cli/#optional-arguments_10", 
            "text": "--tail= N \n     Display only the last  N  lines of the log.", 
            "title": "Optional arguments"
        }, 
        {
            "location": "/reference/cli/#service-scale", 
            "text": "Use the  scale  subcommand to specify how many copies of an existing Layer0 service should run.", 
            "title": "service scale"
        }, 
        {
            "location": "/reference/cli/#usage_33", 
            "text": "l0 service scale   environmentName:serviceName N", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#required-parameters_23", 
            "text": "environmentName \n     The name of the Layer0 environment that contains the service that you want to scale. \n   \n   \n     serviceName \n     The name of the Layer0 service that you want to scale up. \n   \n   \n     N \n     The number of copies of the specified service that should be run.", 
            "title": "Required parameters"
        }, 
        {
            "location": "/reference/cli/#task", 
            "text": "A Layer0 task is a component of an environment. A task executes the contents of a Docker image, as specified in a deploy. A task differs from a service in that a task does not restart after exiting. Additionally, ports are not exposed when using a task.  The  task  command is used with the following subcommands:  create ,  delete ,  get ,  list , and  logs .", 
            "title": "Task"
        }, 
        {
            "location": "/reference/cli/#task-create", 
            "text": "Use the  create  subcommand to create a Layer0 task.", 
            "title": "task create"
        }, 
        {
            "location": "/reference/cli/#usage_34", 
            "text": "l0 task create  [--no-logs] [--copies  copies ]  environmentName taskName deployName", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#required-parameters_24", 
            "text": "environmentName \n     The name of the existing Layer0 environment in which you want to create the task. \n   \n   \n     taskName \n     A name for the task. \n   \n   \n     deployName \n     The name of an existing Layer0 deploy that the task should use.", 
            "title": "Required parameters"
        }, 
        {
            "location": "/reference/cli/#optional-arguments_11", 
            "text": "--copies \n     The number of copies of the task to run (default: 1) \n   \n   \n     --no-logs \n     Disable cloudwatch logging for the service", 
            "title": "Optional arguments"
        }, 
        {
            "location": "/reference/cli/#task-delete", 
            "text": "Use the  delete  subcommand to delete an existing Layer0 task.", 
            "title": "task delete"
        }, 
        {
            "location": "/reference/cli/#usage_35", 
            "text": "l0 task delete  [ environmentName :] taskName", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#required-parameters_25", 
            "text": "taskName \n     The name of the Layer0 task that you want to delete.", 
            "title": "Required parameters"
        }, 
        {
            "location": "/reference/cli/#optional-parameters", 
            "text": "[ environmentName :] \n     The name of the Layer0 environment that contains the task. This parameter is only necessary if multiple environments contain tasks with exactly the same name.", 
            "title": "Optional parameters"
        }, 
        {
            "location": "/reference/cli/#additional-information_12", 
            "text": "Until the record has been purged, the API may indicate that the task is still running. Task records are typically purged within an hour.", 
            "title": "Additional information"
        }, 
        {
            "location": "/reference/cli/#task-get", 
            "text": "Use the  get  subcommand to display information about an existing Layer0 task ( taskName ).", 
            "title": "task get"
        }, 
        {
            "location": "/reference/cli/#usage_36", 
            "text": "l0 task get  [ environmentName :] taskName", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#required-parameters_26", 
            "text": "taskName \n     The name of a Layer0 task for which you want to see information.", 
            "title": "Required parameters"
        }, 
        {
            "location": "/reference/cli/#additional-information_13", 
            "text": "The value of  taskName  does not need to exactly match the name of an existing task. If multiple results are found that match the pattern you specified in  taskName , then information about all matching tasks will be returned.", 
            "title": "Additional information"
        }, 
        {
            "location": "/reference/cli/#task-list", 
            "text": "Use the  task  subcommand to display a list of running tasks in your Layer0.", 
            "title": "task list"
        }, 
        {
            "location": "/reference/cli/#usage_37", 
            "text": "l0 task list", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#task-logs", 
            "text": "Use the  logs  subcommand to display logs for a running Layer0 task.", 
            "title": "task logs"
        }, 
        {
            "location": "/reference/cli/#usage_38", 
            "text": "l0 task logs  [--tail= N  ]  taskName", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#required-parameters_27", 
            "text": "taskName \n     The name of an existing Layer0 task.", 
            "title": "Required parameters"
        }, 
        {
            "location": "/reference/cli/#additional-information_14", 
            "text": "The value of  taskName  does not need to exactly match the name of an existing task. If multiple results are found that match the pattern you specified in  taskName , then information about all matching tasks will be returned.", 
            "title": "Additional information"
        }, 
        {
            "location": "/reference/cli/#task-list_1", 
            "text": "Use the  task  subcommand to display a list of running tasks in your Layer0.", 
            "title": "task list"
        }, 
        {
            "location": "/reference/cli/#usage_39", 
            "text": "l0 task list \n    \n=======", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/setup-cli/", 
            "text": "Layer0 Setup (l0-setup) command-line interface reference\n#\n\n\nThe \nl0-setup\n application is designed to be used with one of several commands: \napply\n, \nbackup\n, \ndestroy\n, \nendpoint\n, \nplan\n, \nrestore\n, \nterraform\n, and \nvpc\n. These commands are detailed in the sections below.\n\n\nGeneral Usage\n#\n\n\n\n  \n\n    \nl0-setup\n [--version] \ncommand\n [\noptions\n] [\nparameters\n]\n\n  \n\n\n\n\n\n\nApply\n#\n\n\nThe \napply\n command is used to create and update Layer0 instances.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0-setup apply\n [--force] [--access_key=\nawsAccessKeyID\n] [--secret_key=\nawsSecretAccessKeyID\n] [--region=\nawsRegion\n] [--dockercfg=\nfilepath\n] [--vpc=\nvpcID\n] \nprefixName\n\n  \n\n\n\n\nRequired parameters\n#\n\n\n\n  \n\n    \nprefixName\n\n    \nThe name of a Layer0Prefix in an existing AWS stack. To learn more about creating a stack, see the \ninstallation guide\n\n  \n\n\n\n\nOptional arguments\n#\n\n\n\n  \n\n    \n--force\n\n    \nAvoid prompting for a missing dockercfg file.\n\n  \n\n  \n\n    \n--access_key=\nawsAccessKeyID\n\n    \nThe Access Key ID of an IAM user associated with the AWS stack in which you are creating the Layer0 instance.\n\n  \n\n  \n\n    \n--secret_key=\nawsSecretAccessKeyID\n\n    \nThe Secret Access Key ID of an IAM user associated with the AWS stack in which you are creating the Layer0 instance.\n\n  \n\n  \n\n    \n--region=\nawsRegion\n\n    \nThe AWS region in which the Layer0 instance resides.\n\n  \n\n  \n\n    \n--dockercfg=\nfilepath\n\n    \nThe path to a valid Docker configuration file (likely found at \n~/.docker/config.json\n for Docker versions 1.7 or greater, or \n~/.dockercfg\n for versions less than 1.7).\n\n  \n\n  \n\n    \n--vpc=\nvpcID\n\n    \nThe ID of a VPC. If blank, \nl0-setup\n will create a new VPC.\n\n  \n\n\n\n\n\n\nBackup\n#\n\n\nThe \nbackup\n command is used to back up your Layer0 configuration files to an S3 bucket. This command is most often used when migrating between versions of Layer0. The \nbackup\n command also runs automatically every time you execute the \nl0-setup apply\n command.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0-setup backup\n \nprefixName\n\n  \n\n\n\n\nRequired parameters\n#\n\n\n\n  \n\n    \nprefixName\n\n    \nThe name of the Layer0Prefix that you want to back up.\n\n  \n\n\n\n\n\n\nDestroy\n#\n\n\nThe \ndestroy\n command is used to delete a Layer0 configuration.\n\n\nBefore you can run the \ndestroy\n command, you must first delete any existing \nloadbalancers\n, \nservices\n and \nenvironments\n that exist in the Layer0 instance, using the \ndelete\n subcommands for each of those components.\n\n\n\n\nCaution\n\n\nDestroying a Layer0 instance cannot be undone; if you created backups of your Layer0 configuration using the \nbackup\n command, those backups will also be deleted when you run the \ndestroy\n command. The only way to re-deploy a destroyed instance is to use the procedures in the \ninstallation guide\n to rebuild it.\n\n\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0-setup destroy\n \nprefixName\n\n  \n\n\n\n\nRequired parameters\n#\n\n\n\n  \n\n    \nprefixName\n\n    \nThe name of a Layer0Prefix in an existing AWS stack.\n\n  \n\n\n\n\nOptional arguments\n#\n\n\n\n  \n\n    \n--force=false\n\n    \nWhen specified, destroy confirmation prompts will not be shown.\n\n  \n\n\n\n\n\n\nEndpoint\n#\n\n\nThe \nendpoint\n command is used to look up the details of a Layer0 endpoint so that you can export them to your shell.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0-setup endpoint\n [-diq] [-s \nsyntax\n] \nprefixName\n\n  \n\n\n\n\nRequired parameters\n#\n\n\n\n  \n\n    \nprefixName\n\n    \nThe name of a Layer0Prefix for which you want to view endpoint information.\n\n  \n\n\n\n\nOptional arguments\n#\n\n\n\n  \n\n    \n-d, --dev=false\n\n    \nShow configuration variables required for local development\n\n  \n\n  \n\n    \n-i, --insecure=false\n\n    \nAllow incomplete SSL configuration. This option is not recommended for production use.\n\n  \n\n  \n\n    \n-q, --quiet=false\n\n    \nSilence CLI and API version mismatch warning messages\n\n  \n\n  \n\n    \n-s, --syntax=\"bash\"\n\n    \nShow commands using the specified syntax (bash, powershell, cmd)\n\n  \n\n\n\n\n\n\nPlan\n#\n\n\nThe \nplan\n command is used to dry-run an \napply\n, displaying what \nwould\n happen without actually making any changes.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0-setup plan\n \nprefixName\n \nterraformArguments\n\n  \n\n\n\n\nRequired parameters\n#\n\n\n\n  \n\n    \nprefixName\n\n    \nThe name of a Layer0Prefix for which you want to plan an update.\n\n  \n\n\n\n\nOptional arguments\n#\n\n\n\n  \n\n    \nterraformArguments\n\n    \nThe terraform arguments to pass to Layer0. Arguments passed in this way must follow the syntax specified in the \nTerraform Commands (CLI) Documentation\n.\n\n  \n\n\n\n\n\n\nRestore\n#\n\n\nThe \nrestore\n command is used to restore Layer0 configuration files that were previously backed up to an S3 bucket using the \nbackup\n command.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0-setup restore\n [--access_key=\nawsAccessKeyID\n] [--secret_key=\nawsSecretAccessKeyID\n] [--region=\nawsRegion\n] \nprefixName\n\n  \n\n\n\n\nRequired parameters\n#\n\n\n\n  \n\n    \nprefixName\n\n    \nThe name of the Layer0Prefix that you want to restore.\n\n  \n\n\n\n\nOptional arguments\n#\n\n\n\n  \n\n    \n--access_key=\nawsAccessKeyID\n\n    \nThe Access Key ID of an IAM user associated with the AWS stack in which the Layer0 instance was created.\n\n  \n\n  \n\n    \n--secret_key=\nawsSecretAccessKeyID\n\n    \nThe Secret Access Key ID of an IAM user associated with the AWS stack in which the Layer0 instance was created.\n\n  \n\n  \n\n    \n--region=\nawsRegion\n\n    \nThe AWS region in which the Layer0 instance resides.\n\n  \n\n\n\n\n\n\nTerraform\n#\n\n\nThe \nterraform\n command is used to issue terraform commands directly to a Layer0 instance. Some terraform commands enable functionality that is not a standard part of the \nl0-setup\n application.\n\n\nFor more information about the capabilities and syntax of terraform commands, see the \nTerraform Commands (CLI) Documentation\n.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0-setup terraform\n \nprefixName\n \nterraformArguments\n\n  \n\n\n\n\nRequired parameters\n#\n\n\n\n  \n\n    \nprefixName\n\n    \nThe name of a Layer0Prefix in which you want to execute terraform commands.\n\n  \n\n  \n\n    \nterraformArguments\n\n    \nThe terraform arguments to pass to Layer0. Arguments passed in this way must follow the syntax specified in the \nTerraform Commands (CLI) Documentation\n.\n\n  \n\n\n\n\n\n\nVPC\n#\n\n\nThe \nvpc\n command is used to look up details from a Virtual Private Cloud (VPC) instance.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0-setup vpc\n [--access_key=\nawsAccessKeyID\n] [--secret_key=\nawsSecretAccessKeyID\n] [--region=\nawsRegion\n] \nprefixName\n\n  \n\n\n\n\nRequired parameters\n#\n\n\n\n  \n\n    \nprefixName\n\n    \nThe name of a Layer0Prefix for which you want to view VPC information.\n\n  \n\n\n\n\nOptional arguments\n#\n\n\n\n  \n\n    \n--access_key=\nawsAccessKeyID\n\n    \nThe Access Key ID of an IAM user associated with the AWS stack in which you are creating the Layer0 instance.\n\n  \n\n  \n\n    \n--secret_key=\nawsSecretAccessKeyID\n\n    \nThe Secret Access Key ID of an IAM user associated with the AWS stack in which you are creating the Layer0 instance.\n\n  \n\n  \n\n    \n--region=\nawsRegion\n\n    \nThe AWS region in which the Layer0 instance resides.", 
            "title": "Layer0 Setup CLI"
        }, 
        {
            "location": "/reference/setup-cli/#layer0-setup-l0-setup-command-line-interface-reference", 
            "text": "The  l0-setup  application is designed to be used with one of several commands:  apply ,  backup ,  destroy ,  endpoint ,  plan ,  restore ,  terraform , and  vpc . These commands are detailed in the sections below.", 
            "title": "Layer0 Setup (l0-setup) command-line interface reference"
        }, 
        {
            "location": "/reference/setup-cli/#general-usage", 
            "text": "l0-setup  [--version]  command  [ options ] [ parameters ]", 
            "title": "General Usage"
        }, 
        {
            "location": "/reference/setup-cli/#apply", 
            "text": "The  apply  command is used to create and update Layer0 instances.", 
            "title": "Apply"
        }, 
        {
            "location": "/reference/setup-cli/#usage", 
            "text": "l0-setup apply  [--force] [--access_key= awsAccessKeyID ] [--secret_key= awsSecretAccessKeyID ] [--region= awsRegion ] [--dockercfg= filepath ] [--vpc= vpcID ]  prefixName", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/setup-cli/#required-parameters", 
            "text": "prefixName \n     The name of a Layer0Prefix in an existing AWS stack. To learn more about creating a stack, see the  installation guide", 
            "title": "Required parameters"
        }, 
        {
            "location": "/reference/setup-cli/#optional-arguments", 
            "text": "--force \n     Avoid prompting for a missing dockercfg file. \n   \n   \n     --access_key= awsAccessKeyID \n     The Access Key ID of an IAM user associated with the AWS stack in which you are creating the Layer0 instance. \n   \n   \n     --secret_key= awsSecretAccessKeyID \n     The Secret Access Key ID of an IAM user associated with the AWS stack in which you are creating the Layer0 instance. \n   \n   \n     --region= awsRegion \n     The AWS region in which the Layer0 instance resides. \n   \n   \n     --dockercfg= filepath \n     The path to a valid Docker configuration file (likely found at  ~/.docker/config.json  for Docker versions 1.7 or greater, or  ~/.dockercfg  for versions less than 1.7). \n   \n   \n     --vpc= vpcID \n     The ID of a VPC. If blank,  l0-setup  will create a new VPC.", 
            "title": "Optional arguments"
        }, 
        {
            "location": "/reference/setup-cli/#backup", 
            "text": "The  backup  command is used to back up your Layer0 configuration files to an S3 bucket. This command is most often used when migrating between versions of Layer0. The  backup  command also runs automatically every time you execute the  l0-setup apply  command.", 
            "title": "Backup"
        }, 
        {
            "location": "/reference/setup-cli/#usage_1", 
            "text": "l0-setup backup   prefixName", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/setup-cli/#required-parameters_1", 
            "text": "prefixName \n     The name of the Layer0Prefix that you want to back up.", 
            "title": "Required parameters"
        }, 
        {
            "location": "/reference/setup-cli/#destroy", 
            "text": "The  destroy  command is used to delete a Layer0 configuration.  Before you can run the  destroy  command, you must first delete any existing  loadbalancers ,  services  and  environments  that exist in the Layer0 instance, using the  delete  subcommands for each of those components.   Caution  Destroying a Layer0 instance cannot be undone; if you created backups of your Layer0 configuration using the  backup  command, those backups will also be deleted when you run the  destroy  command. The only way to re-deploy a destroyed instance is to use the procedures in the  installation guide  to rebuild it.", 
            "title": "Destroy"
        }, 
        {
            "location": "/reference/setup-cli/#usage_2", 
            "text": "l0-setup destroy   prefixName", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/setup-cli/#required-parameters_2", 
            "text": "prefixName \n     The name of a Layer0Prefix in an existing AWS stack.", 
            "title": "Required parameters"
        }, 
        {
            "location": "/reference/setup-cli/#optional-arguments_1", 
            "text": "--force=false \n     When specified, destroy confirmation prompts will not be shown.", 
            "title": "Optional arguments"
        }, 
        {
            "location": "/reference/setup-cli/#endpoint", 
            "text": "The  endpoint  command is used to look up the details of a Layer0 endpoint so that you can export them to your shell.", 
            "title": "Endpoint"
        }, 
        {
            "location": "/reference/setup-cli/#usage_3", 
            "text": "l0-setup endpoint  [-diq] [-s  syntax ]  prefixName", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/setup-cli/#required-parameters_3", 
            "text": "prefixName \n     The name of a Layer0Prefix for which you want to view endpoint information.", 
            "title": "Required parameters"
        }, 
        {
            "location": "/reference/setup-cli/#optional-arguments_2", 
            "text": "-d, --dev=false \n     Show configuration variables required for local development \n   \n   \n     -i, --insecure=false \n     Allow incomplete SSL configuration. This option is not recommended for production use. \n   \n   \n     -q, --quiet=false \n     Silence CLI and API version mismatch warning messages \n   \n   \n     -s, --syntax=\"bash\" \n     Show commands using the specified syntax (bash, powershell, cmd)", 
            "title": "Optional arguments"
        }, 
        {
            "location": "/reference/setup-cli/#plan", 
            "text": "The  plan  command is used to dry-run an  apply , displaying what  would  happen without actually making any changes.", 
            "title": "Plan"
        }, 
        {
            "location": "/reference/setup-cli/#usage_4", 
            "text": "l0-setup plan   prefixName   terraformArguments", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/setup-cli/#required-parameters_4", 
            "text": "prefixName \n     The name of a Layer0Prefix for which you want to plan an update.", 
            "title": "Required parameters"
        }, 
        {
            "location": "/reference/setup-cli/#optional-arguments_3", 
            "text": "terraformArguments \n     The terraform arguments to pass to Layer0. Arguments passed in this way must follow the syntax specified in the  Terraform Commands (CLI) Documentation .", 
            "title": "Optional arguments"
        }, 
        {
            "location": "/reference/setup-cli/#restore", 
            "text": "The  restore  command is used to restore Layer0 configuration files that were previously backed up to an S3 bucket using the  backup  command.", 
            "title": "Restore"
        }, 
        {
            "location": "/reference/setup-cli/#usage_5", 
            "text": "l0-setup restore  [--access_key= awsAccessKeyID ] [--secret_key= awsSecretAccessKeyID ] [--region= awsRegion ]  prefixName", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/setup-cli/#required-parameters_5", 
            "text": "prefixName \n     The name of the Layer0Prefix that you want to restore.", 
            "title": "Required parameters"
        }, 
        {
            "location": "/reference/setup-cli/#optional-arguments_4", 
            "text": "--access_key= awsAccessKeyID \n     The Access Key ID of an IAM user associated with the AWS stack in which the Layer0 instance was created. \n   \n   \n     --secret_key= awsSecretAccessKeyID \n     The Secret Access Key ID of an IAM user associated with the AWS stack in which the Layer0 instance was created. \n   \n   \n     --region= awsRegion \n     The AWS region in which the Layer0 instance resides.", 
            "title": "Optional arguments"
        }, 
        {
            "location": "/reference/setup-cli/#terraform", 
            "text": "The  terraform  command is used to issue terraform commands directly to a Layer0 instance. Some terraform commands enable functionality that is not a standard part of the  l0-setup  application.  For more information about the capabilities and syntax of terraform commands, see the  Terraform Commands (CLI) Documentation .", 
            "title": "Terraform"
        }, 
        {
            "location": "/reference/setup-cli/#usage_6", 
            "text": "l0-setup terraform   prefixName   terraformArguments", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/setup-cli/#required-parameters_6", 
            "text": "prefixName \n     The name of a Layer0Prefix in which you want to execute terraform commands. \n   \n   \n     terraformArguments \n     The terraform arguments to pass to Layer0. Arguments passed in this way must follow the syntax specified in the  Terraform Commands (CLI) Documentation .", 
            "title": "Required parameters"
        }, 
        {
            "location": "/reference/setup-cli/#vpc", 
            "text": "The  vpc  command is used to look up details from a Virtual Private Cloud (VPC) instance.", 
            "title": "VPC"
        }, 
        {
            "location": "/reference/setup-cli/#usage_7", 
            "text": "l0-setup vpc  [--access_key= awsAccessKeyID ] [--secret_key= awsSecretAccessKeyID ] [--region= awsRegion ]  prefixName", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/setup-cli/#required-parameters_7", 
            "text": "prefixName \n     The name of a Layer0Prefix for which you want to view VPC information.", 
            "title": "Required parameters"
        }, 
        {
            "location": "/reference/setup-cli/#optional-arguments_5", 
            "text": "--access_key= awsAccessKeyID \n     The Access Key ID of an IAM user associated with the AWS stack in which you are creating the Layer0 instance. \n   \n   \n     --secret_key= awsSecretAccessKeyID \n     The Secret Access Key ID of an IAM user associated with the AWS stack in which you are creating the Layer0 instance. \n   \n   \n     --region= awsRegion \n     The AWS region in which the Layer0 instance resides.", 
            "title": "Optional arguments"
        }, 
        {
            "location": "/reference/terraform-plugin/", 
            "text": "Layer0 Terraform Provider Reference\n#\n\n\nTerraform is an open-source tool for provisioning and managing infrastructure.\nIf you are new to Terraform, we recommend checking out their \ndocumentation\n.\n\n\nLayer0 has built a custom \nprovider\n for Layer0.\nThis provider allows users to create, manage, and update Layer0 entities using Terraform.\n\n\nPrerequisites\n#\n\n\nTo use the Layer0 Terraform provider plugin, first ensure that you have a copy of Terraform and that it is accessible in your system path. A Terraform binary can be found in the \n/bin\n directory of a \nLayer0 release\n. Alternatively, you can \ndownload Terraform\n for the latest version.\n\n\nThe Layer0 Terraform provider plugin in compatible with Terraform v0.2+.\n\n\nInstall\n#\n\n\nDownload a Layer0 v0.8.4+ \nrelease\n. The Terraform plugin binary is located in the release zip file as \nterraform-provider-layer0\n. Copy this \nterraform-provider-layer0\n binary into the same directory you installed Terraform - and you're done!\n\n\nFor further information, see Terraform's documentation on installing a Terraform plugin \nhere\n.\n\n\nGetting Started\n#\n\n\n\n\nCheckout the \nTerraform\n section of the Guestbook walkthrough \nhere\n.\n\n\nWe've added some tips and links to helpful resources in the \nBest Practices\n section below.\n\n\n\n\n\n\nProvider\n#\n\n\nThe Layer0 provider is used to interact with a Layer0 API.\nThe provider needs to be configured with the proper credentials before it can be used.\n\n\nExample Usage\n#\n\n\n# Add 'endpoint' and 'token' variables\nvariable \nendpoint\n {}\n\nvariable \ntoken\n {}\n\n# Configure the layer0 provider\nprovider \nlayer0\n {\n  endpoint        = \n${var.endpoint}\n\n  token           = \n${var.token}\n\n  skip_ssl_verify = true\n}\n\n\n\n\nArgument Reference\n#\n\n\nThe following arguments are supported:\n\n\n\n\nConfiguration\n\n\nThe \nendpoint\n and \ntoken\n variables for your layer0 api can be found using the \nl0-setup endpoint\n command\n\n\n\n\n\n\nendpoint\n - (Required) The endpoint of the layer0 api\n\n\ntoken\n - (Required) The authentication token for the layer0 api\n\n\nskip_ssl_verify\n - (Optional) If true, ssl certificate mismatch warnings will be ignored\n\n\n\n\n\n\nAPI Data Source\n#\n\n\nThe API data source is used to extract useful read-only variables from the Layer0 API.\n\n\nExample Usage\n#\n\n\n# Configure the api data source\ndata \nlayer0_api\n \nconfig\n {}\n\n# Output the layer0 vpc id\noutput \nvpc id\n {\n  val = \n${data.layer0_api.vpc_id}\n\n}\n\n\n\n\nAttribute Reference\n#\n\n\nThe following attributes are exported:\n\n\n\n\nprefix\n - The prefix of the layer0 instance\n\n\nvpc_id\n - The vpc id of the layer0 instance\n\n\npublic_subnets\n - A list containing the 2 public subnet ids in the layer0 vpc\n\n\nprivate_subnets\n - A list containing the 2 private subnet ids in the layer0 vpc\n\n\n\n\n\n\nDeploy\n#\n\n\nProvides a Layer0 Deploy.\n\n\nPerforming variable substitution inside of your deploy's json file (typically named \nDockerrun.aws.json\n) can be done through Terraform's \ntemplate_file\n.\nFor a working example, please see the sample \nGuestbook\n application\n\n\nExample Usage\n#\n\n\n# Configure the deploy template\ndata \ntemplate_file\n \nguestbook\n {\n  template = \n${file(\nDockerrun.aws.json\n)}\n\n  vars {\n    docker_image_tag = \nlatest\n\n  }\n}\n\n# Create a deploy using the rendered template\nresource \nlayer0_deploy\n \nguestbook\n {\n  name    = \nguestbook\n\n  content = \n${data.template_file.guestbook.rendered}\n\n}\n\n\n\n\nArgument Reference\n#\n\n\nThe following arguments are supported:\n\n\n\n\nname\n - (Required) The name of the deploy\n\n\ncontent\n - (Required) The content of the deploy\n\n\n\n\nAttribute Reference\n#\n\n\nThe following attributes are exported:\n\n\n\n\nid\n - The id of the deploy\n\n\nname\n - The name of the deploy\n\n\nversion\n - The version number of the deploy\n\n\n\n\n\n\nEnvironment\n#\n\n\nProvides a Layer0 Environment\n\n\nExample Usage\n#\n\n\n# Create a new environment\nresource \nlayer0_environment\n \ndemo\n {\n  name      = \ndemo\n\n  size      = \nm3.medium\n\n  min_count = 0\n  user_data = \necho hello, world\n\n  os        = \nlinux\n\n  ami       = \nami123\n\n}\n\n\n\n\nArgument Reference\n#\n\n\nThe following arguments are supported:\n\n\n\n\nname\n - (Required) The name of the environment\n\n\nsize\n - (Optional, Default: \"m3.medium\") The size of the instances in the environment.\nAvailable instance sizes can be found \nhere\n\n\nmin_count\n - (Optional, Default: 0) The minimum number of instances allowed in the environment\n\n\nuser-data\n - (Optional) The user data template to use for the environment's autoscaling group.\nSee the \ncli reference\n for the default template.\n\n\nos\n - (Optional, Default: \"linux\") Specifies the type of operating system used in the environment.\nOptions are \"linux\" or \"windows\".\n\n\nami\n - (Optional) A custom AMI ID to use in the environment. \nIf not specified, Layer0 will use its default AMI ID for the specified operating system.\n\n\n\n\nAttribute Reference\n#\n\n\nThe following attributes are exported:\n\n\n\n\nid\n - The id of the environment\n\n\nname\n - The name of the environment\n\n\nsize\n - The size of the instances in the environment\n\n\ncluster_count\n - The current number instances in the environment\n\n\nsecurity_group_id\n - The ID of the environment's security group\n\n\nos\n - The operating system used for the environment\n\n\nami\n - The AMI ID used for the environment\n\n\n\n\n\n\nLoad Balancer\n#\n\n\nProvides a Layer0 Load Balancer\n\n\nExample Usage\n#\n\n\n# Create a new load balancer\nresource \nlayer0_load_balancer\n \nguestbook\n {\n  name        = \nguestbook\n\n  environment = \ndemo123\n\n  private     = false\n\n  port {\n    host_port      = 80\n    container_port = 80\n    protocol       = \nhttp\n\n  }\n\n  port {\n    host_port      = 443\n    container_port = 443\n    protocol       = \nhttps\n\n    certificate    = \ncert\n\n  }\n\n  health_check {\n    target              = \ntcp:80\n\n    interval            = 30\n    timeout             = 5\n    healthy_threshold   = 2\n    unhealthy_threshold = 2\n  }\n}\n\n\n\n\nArgument Reference\n#\n\n\nThe following arguments are supported:\n\n\n\n\nname\n - (Required) The name of the load balancer\n\n\nenvironment\n - (Required) The id of the environment to place the load balancer inside of\n\n\nprivate\n - (Optional) If true, the load balancer will not be exposed to the public internet\n\n\nport\n - (Optional, Default: 80:80/tcp) A list of port blocks. Ports documented below\n\n\nhealth_check\n - (Optional, Default: {\"TCP:80\" 30 5 2 2}) A health_check block. Health check documented below\n\n\n\n\nPorts (\nport\n) support the following:\n\n\n\n\nhost_port\n - (Required) The port on the load balancer to listen on\n\n\ncontainer_port\n - (Required) The port on the docker container to route to\n\n\nprotocol\n - (Required) The protocol to listen on. Valid values are \nHTTP, HTTPS, TCP, or SSL\n\n\ncertificate\n - (Optional) The name of an SSL certificate. Only required if the \nHTTP\n or \nSSL\n protocol is used.\n\n\n\n\nHealthcheck (\nhealth_check\n) supports the following:\n\n\n\n\ntarget\n - (Required) The target of the check. Valid pattern is \"${PROTOCOL}:${PORT}${PATH}\", where PROTOCOL values are:\n\n\nHTTP\n, \nHTTPS\n - PORT and PATH are required\n\n\nTCP\n, \nSSL\n - PORT is required, PATH is not supported\n\n\n\n\n\n\ninterval\n - (Required) The interval between checks.\n\n\ntimeout\n - (Required) The length of time before the check times out.\n\n\nhealthy_threshold\n - (Required) The number of checks before the instance is declared healthy.\n\n\nunhealthy_threshold\n - (Required) The number of checks before the instance is declared unhealthy.\n\n\n\n\nAttribute Reference\n#\n\n\nThe following attributes are exported:\n\n\n\n\nid\n - The id of the load balancer\n\n\nname\n - The name of the load balancer\n\n\nenvironment\n - The id of the environment the load balancer exists in\n\n\nprivate\n - Whether or not the load balancer is private\n\n\nurl\n - The URL of the load balancer\n\n\n\n\n\n\nService\n#\n\n\nProvides a Layer0 Service\n\n\nExample Usage\n#\n\n\n# Create a new service\nresource \nlayer0_service\n \nguestbook\n {\n  name          = \nguestbook\n\n  environment   = \nenvironment123\n\n  deploy        = \ndeploy123\n\n  load_balancer = \nloadbalancer123\n\n  scale         = 3\n  wait          = true\n}\n\n\n\n\nArgument Reference\n#\n\n\nThe following arguments are supported:\n\n\n\n\nname\n - (Required) The name of the service\n\n\nenvironment\n - (Required) The id of the environment to place the service inside of\n\n\ndeploy\n - (Required) The id of the deploy for the service to run\n\n\nload_balancer\n (Optional) The id of the load balancer to place the service behind\n\n\nscale\n (Optional, Default: 1) The number of copies of the service to run\n\n\nwait\n (Optional) If true, will wait until the service's deployment completes before returning\n\n\n\n\nAttribute Reference\n#\n\n\nThe following attributes are exported:\n\n\n\n\nid\n - The id of the service\n\n\nname\n - The name of the service\n\n\nenvironment\n - The id of the environment the service exists in\n\n\ndeploy\n - The id of the deploy the service is running\n\n\nload_balancer\n - The id of the load balancer the service is behind (if \nload_balancer\n was set)\n\n\nscale\n - The current desired scale of the service\n\n\n\n\n\n\nBest Practices\n#\n\n\n\n\nAlways run \nTerraform plan\n before \nterraform apply\n.\nThis will show you what action(s) Terraform plans to make before actually executing them.\n\n\nUse \nvariables\n to reference secrets.\nSecrets can be placed in a file named \nTerraform.tfvars\n, or by setting \nTF_VAR_*\n environment variables.\nMore information can be found \nhere\n.\n\n\nUse Terraform's \nremote\n command to backup and sync your \nterraform.tfstate\n file across different members in your organization.\nTerraform has documentation for using S3 as a backend \nhere\n.\n\n\nTerraform \nmodules\n allow you to define and consume reusable components.\n\n\nExample configurations can be found \nhere", 
            "title": "Layer0 Terraform Plugin"
        }, 
        {
            "location": "/reference/terraform-plugin/#layer0-terraform-provider-reference", 
            "text": "Terraform is an open-source tool for provisioning and managing infrastructure.\nIf you are new to Terraform, we recommend checking out their  documentation .  Layer0 has built a custom  provider  for Layer0.\nThis provider allows users to create, manage, and update Layer0 entities using Terraform.", 
            "title": "Layer0 Terraform Provider Reference"
        }, 
        {
            "location": "/reference/terraform-plugin/#prerequisites", 
            "text": "To use the Layer0 Terraform provider plugin, first ensure that you have a copy of Terraform and that it is accessible in your system path. A Terraform binary can be found in the  /bin  directory of a  Layer0 release . Alternatively, you can  download Terraform  for the latest version.  The Layer0 Terraform provider plugin in compatible with Terraform v0.2+.", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/reference/terraform-plugin/#install", 
            "text": "Download a Layer0 v0.8.4+  release . The Terraform plugin binary is located in the release zip file as  terraform-provider-layer0 . Copy this  terraform-provider-layer0  binary into the same directory you installed Terraform - and you're done!  For further information, see Terraform's documentation on installing a Terraform plugin  here .", 
            "title": "Install"
        }, 
        {
            "location": "/reference/terraform-plugin/#getting-started", 
            "text": "Checkout the  Terraform  section of the Guestbook walkthrough  here .  We've added some tips and links to helpful resources in the  Best Practices  section below.", 
            "title": "Getting Started"
        }, 
        {
            "location": "/reference/terraform-plugin/#provider", 
            "text": "The Layer0 provider is used to interact with a Layer0 API.\nThe provider needs to be configured with the proper credentials before it can be used.", 
            "title": "Provider"
        }, 
        {
            "location": "/reference/terraform-plugin/#example-usage", 
            "text": "# Add 'endpoint' and 'token' variables\nvariable  endpoint  {}\n\nvariable  token  {}\n\n# Configure the layer0 provider\nprovider  layer0  {\n  endpoint        =  ${var.endpoint} \n  token           =  ${var.token} \n  skip_ssl_verify = true\n}", 
            "title": "Example Usage"
        }, 
        {
            "location": "/reference/terraform-plugin/#argument-reference", 
            "text": "The following arguments are supported:   Configuration  The  endpoint  and  token  variables for your layer0 api can be found using the  l0-setup endpoint  command    endpoint  - (Required) The endpoint of the layer0 api  token  - (Required) The authentication token for the layer0 api  skip_ssl_verify  - (Optional) If true, ssl certificate mismatch warnings will be ignored", 
            "title": "Argument Reference"
        }, 
        {
            "location": "/reference/terraform-plugin/#api-data-source", 
            "text": "The API data source is used to extract useful read-only variables from the Layer0 API.", 
            "title": "API Data Source"
        }, 
        {
            "location": "/reference/terraform-plugin/#example-usage_1", 
            "text": "# Configure the api data source\ndata  layer0_api   config  {}\n\n# Output the layer0 vpc id\noutput  vpc id  {\n  val =  ${data.layer0_api.vpc_id} \n}", 
            "title": "Example Usage"
        }, 
        {
            "location": "/reference/terraform-plugin/#attribute-reference", 
            "text": "The following attributes are exported:   prefix  - The prefix of the layer0 instance  vpc_id  - The vpc id of the layer0 instance  public_subnets  - A list containing the 2 public subnet ids in the layer0 vpc  private_subnets  - A list containing the 2 private subnet ids in the layer0 vpc", 
            "title": "Attribute Reference"
        }, 
        {
            "location": "/reference/terraform-plugin/#deploy", 
            "text": "Provides a Layer0 Deploy.  Performing variable substitution inside of your deploy's json file (typically named  Dockerrun.aws.json ) can be done through Terraform's  template_file .\nFor a working example, please see the sample  Guestbook  application", 
            "title": "Deploy"
        }, 
        {
            "location": "/reference/terraform-plugin/#example-usage_2", 
            "text": "# Configure the deploy template\ndata  template_file   guestbook  {\n  template =  ${file( Dockerrun.aws.json )} \n  vars {\n    docker_image_tag =  latest \n  }\n}\n\n# Create a deploy using the rendered template\nresource  layer0_deploy   guestbook  {\n  name    =  guestbook \n  content =  ${data.template_file.guestbook.rendered} \n}", 
            "title": "Example Usage"
        }, 
        {
            "location": "/reference/terraform-plugin/#argument-reference_1", 
            "text": "The following arguments are supported:   name  - (Required) The name of the deploy  content  - (Required) The content of the deploy", 
            "title": "Argument Reference"
        }, 
        {
            "location": "/reference/terraform-plugin/#attribute-reference_1", 
            "text": "The following attributes are exported:   id  - The id of the deploy  name  - The name of the deploy  version  - The version number of the deploy", 
            "title": "Attribute Reference"
        }, 
        {
            "location": "/reference/terraform-plugin/#environment", 
            "text": "Provides a Layer0 Environment", 
            "title": "Environment"
        }, 
        {
            "location": "/reference/terraform-plugin/#example-usage_3", 
            "text": "# Create a new environment\nresource  layer0_environment   demo  {\n  name      =  demo \n  size      =  m3.medium \n  min_count = 0\n  user_data =  echo hello, world \n  os        =  linux \n  ami       =  ami123 \n}", 
            "title": "Example Usage"
        }, 
        {
            "location": "/reference/terraform-plugin/#argument-reference_2", 
            "text": "The following arguments are supported:   name  - (Required) The name of the environment  size  - (Optional, Default: \"m3.medium\") The size of the instances in the environment.\nAvailable instance sizes can be found  here  min_count  - (Optional, Default: 0) The minimum number of instances allowed in the environment  user-data  - (Optional) The user data template to use for the environment's autoscaling group.\nSee the  cli reference  for the default template.  os  - (Optional, Default: \"linux\") Specifies the type of operating system used in the environment.\nOptions are \"linux\" or \"windows\".  ami  - (Optional) A custom AMI ID to use in the environment. \nIf not specified, Layer0 will use its default AMI ID for the specified operating system.", 
            "title": "Argument Reference"
        }, 
        {
            "location": "/reference/terraform-plugin/#attribute-reference_2", 
            "text": "The following attributes are exported:   id  - The id of the environment  name  - The name of the environment  size  - The size of the instances in the environment  cluster_count  - The current number instances in the environment  security_group_id  - The ID of the environment's security group  os  - The operating system used for the environment  ami  - The AMI ID used for the environment", 
            "title": "Attribute Reference"
        }, 
        {
            "location": "/reference/terraform-plugin/#load-balancer", 
            "text": "Provides a Layer0 Load Balancer", 
            "title": "Load Balancer"
        }, 
        {
            "location": "/reference/terraform-plugin/#example-usage_4", 
            "text": "# Create a new load balancer\nresource  layer0_load_balancer   guestbook  {\n  name        =  guestbook \n  environment =  demo123 \n  private     = false\n\n  port {\n    host_port      = 80\n    container_port = 80\n    protocol       =  http \n  }\n\n  port {\n    host_port      = 443\n    container_port = 443\n    protocol       =  https \n    certificate    =  cert \n  }\n\n  health_check {\n    target              =  tcp:80 \n    interval            = 30\n    timeout             = 5\n    healthy_threshold   = 2\n    unhealthy_threshold = 2\n  }\n}", 
            "title": "Example Usage"
        }, 
        {
            "location": "/reference/terraform-plugin/#argument-reference_3", 
            "text": "The following arguments are supported:   name  - (Required) The name of the load balancer  environment  - (Required) The id of the environment to place the load balancer inside of  private  - (Optional) If true, the load balancer will not be exposed to the public internet  port  - (Optional, Default: 80:80/tcp) A list of port blocks. Ports documented below  health_check  - (Optional, Default: {\"TCP:80\" 30 5 2 2}) A health_check block. Health check documented below   Ports ( port ) support the following:   host_port  - (Required) The port on the load balancer to listen on  container_port  - (Required) The port on the docker container to route to  protocol  - (Required) The protocol to listen on. Valid values are  HTTP, HTTPS, TCP, or SSL  certificate  - (Optional) The name of an SSL certificate. Only required if the  HTTP  or  SSL  protocol is used.   Healthcheck ( health_check ) supports the following:   target  - (Required) The target of the check. Valid pattern is \"${PROTOCOL}:${PORT}${PATH}\", where PROTOCOL values are:  HTTP ,  HTTPS  - PORT and PATH are required  TCP ,  SSL  - PORT is required, PATH is not supported    interval  - (Required) The interval between checks.  timeout  - (Required) The length of time before the check times out.  healthy_threshold  - (Required) The number of checks before the instance is declared healthy.  unhealthy_threshold  - (Required) The number of checks before the instance is declared unhealthy.", 
            "title": "Argument Reference"
        }, 
        {
            "location": "/reference/terraform-plugin/#attribute-reference_3", 
            "text": "The following attributes are exported:   id  - The id of the load balancer  name  - The name of the load balancer  environment  - The id of the environment the load balancer exists in  private  - Whether or not the load balancer is private  url  - The URL of the load balancer", 
            "title": "Attribute Reference"
        }, 
        {
            "location": "/reference/terraform-plugin/#service", 
            "text": "Provides a Layer0 Service", 
            "title": "Service"
        }, 
        {
            "location": "/reference/terraform-plugin/#example-usage_5", 
            "text": "# Create a new service\nresource  layer0_service   guestbook  {\n  name          =  guestbook \n  environment   =  environment123 \n  deploy        =  deploy123 \n  load_balancer =  loadbalancer123 \n  scale         = 3\n  wait          = true\n}", 
            "title": "Example Usage"
        }, 
        {
            "location": "/reference/terraform-plugin/#argument-reference_4", 
            "text": "The following arguments are supported:   name  - (Required) The name of the service  environment  - (Required) The id of the environment to place the service inside of  deploy  - (Required) The id of the deploy for the service to run  load_balancer  (Optional) The id of the load balancer to place the service behind  scale  (Optional, Default: 1) The number of copies of the service to run  wait  (Optional) If true, will wait until the service's deployment completes before returning", 
            "title": "Argument Reference"
        }, 
        {
            "location": "/reference/terraform-plugin/#attribute-reference_4", 
            "text": "The following attributes are exported:   id  - The id of the service  name  - The name of the service  environment  - The id of the environment the service exists in  deploy  - The id of the deploy the service is running  load_balancer  - The id of the load balancer the service is behind (if  load_balancer  was set)  scale  - The current desired scale of the service", 
            "title": "Attribute Reference"
        }, 
        {
            "location": "/reference/terraform-plugin/#best-practices", 
            "text": "Always run  Terraform plan  before  terraform apply .\nThis will show you what action(s) Terraform plans to make before actually executing them.  Use  variables  to reference secrets.\nSecrets can be placed in a file named  Terraform.tfvars , or by setting  TF_VAR_*  environment variables.\nMore information can be found  here .  Use Terraform's  remote  command to backup and sync your  terraform.tfstate  file across different members in your organization.\nTerraform has documentation for using S3 as a backend  here .  Terraform  modules  allow you to define and consume reusable components.  Example configurations can be found  here", 
            "title": "Best Practices"
        }, 
        {
            "location": "/reference/updateservice/", 
            "text": "Updating a Layer0 service\n#\n\n\nThere are three methods of updating an existing Layer0 service. The first method is to update the existing Deploy to refer to a new Docker task definition. The second method is to create a new Service that uses the same Loadbalancer. The third method is to create both a new Loadbalancer and a new Service.\n\n\nThere are advantages and disadvantages to each of these methods. The following sections discuss the advantages and disadvantages of using each method, and include procedures for implementing each method.\n\n\nMethod 1: Refer to a new task definition\n#\n\n\nThis method of updating a Layer0 application is the easiest to implement, because you do not need to rescale the Service or modify the Loadbalancer. This method is completely transparent to all other components of the application, and using this method does not involve any downtime.\n\n\nThe disadvantage of using this method is that you cannot perform A/B testing of the old and new services, and you cannot control which traffic goes to the old service and which goes to the new one.\n\n\nTo replace a Deploy to refer to a new task definition:\n\n\n\n\nAt the command line, type the following to create a new Deploy: \nl0 deploy create [pathToTaskDefinition] [deployName]\nNote that if \n[deployName]\n already exists, this step will create a new version of that Deploy.\n\n\nType the following to update the existing Service: \nl0 service update [existingServiceName] [deployName]\nBy default, the Service you specify in this command will refer to the latest version of \n[deployName]\n, if multiple versions of the Deploy exist.\nNote\nIf you want to refer to a specific version of the Deploy, type the following command instead of the one shown above: \nl0 service update [serviceName] [deployName]:[deployVersion]\n\n\n\n\nMethod 2: Create a new Deploy and Service using the same Loadbalancer\n#\n\n\nThis method of updating a Layer0 application is also rather easy to implement. Like the method described in the previous section, this method is completely transparent to all other services and components of the application. This method also you allows you to re-scale the service if necessary, using the \nl0 service scale\n command. Finally, this method allows for indirect A/B testing of the application; you can change the scale of the application, and observe the success and failure rates.\n\n\nThe disadvantage of using this method is that you cannot control the routing of traffic between the old and new versions of the application.\n\n\nTo create a new Deploy and Service:\n\n\n\n\nAt the command line, type the following to create a new Deploy (or a new version of the Deploy, if \n[deployName]\n already exists):\n \nl0 deploy create [pathToTaskDefinition] [deployName]\n\n\nType the following command to create a new Service that refers to \n[deployName]\n behind an existing Loadbalancer named \n[loadbalancerName]\n:\n \nl0 service create --loadbalancer [loadbalancerName] [environmentName] [deployName]\n\n\nCheck to make sure that the new Service is working as expected. If it is, and you do not want to keep the old Service, type the following command to delete the old Service: \nl0 service delete [oldServiceName]\n\n\n\n\nMethod 3: Create a new Deploy, Loadbalancer and Service\n#\n\n\nThe final method of updating a Layer0 service is to create an entirely new Deploy, Loadbalancer and Service. This method gives you complete control over both the new and the old Service, and allows you to perform true A/B testing by routing traffic to individual Services.\n\n\nThe disadvantage of using this method is that you need to implement a method of routing traffic between the new and the old Loadbalancer.\n\n\nTo create a new Deploy, Loadbalancer and Service:\n\n\n\n\nAt the command line, type the following command to create a new Deploy:\nl0 deploy create [pathToTaskDefinition] [deployName]\n\n\nType the following command to create a new Loadbalancer:\n \nl0 loadbalancer create --port [portNumber] [environmentName] [loadbalancerName] [deployName]\nNote\nThe value of \n[loadbalancerName]\n in the above command must be unique.\n\n\nType the following command to create a new Service: \nl0 service create --loadbalancer [loadBalancerName] [environmentName] [serviceName] [deployName]\nNote\nThe value of \n[serviceName]\n in the above command  must be unique.\n\n\nImplement a method of routing traffic between the old and new Services, such as \nHAProxy\n or \nConsul\n.", 
            "title": "Updating a Service"
        }, 
        {
            "location": "/reference/updateservice/#updating-a-layer0-service", 
            "text": "There are three methods of updating an existing Layer0 service. The first method is to update the existing Deploy to refer to a new Docker task definition. The second method is to create a new Service that uses the same Loadbalancer. The third method is to create both a new Loadbalancer and a new Service.  There are advantages and disadvantages to each of these methods. The following sections discuss the advantages and disadvantages of using each method, and include procedures for implementing each method.", 
            "title": "Updating a Layer0 service"
        }, 
        {
            "location": "/reference/updateservice/#method-1-refer-to-a-new-task-definition", 
            "text": "This method of updating a Layer0 application is the easiest to implement, because you do not need to rescale the Service or modify the Loadbalancer. This method is completely transparent to all other components of the application, and using this method does not involve any downtime.  The disadvantage of using this method is that you cannot perform A/B testing of the old and new services, and you cannot control which traffic goes to the old service and which goes to the new one.  To replace a Deploy to refer to a new task definition:   At the command line, type the following to create a new Deploy:  l0 deploy create [pathToTaskDefinition] [deployName] Note that if  [deployName]  already exists, this step will create a new version of that Deploy.  Type the following to update the existing Service:  l0 service update [existingServiceName] [deployName] By default, the Service you specify in this command will refer to the latest version of  [deployName] , if multiple versions of the Deploy exist. Note If you want to refer to a specific version of the Deploy, type the following command instead of the one shown above:  l0 service update [serviceName] [deployName]:[deployVersion]", 
            "title": "Method 1: Refer to a new task definition"
        }, 
        {
            "location": "/reference/updateservice/#method-2-create-a-new-deploy-and-service-using-the-same-loadbalancer", 
            "text": "This method of updating a Layer0 application is also rather easy to implement. Like the method described in the previous section, this method is completely transparent to all other services and components of the application. This method also you allows you to re-scale the service if necessary, using the  l0 service scale  command. Finally, this method allows for indirect A/B testing of the application; you can change the scale of the application, and observe the success and failure rates.  The disadvantage of using this method is that you cannot control the routing of traffic between the old and new versions of the application.  To create a new Deploy and Service:   At the command line, type the following to create a new Deploy (or a new version of the Deploy, if  [deployName]  already exists):   l0 deploy create [pathToTaskDefinition] [deployName]  Type the following command to create a new Service that refers to  [deployName]  behind an existing Loadbalancer named  [loadbalancerName] :   l0 service create --loadbalancer [loadbalancerName] [environmentName] [deployName]  Check to make sure that the new Service is working as expected. If it is, and you do not want to keep the old Service, type the following command to delete the old Service:  l0 service delete [oldServiceName]", 
            "title": "Method 2: Create a new Deploy and Service using the same Loadbalancer"
        }, 
        {
            "location": "/reference/updateservice/#method-3-create-a-new-deploy-loadbalancer-and-service", 
            "text": "The final method of updating a Layer0 service is to create an entirely new Deploy, Loadbalancer and Service. This method gives you complete control over both the new and the old Service, and allows you to perform true A/B testing by routing traffic to individual Services.  The disadvantage of using this method is that you need to implement a method of routing traffic between the new and the old Loadbalancer.  To create a new Deploy, Loadbalancer and Service:   At the command line, type the following command to create a new Deploy: l0 deploy create [pathToTaskDefinition] [deployName]  Type the following command to create a new Loadbalancer:   l0 loadbalancer create --port [portNumber] [environmentName] [loadbalancerName] [deployName] Note The value of  [loadbalancerName]  in the above command must be unique.  Type the following command to create a new Service:  l0 service create --loadbalancer [loadBalancerName] [environmentName] [serviceName] [deployName] Note The value of  [serviceName]  in the above command  must be unique.  Implement a method of routing traffic between the old and new Services, such as  HAProxy  or  Consul .", 
            "title": "Method 3: Create a new Deploy, Loadbalancer and Service"
        }, 
        {
            "location": "/reference/consul/", 
            "text": "Consul reference\n#\n\n\nConsul\n is an open-source tool for discovering and configuring services in your network architecture. Specifically, Consul provides the following features:\n\n\n\n\nDiscovery of services\n\n\nMonitoring of the health of services\n\n\nKey/value storage with a simple HTTP API\n\n\n\n\nConsul Agent\n#\n\n\nThe \nConsul Agent\n exposes a DNS API for easy consumption of data generated by \nRegistrator\n. The Consul Agent can run either in server or client mode.\n\n\nWhen run as a Layer0 Service, the Consul Agent runs in server mode. To ensure the integrity of your data, the service in which you run consul should be scaled to size 3 or greater. A group of several consul deployments is known as a \"\ncluster\n.\"\n\n\nOther Layer0 Services that use Consul will run the Consul Agent in client mode, alongside their application containers.\nThe client is a very lightweight process that registers services, runs health checks, and forwards queries to servers.\n\n\nRegistrator\n#\n\n\nRegistrator\n is a tool that automatically registers and deregisters services into a Consul Cluster by inspecting Docker containers as they come online.\nContainer registration is based off of environment variables on the container.\nLayer0 Services that use Consul will run Registrator alongside their application containers.\n\n\nService Configuration\n#\n\n\nLayer0 Services that use Consul will need to add the \nRegistrator\n and \nConsul Agent\n definitions to the\n\ncontainerDefinitions\n section of your Deploys. You must also add the \nDocker Socket\n definition to the \nvolumes\n section of your Deploys.\n\n\nFor an example of a Deploy that uses Consul, see the \nGuestbook with Consul\n guide.\n\n\n\n\nRegistrator Container Definition\n#\n\n\n{\n    \nname\n: \nregistrator\n,\n    \nimage\n: \ngliderlabs/registrator:master\n,\n    \nessential\n: true,\n    \nlinks\n: [\nconsul-agent\n],\n    \nentrypoint\n: [\n/bin/sh\n, \n-c\n],\n    \ncommand\n: [\n/bin/registrator -retry-attempts=-1 -retry-interval=30000 -ip $(wget http://169.254.169.254/latest/meta-data/local-ipv4 -q -O -) consul://consul-agent:8500\n],\n    \nmemory\n: 128,\n    \nmountPoints\n: [\n        {\n            \nsourceVolume\n: \ndockersocket\n,\n            \ncontainerPath\n: \n/tmp/docker.sock\n\n        }\n    ]\n},\n\n\n\n\n\n\nConsul Agent Container Definition\n#\n\n\n\n\nWarning\n\n\n\n\nYou must replace \nurl\n with your Layer0 Consul Load Balancer's\n\n\n{\n    \nname\n: \nconsul-agent\n,\n    \nimage\n: \nprogrium/consul\n,\n    \nessential\n: true,\n    \nentrypoint\n: [\n/bin/bash\n, \n-c\n],\n    \ncommand\n: [\n/bin/start -advertise $(wget http://169.254.169.254/latest/meta-data/local-ipv4 -q -O -) -retry-join $EXTERNAL_URL -recursor $UPSTREAM_DNS -retry-interval 30s\n],\n    \nmemory\n: 128,\n    \nportMappings\n: [\n        {\n            \nhostPort\n: 8500,\n            \ncontainerPort\n: 8500\n        },\n        {\n            \nhostPort\n: 53,\n            \ncontainerPort\n: 53,\n            \nprotocol\n: \nudp\n\n        }\n    ],\n    \nenvironment\n: [\n        {\n            \nname\n: \nEXTERNAL_URL\n,\n            \nvalue\n: \nurl\n\n    },\n    {\n            \nname\n: \nUPSTREAM_DNS\n,\n            \nvalue\n: \n10.100.0.2\n\n        }\n    ]\n},\n\n\n\n\nEnvironment Variables\n#\n\n\n\n\nEXTERNAL_URL\n - URL of the consul cluster\n\n\nUPSTREAM_DNS\n - The DNS server consul-agent queries for DNS entries that it cannot resolve internally (e.g. google.com)\n\n\nThe default value for \nUPSTREAM_DNS\n assumes you're using the default Layer0 configuration, making your internal DNS endpoint 10.100.0.2.  If you are a using a non standard configuration (e.g. installing Layer0 in an existing VPC with a CIDR other than \n10.100.0.0/16\n) please modify this variable accordingly.\n\n\n\n\n\n\n\n\n\n\nDocker Socket Volume Definition\n#\n\n\nvolumes\n: [\n    {\n        \nname\n: \ndockersocket\n,\n        \nhost\n: {\n                \nsourcePath\n: \n/var/run/docker.sock\n\n        }\n    }\n],", 
            "title": "Consul"
        }, 
        {
            "location": "/reference/consul/#consul-reference", 
            "text": "Consul  is an open-source tool for discovering and configuring services in your network architecture. Specifically, Consul provides the following features:   Discovery of services  Monitoring of the health of services  Key/value storage with a simple HTTP API", 
            "title": "Consul reference"
        }, 
        {
            "location": "/reference/consul/#consul-agent", 
            "text": "The  Consul Agent  exposes a DNS API for easy consumption of data generated by  Registrator . The Consul Agent can run either in server or client mode.  When run as a Layer0 Service, the Consul Agent runs in server mode. To ensure the integrity of your data, the service in which you run consul should be scaled to size 3 or greater. A group of several consul deployments is known as a \" cluster .\"  Other Layer0 Services that use Consul will run the Consul Agent in client mode, alongside their application containers.\nThe client is a very lightweight process that registers services, runs health checks, and forwards queries to servers.", 
            "title": "Consul Agent"
        }, 
        {
            "location": "/reference/consul/#registrator", 
            "text": "Registrator  is a tool that automatically registers and deregisters services into a Consul Cluster by inspecting Docker containers as they come online.\nContainer registration is based off of environment variables on the container.\nLayer0 Services that use Consul will run Registrator alongside their application containers.", 
            "title": "Registrator"
        }, 
        {
            "location": "/reference/consul/#service-configuration", 
            "text": "Layer0 Services that use Consul will need to add the  Registrator  and  Consul Agent  definitions to the containerDefinitions  section of your Deploys. You must also add the  Docker Socket  definition to the  volumes  section of your Deploys.  For an example of a Deploy that uses Consul, see the  Guestbook with Consul  guide.", 
            "title": "Service Configuration"
        }, 
        {
            "location": "/reference/consul/#registrator-container-definition", 
            "text": "{\n     name :  registrator ,\n     image :  gliderlabs/registrator:master ,\n     essential : true,\n     links : [ consul-agent ],\n     entrypoint : [ /bin/sh ,  -c ],\n     command : [ /bin/registrator -retry-attempts=-1 -retry-interval=30000 -ip $(wget http://169.254.169.254/latest/meta-data/local-ipv4 -q -O -) consul://consul-agent:8500 ],\n     memory : 128,\n     mountPoints : [\n        {\n             sourceVolume :  dockersocket ,\n             containerPath :  /tmp/docker.sock \n        }\n    ]\n},", 
            "title": "Registrator Container Definition"
        }, 
        {
            "location": "/reference/consul/#consul-agent-container-definition", 
            "text": "Warning   You must replace  url  with your Layer0 Consul Load Balancer's  {\n     name :  consul-agent ,\n     image :  progrium/consul ,\n     essential : true,\n     entrypoint : [ /bin/bash ,  -c ],\n     command : [ /bin/start -advertise $(wget http://169.254.169.254/latest/meta-data/local-ipv4 -q -O -) -retry-join $EXTERNAL_URL -recursor $UPSTREAM_DNS -retry-interval 30s ],\n     memory : 128,\n     portMappings : [\n        {\n             hostPort : 8500,\n             containerPort : 8500\n        },\n        {\n             hostPort : 53,\n             containerPort : 53,\n             protocol :  udp \n        }\n    ],\n     environment : [\n        {\n             name :  EXTERNAL_URL ,\n             value :  url \n    },\n    {\n             name :  UPSTREAM_DNS ,\n             value :  10.100.0.2 \n        }\n    ]\n},", 
            "title": "Consul Agent Container Definition"
        }, 
        {
            "location": "/reference/consul/#environment-variables", 
            "text": "EXTERNAL_URL  - URL of the consul cluster  UPSTREAM_DNS  - The DNS server consul-agent queries for DNS entries that it cannot resolve internally (e.g. google.com)  The default value for  UPSTREAM_DNS  assumes you're using the default Layer0 configuration, making your internal DNS endpoint 10.100.0.2.  If you are a using a non standard configuration (e.g. installing Layer0 in an existing VPC with a CIDR other than  10.100.0.0/16 ) please modify this variable accordingly.", 
            "title": "Environment Variables"
        }, 
        {
            "location": "/reference/consul/#docker-socket-volume-definition", 
            "text": "volumes : [\n    {\n         name :  dockersocket ,\n         host : {\n                 sourcePath :  /var/run/docker.sock \n        }\n    }\n],", 
            "title": "Docker Socket Volume Definition"
        }, 
        {
            "location": "/reference/task_definition/", 
            "text": "Task Definitions\n#\n\n\nThis guide gives some overview into the composition of a task definition.\nFor more comprehensive documentation, we recommend taking a look at the official AWS docs:\n\n\n\n\nCreating a Task Definition\n\n\nTask Definition Parameters\n\n\n\n\nSample\n#\n\n\nThe following snippet contains the task definition for the \nGuestbook\n application\n\n\n{\n    \nAWSEBDockerrunVersion\n: 2,\n    \ncontainerDefinitions\n: [\n        {\n            \nname\n: \nl0-demo-guestbook\n,\n            \nimage\n: \nd.ims.io/xfra/l0-guestbook\n,\n            \nessential\n: true,\n            \nmemory\n: 128,\n            \nportMappings\n: [\n                {\n                    \nhostPort\n: 80,\n                    \ncontainerPort\n: 80\n                }\n            ],\n            \nenvironment\n: [\n                {\n                    \nname\n: \nSERVICE_80_NAME\n,\n                    \nvalue\n: \nl0-guestbook\n\n                }\n            ]\n        }\n    ]\n}\n\n\n\n\n\n\nName\n The name of the container\n\n\n\n\n\n\nWarning\n\n\n\n\nIf you wish to update your task definition, the container names \nmust\n remain the same.\nIf any container names are changed or removed in an updated task definition,\nECS will not know how the existing container(s) should be mapped over and you will not be able to deploy the updated task definition.\nIf you encounter a scenario where you must change or remove a container's name in a task definition, we recommend re-creating the Layer0 Deploy and Service.\n\n\n\n\nImage\n The Docker image used to build the container. The image format is \nurl/image:tag\n\n\nThe \nurl\n specifies which Docker Repo to pull the image from (e.g. \nd.ims.io\n).\nIf \nurl\n is not specified, \nDocker Hub\n is used\n\n\nThe \nimage\n specifies the name of the image to grab\n\n\nThe \ntag\n specifies which version of image to grab.\nIf \ntag\n is not specified, \n:latest\n is used\n\n\n\n\n\n\nEssential\n If set to \ntrue\n, all other containers in the task definition will be stopped if that container fails or stops for any reason.\nOtherwise, the container's failure will not affect the rest of the containers in the task definition.\n\n\nMemory\n The number of MiB of memory to reserve for the container.\nIf your container attempts to exceed the memory allocated here, the container is killed\n\n\nPortMappings\n A list of hostPort, containerPort mappings for the container\n\n\nHostPort\n The port number on the host instance reserved for your container.\nIf your Layer0 Service is behind a Layer0 Load Balancer, this should map to an \ninstancePort\n on the Layer0 Load Balancer.\n\n\nContainerPort\n The port number the container should receive traffic on.\nAny traffic received from the instance's \nhostPort\n will be forwarded to the container on this port\n\n\n\n\n\n\nEnvironment\n A list of key/value pairs that will be available to the container as environment variables", 
            "title": "Task Definitions"
        }, 
        {
            "location": "/reference/task_definition/#task-definitions", 
            "text": "This guide gives some overview into the composition of a task definition.\nFor more comprehensive documentation, we recommend taking a look at the official AWS docs:   Creating a Task Definition  Task Definition Parameters", 
            "title": "Task Definitions"
        }, 
        {
            "location": "/reference/task_definition/#sample", 
            "text": "The following snippet contains the task definition for the  Guestbook  application  {\n     AWSEBDockerrunVersion : 2,\n     containerDefinitions : [\n        {\n             name :  l0-demo-guestbook ,\n             image :  d.ims.io/xfra/l0-guestbook ,\n             essential : true,\n             memory : 128,\n             portMappings : [\n                {\n                     hostPort : 80,\n                     containerPort : 80\n                }\n            ],\n             environment : [\n                {\n                     name :  SERVICE_80_NAME ,\n                     value :  l0-guestbook \n                }\n            ]\n        }\n    ]\n}   Name  The name of the container    Warning   If you wish to update your task definition, the container names  must  remain the same.\nIf any container names are changed or removed in an updated task definition,\nECS will not know how the existing container(s) should be mapped over and you will not be able to deploy the updated task definition.\nIf you encounter a scenario where you must change or remove a container's name in a task definition, we recommend re-creating the Layer0 Deploy and Service.   Image  The Docker image used to build the container. The image format is  url/image:tag  The  url  specifies which Docker Repo to pull the image from (e.g.  d.ims.io ).\nIf  url  is not specified,  Docker Hub  is used  The  image  specifies the name of the image to grab  The  tag  specifies which version of image to grab.\nIf  tag  is not specified,  :latest  is used    Essential  If set to  true , all other containers in the task definition will be stopped if that container fails or stops for any reason.\nOtherwise, the container's failure will not affect the rest of the containers in the task definition.  Memory  The number of MiB of memory to reserve for the container.\nIf your container attempts to exceed the memory allocated here, the container is killed  PortMappings  A list of hostPort, containerPort mappings for the container  HostPort  The port number on the host instance reserved for your container.\nIf your Layer0 Service is behind a Layer0 Load Balancer, this should map to an  instancePort  on the Layer0 Load Balancer.  ContainerPort  The port number the container should receive traffic on.\nAny traffic received from the instance's  hostPort  will be forwarded to the container on this port    Environment  A list of key/value pairs that will be available to the container as environment variables", 
            "title": "Sample"
        }, 
        {
            "location": "/reference/architecture/", 
            "text": "Layer0 Architecture\n#\n\n\nLayer0 is built on top of the following primary technologies:\n\n\n\n\nApplication Container: \nDocker\n\n\nCloud Provider: \nAmazon Web Services\n\n\nContainer Management: \nAmazon EC2 Container Service (ECS)\n\n\nLoad Balancing: \nAmazon Elastic Load Balancing\n\n\nInfrastructure Configuration: Hashicorp \nTerraform\n\n\nIdentity Management: \nAuth0", 
            "title": "Architecture"
        }, 
        {
            "location": "/reference/architecture/#layer0-architecture", 
            "text": "Layer0 is built on top of the following primary technologies:   Application Container:  Docker  Cloud Provider:  Amazon Web Services  Container Management:  Amazon EC2 Container Service (ECS)  Load Balancing:  Amazon Elastic Load Balancing  Infrastructure Configuration: Hashicorp  Terraform  Identity Management:  Auth0", 
            "title": "Layer0 Architecture"
        }, 
        {
            "location": "/reference/ecr/", 
            "text": "EC2 Container Registry\n#\n\n\nECR is an Amazon implementation of a docker registry.  It acts as a private registry in your AWS account, which can be accessed from any docker client, and Layer0.  Consider using ECR if you have stability issues with hosted docker registries, and do not wish to share your images publicly on \ndockerhub\n.\n\n\nSetup\n#\n\n\nWhen interacting with ECR, you will first need to create a repository and a login to interact from your development machine.\n\n\nRepository\n#\n\n\nEach repository needs to be created by an AWS api call.\n\n\n  \n aws ecr create-repository --repository-name myteam/myproject\n\n\n\n\nLogin\n#\n\n\nTo authenticate with the ECR service, Amazon provides the \nget-login\n command, which generates an authentication token, and returns a docker command to set it up\n\n\n  \n aws ecr get-login\n  # this command will return the following: (password is typically hundreds of characters)\n  docker login -u AWS -p password -e none https://aws_account_id.dkr.ecr.us-east-1.amazonaws.com\n\n\n\n\nExecute the provided docker command to store the login credentials\n\n\nAfterward creating the repository and local login credentials you may interact with images (and tags) under this path from a local docker client.\n\n\n  docker pull ${ecr-url}/myteam/myproject\n  docker push ${ecr-url}/myteam/myproject:custom-tag-1\n\n\n\n\nDeploy Example\n#\n\n\nHere we'll walk through using ECR when deploying to Layer0,  Using a very basic wait container.\n\n\nMake docker image\n#\n\n\nYour docker image can be built locally or pulled from dockerhub.  For this example, we made a service that waits and then exits (useful for triggering regular restarts).\n\n\nFROM busybox\n\nENV SLEEP_TIME=60\n\nCMD sleep $SLEEP_TIME\n\n\n\n\nThen build the file, with the tag \nxfra/wait\n\n\n \n docker build -f Dockerfile.wait -t xfra/wait .\n\n\n\n\nUpload to ECR\n#\n\n\nAfter preparing a login and registry, tag the image with the remote url, and use \ndocker push\n\n\n  docker tag xfra/wait 111222333444.dkr.ecr.us-east-1.amazonaws.com/xfra-wait\n  docker push 111222333444.dkr.ecr.us-east-1.amazonaws.com/xfra-wait\n\n\n\n\n\n\nNote: your account id in this url will be different.\n\n\n\n\nCreate a deploy\n#\n\n\nTo run this image in Layer0, we create a dockerrun file, describing the instance and any additional variables\n\n\n{\n  \ncontainerDefinitions\n: [\n    {\n      \nname\n: \ntimeout\n,\n      \nimage\n: \n111222333444.dkr.ecr.us-east-1.amazonaws.com/xfra-wait:latest\n,\n      \nessential\n: true,\n      \nmemory\n: 10,\n      \nenvironment\n: [\n        { \nname\n: \nSLEEP_TIME\n, \nvalue\n: \n43200\n }\n      ]\n    }\n  ]\n}\n\n\n\n\nAnd create that in Layer0\n\n\n  l0 deploy create timeout.dockerrun.aws.json timeout\n\n\n\n\nDeploy\n#\n\n\nFinally, run that deploy as a service or a task. (the service will restart every 12 hours)\n\n\n  l0 service create demo timeoutsvc timeout:latest\n\n\n\n\nReferences\n#\n\n\n\n\nECR User Guide\n\n\ncreate-repository\n\n\nget-login", 
            "title": "ECR"
        }, 
        {
            "location": "/reference/ecr/#ec2-container-registry", 
            "text": "ECR is an Amazon implementation of a docker registry.  It acts as a private registry in your AWS account, which can be accessed from any docker client, and Layer0.  Consider using ECR if you have stability issues with hosted docker registries, and do not wish to share your images publicly on  dockerhub .", 
            "title": "EC2 Container Registry"
        }, 
        {
            "location": "/reference/ecr/#setup", 
            "text": "When interacting with ECR, you will first need to create a repository and a login to interact from your development machine.", 
            "title": "Setup"
        }, 
        {
            "location": "/reference/ecr/#repository", 
            "text": "Each repository needs to be created by an AWS api call.      aws ecr create-repository --repository-name myteam/myproject", 
            "title": "Repository"
        }, 
        {
            "location": "/reference/ecr/#login", 
            "text": "To authenticate with the ECR service, Amazon provides the  get-login  command, which generates an authentication token, and returns a docker command to set it up      aws ecr get-login\n  # this command will return the following: (password is typically hundreds of characters)\n  docker login -u AWS -p password -e none https://aws_account_id.dkr.ecr.us-east-1.amazonaws.com  Execute the provided docker command to store the login credentials  Afterward creating the repository and local login credentials you may interact with images (and tags) under this path from a local docker client.    docker pull ${ecr-url}/myteam/myproject\n  docker push ${ecr-url}/myteam/myproject:custom-tag-1", 
            "title": "Login"
        }, 
        {
            "location": "/reference/ecr/#deploy-example", 
            "text": "Here we'll walk through using ECR when deploying to Layer0,  Using a very basic wait container.", 
            "title": "Deploy Example"
        }, 
        {
            "location": "/reference/ecr/#make-docker-image", 
            "text": "Your docker image can be built locally or pulled from dockerhub.  For this example, we made a service that waits and then exits (useful for triggering regular restarts).  FROM busybox\n\nENV SLEEP_TIME=60\n\nCMD sleep $SLEEP_TIME  Then build the file, with the tag  xfra/wait     docker build -f Dockerfile.wait -t xfra/wait .", 
            "title": "Make docker image"
        }, 
        {
            "location": "/reference/ecr/#upload-to-ecr", 
            "text": "After preparing a login and registry, tag the image with the remote url, and use  docker push    docker tag xfra/wait 111222333444.dkr.ecr.us-east-1.amazonaws.com/xfra-wait\n  docker push 111222333444.dkr.ecr.us-east-1.amazonaws.com/xfra-wait   Note: your account id in this url will be different.", 
            "title": "Upload to ECR"
        }, 
        {
            "location": "/reference/ecr/#create-a-deploy", 
            "text": "To run this image in Layer0, we create a dockerrun file, describing the instance and any additional variables  {\n   containerDefinitions : [\n    {\n       name :  timeout ,\n       image :  111222333444.dkr.ecr.us-east-1.amazonaws.com/xfra-wait:latest ,\n       essential : true,\n       memory : 10,\n       environment : [\n        {  name :  SLEEP_TIME ,  value :  43200  }\n      ]\n    }\n  ]\n}  And create that in Layer0    l0 deploy create timeout.dockerrun.aws.json timeout", 
            "title": "Create a deploy"
        }, 
        {
            "location": "/reference/ecr/#deploy", 
            "text": "Finally, run that deploy as a service or a task. (the service will restart every 12 hours)    l0 service create demo timeoutsvc timeout:latest", 
            "title": "Deploy"
        }, 
        {
            "location": "/reference/ecr/#references", 
            "text": "ECR User Guide  create-repository  get-login", 
            "title": "References"
        }, 
        {
            "location": "/troubleshooting/commonissues/", 
            "text": "Common issues and their solutions\n#\n\n\n\"Connection refused\" error when executing Layer0 commands\n#\n\n\nWhen executing commands using the Layer0 CLI, you may see the following error message: \"Get http://localhost:9090/\ncommand\n/: dial tcp 127.0.0.1:9090: connection refused\", where \ncommand\n is the Layer0 command you are trying to execute.\n\n\nThis error indicates that your Layer0 environment variables have not been set for the current session. See the \n\"Configure environment variables\" section\n of the Layer0 installation guide for instructions for setting up your environment variables.\n\n\n\n\n\"Invalid Dockerrun.aws.json\" error when creating a deploy\n#\n\n\nByte Order Marks (BOM) in Dockerrun file\n#\n\n\nIf your Dockerrun.aws.json file contains a Byte Order Marker, you may receive an \"Invalid Dockerrun.aws.json\" error when creating a deploy. If you create or edit the Dockerrun file using Visual Studio, and you have not modified the file encoding settings in Visual Studio, you are likely to encounter this error.\n\n\nTo remove the BOM:\n\n\n\n\n\n\nAt the command line, type the following to remove the BOM:\n\n\n\n\n(Linux/OS X) \ntail -c +4\n \nDockerrunFile\n \n \nDockerrunFileNew\n\n\nReplace \nDockerrunFile\n with the path to your Dockerrun file, and \nDockerrunFileNew\n with a new name for the Dockerrun file without the BOM.\n\n\n\n\n\n\n\n\nAlternatively, you can use the \ndos2unix file converter\n to remove the BOM from your Dockerrun files. Dos2unix is available for Windows, Linux and Mac OS.\n\n\nTo remove the BOM using dos2unix:\n\n\n\n\n\n\nAt the command line, type the following:\n\n\n\n\ndos2unix --remove-bom -n\n \nDockerrunFile\n \nDockerrunFileNew\n\n\nReplace \nDockerrunFile\n with the path to your Dockerrun file, and \nDockerrunFileNew\n with a new name for the Dockerrun file without the BOM.\n\n\n\n\n\n\n\n\n\n\n\"AWS Error: the key pair '\n' does not exist (code 'ValidationError')\" with l0-setup\n#\n\n\nThis occurs when you pass a non-existent EC2 keypair to l0-setup. To fix this, follow the instructions for \ncreating an EC2 Key Pair\n.\n\n\n\n\nAfter you've created a new EC2 Key Pair, run the following command:\n\n\n  \nl0-setup plan\n \nprefix\n \n-var key_pair\n=\nkeypair", 
            "title": "Common Issues"
        }, 
        {
            "location": "/troubleshooting/commonissues/#common-issues-and-their-solutions", 
            "text": "", 
            "title": "Common issues and their solutions"
        }, 
        {
            "location": "/troubleshooting/commonissues/#connection-refused-error-when-executing-layer0-commands", 
            "text": "When executing commands using the Layer0 CLI, you may see the following error message: \"Get http://localhost:9090/ command /: dial tcp 127.0.0.1:9090: connection refused\", where  command  is the Layer0 command you are trying to execute.  This error indicates that your Layer0 environment variables have not been set for the current session. See the  \"Configure environment variables\" section  of the Layer0 installation guide for instructions for setting up your environment variables.", 
            "title": "\"Connection refused\" error when executing Layer0 commands"
        }, 
        {
            "location": "/troubleshooting/commonissues/#invalid-dockerrunawsjson-error-when-creating-a-deploy", 
            "text": "", 
            "title": "\"Invalid Dockerrun.aws.json\" error when creating a deploy"
        }, 
        {
            "location": "/troubleshooting/commonissues/#byte-order-marks-bom-in-dockerrun-file", 
            "text": "If your Dockerrun.aws.json file contains a Byte Order Marker, you may receive an \"Invalid Dockerrun.aws.json\" error when creating a deploy. If you create or edit the Dockerrun file using Visual Studio, and you have not modified the file encoding settings in Visual Studio, you are likely to encounter this error.  To remove the BOM:    At the command line, type the following to remove the BOM:   (Linux/OS X)  tail -c +4   DockerrunFile     DockerrunFileNew  Replace  DockerrunFile  with the path to your Dockerrun file, and  DockerrunFileNew  with a new name for the Dockerrun file without the BOM.     Alternatively, you can use the  dos2unix file converter  to remove the BOM from your Dockerrun files. Dos2unix is available for Windows, Linux and Mac OS.  To remove the BOM using dos2unix:    At the command line, type the following:   dos2unix --remove-bom -n   DockerrunFile   DockerrunFileNew  Replace  DockerrunFile  with the path to your Dockerrun file, and  DockerrunFileNew  with a new name for the Dockerrun file without the BOM.", 
            "title": "Byte Order Marks (BOM) in Dockerrun file"
        }, 
        {
            "location": "/troubleshooting/commonissues/#aws-error-the-key-pair-does-not-exist-code-validationerror-with-l0-setup", 
            "text": "This occurs when you pass a non-existent EC2 keypair to l0-setup. To fix this, follow the instructions for  creating an EC2 Key Pair .   After you've created a new EC2 Key Pair, run the following command: \n   l0-setup plan   prefix   -var key_pair = keypair", 
            "title": "\"AWS Error: the key pair '' does not exist (code 'ValidationError')\" with l0-setup"
        }, 
        {
            "location": "/troubleshooting/ssh/", 
            "text": "Secure Shell (SSH)\n#\n\n\nYou can use Secure Shell (SSH) to access your Layer0 environment(s).\n\n\nBy default, Layer0 Setup asks for an EC2 key pair when creating a new Layer0. This key pair is associated with all machines that host your Layer0 Services. This means you can use SSH to log into the underlying Docker host to perform tasks such as troubleshooting failing containers or viewing logs. For information about creating an EC2 key pair, see \nInstall and Configure Layer0\n.\n\n\n\n\nWarning\n\n\nThis section is recommended for development debugging only.\nIt is \nnot\n recommended for production environments.\n\n\n\n\nTo SSH into a Service\n#\n\n\n\n\nIn a console window, add port 2222:22/tcp to your Service's load balancer:\n\n\n\n\nl0 loadbalancer addport \nname\n 2222:22/tcp\n\n\n\n\n\n  \nSSH into your Service by supplying the load balancer url and key pair file name.\n\n\n\n\n\nssh -i \nkey pair path and file name\n ec2-user@\nload balancer url\n -p 2222\n\n\n\n\n\n  \nIf required, Use Docker to access a specific container with Bash.\n\n\n\n\n\ndocker exec -it \ncontainer id\n /bin/bash\n\n\n\n\nRemarks\n#\n\n\nYou can get the load balancer url from the Load Balancers section of your Layer0 AWS console.\n\n\nUse the \nloadbalancer dropport\n subcommand to remove a port configuration from an existing Layer0 load balancer.\n\n\nYou \ncannot\n change the key pair after a Layer0 has been created. If you lose your key pair or need to generate a new one, you will need to create a new Layer0.\n\n\nIf your Service is behind a private load balancer, or none at all, you can either re-create your Service behind a public load balancer, use an existing public load balancer as a \"jump\" point, or create a new Layer0 Service behind a public load balancer to serve as a \"jump\" point.", 
            "title": "Secure Shell (SSH)"
        }, 
        {
            "location": "/troubleshooting/ssh/#secure-shell-ssh", 
            "text": "You can use Secure Shell (SSH) to access your Layer0 environment(s).  By default, Layer0 Setup asks for an EC2 key pair when creating a new Layer0. This key pair is associated with all machines that host your Layer0 Services. This means you can use SSH to log into the underlying Docker host to perform tasks such as troubleshooting failing containers or viewing logs. For information about creating an EC2 key pair, see  Install and Configure Layer0 .   Warning  This section is recommended for development debugging only.\nIt is  not  recommended for production environments.", 
            "title": "Secure Shell (SSH)"
        }, 
        {
            "location": "/troubleshooting/ssh/#to-ssh-into-a-service", 
            "text": "In a console window, add port 2222:22/tcp to your Service's load balancer:   l0 loadbalancer addport  name  2222:22/tcp  \n   SSH into your Service by supplying the load balancer url and key pair file name.   ssh -i  key pair path and file name  ec2-user@ load balancer url  -p 2222  \n   If required, Use Docker to access a specific container with Bash.   docker exec -it  container id  /bin/bash", 
            "title": "To SSH into a Service"
        }, 
        {
            "location": "/troubleshooting/ssh/#remarks", 
            "text": "You can get the load balancer url from the Load Balancers section of your Layer0 AWS console.  Use the  loadbalancer dropport  subcommand to remove a port configuration from an existing Layer0 load balancer.  You  cannot  change the key pair after a Layer0 has been created. If you lose your key pair or need to generate a new one, you will need to create a new Layer0.  If your Service is behind a private load balancer, or none at all, you can either re-create your Service behind a public load balancer, use an existing public load balancer as a \"jump\" point, or create a new Layer0 Service behind a public load balancer to serve as a \"jump\" point.", 
            "title": "Remarks"
        }
    ]
}